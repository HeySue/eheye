% \documentclass[12pt]{article}
% \usepackage{xcolor}
% \input{nams.tex}
% \usepackage[numbers]{natbib}

% \title{Literature review}
% \date{\vspace{-5ex}}


% \begin{document}
% \maketitle
\chapter{Literature review}
\label{ch: literature_review}


The field of quantile estimation is big, and this thesis is incapable of covering all the topics.
\marginpar{This part feels weird}
This chapter details some of the work that has been done in the quantile estimation on streaming data, and is organised as follows:

Section~\ref{streamingdata} introduces the data stream phenomenon and some quantile estimation methods on it, which are classifies by algorithm types. 
% while section~\ref{singlequantile} and ~\ref{multiquantile} classify quantile estimation problems by aims

Section~\ref{singlequantile} discusses algorithms on single quantile estimation, and especially details two methods for restricted memory storage limit.

Section~\ref{multiquantile} presents algorithms on the problem of multi-quantile estimation, which estimate multiple quantiles at the same time, and the methods are 

%----------------------------------------------------------------------------------------
%   Quantile estimation methods on streaming data
%----------------------------------------------------------------------------------------

\section{Quantile estimation methods on streaming data}
\label{streamingdata}
Data stream is the phenomenon that input data comes at a high rate instead of being accessible all at the same time\cite{muthukrishnanDataStreamsAlgorithms2005}.
Generally speaking, the continuous data can come without a start or end, or persistent time intervals.
Streaming data has become popular in industrial application, especially big data use cases, for it
``includes a wide variety of data such as log files generated by customers using your mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.''\cite{WhatStreamingData}.

Compared with instantly accessible data, three challenges stand out in data stream: transmission, computation and storage\cite{muthukrishnanDataStreamsAlgorithms2005}.
Transmission problem is difficult when there are frequent delays or data in complicated format,
while challenges in computation comes with high complexity programs. Also, the balance between long-term demanding and capable memory storage can be another tricky problem.
In this research, we focus only on the latter two problems. The aim range is limited to programs with low computation complexity and restricted memory storage, under the assumption that the low-dimensional input data is transmitted at a not very high rate.
\\\\
A common assumption people make about data stream is that the data points are considered as independent individual samples from a certain statistic distribution. 
Although access to the distribution is impossible, studies on the random samples can reveal some useful information. 
Quantile is an important \textbf{feature (?)} for a distribution, which represents the cutting points which divides a data distribution by its ranking.
At the first sight, the computation of quantile is relatively as long as the dataset is sorted.
The data stream, however, is updated at the arrival of each single input, indicating quantile calculation then demands one sorting for every updates.
For what's worse, the unknown size of data stream can cause a severe memory shortage problem for any system. 
The conclusion now becomes clear that quantile calculation fail to be a feasible solution for data stream.

Quantile estimation replaces calculation for data streams due to the computation and storage problems.
The estimation works with limited computational power and memory storage.
Generally, quantile estimation algorithms trade off accuracy for computation and storage.
\citeauthor{munroSelectionSortingLimited1980} \cite{munroSelectionSortingLimited1980} has shown that, for any algorithm that computes the exact $\phi$-quantile from an $N$-element data stream with $p$ passes, requires at least $\Omega(N^{\frac{1}{p}})$ space.
Therefore quantile estimation algorithms all necessarily aim for sub-linear space.
In this chapter, we introduce other people's work on how improvements are made for higher estimation accuracy with lower computational and memory complexity, and that how the trade off is balanced in different situations. Section~\ref{deterministic}, ~\ref{randomised} and ~\ref{other} divide the algorithms by their types: deterministic, randomised and others. The classification is based on works from \citeauthor{buragohainQuantilesStreams2009}\cite{buragohainQuantilesStreams2009} and \citeauthor{wangQuantilesDataStreams2013}\cite{wangQuantilesDataStreams2013}.
% Model: time series: each input $a_i$ equals $A[i]$ and appears in increasing order.
% One-pass application: the reading of an input is done in exactly once and all inputs are read in order. Usually, the process for each input takes only one or few passes. 

It is worth noting that since quantiles are related with ranking, a quantile estimation algorithm takes one-dimensional numerical data input by default. The simple data format also partially explains why transmission is not usually considered a problem in most quantile estimation algorithms.
% -----------------------------------------------------------------------------
% to be finished
\subsection{Deterministic algorithms}
\label{deterministic}
% Manku's work
The first deterministic algorithm on quantile estimation was proposed by \citeauthor{mankuApproximateMediansOther1998}\cite{mankuApproximateMediansOther1998} (referred to as the MRL algorithm below), based on the work by \citeauthor{munroSelectionSortingLimited1980}\cite{munroSelectionSortingLimited1980}. Along with the algorithm, the notation \textit{$\epsilon$-approximate $\phi$-quantile} is first introduced for accuracy measurement by \citeauthor{mankuApproximateMediansOther1998}, which describes the property of guaranteed accuracy on $\phi$-quantile within a pre-specified precision $\epsilon$. The algorithm has a space complexity $O(\frac{1}{\epsilon}\log^2 (\epsilon N))$ to compute \textit{$\epsilon$-approximate quantile summary}. Building on this, two improvement algorithms have been proposed by \citeauthor{greenwaldQuantilesEquidepthHistograms2016a}\cite{greenwaldQuantilesEquidepthHistograms2016a} and \citeauthor{shrivastavaMediansNewAggregation2004}\cite{shrivastavaMediansNewAggregation2004}.

\citeauthor{greenwaldQuantilesEquidepthHistograms2016a}\cite{greenwaldQuantilesEquidepthHistograms2016a} propose an algorithm (referred to as the GK algorithm below) that has a worst-case space requirement of $O(\frac{1}{\epsilon}\log(\epsilon N))$, surpassing the MRL algorithm both theoretically and empirically. The general idea of GK algorithm maintains a sorted subset of data input by the combine and prune operations which keep the precision in control. The space complexity is then improved to $O(\frac{1}{\epsilon}\log^{\frac{3}{2}}\frac{1}{\epsilon})$ by \citeauthor{agarwalMergeableSummaries2013}\cite{agarwalMergeableSummaries2013}, who provide a mergeable sketch based on the GK algorithm.

A deterministic, fixed-universe algorithm \textit{Q-Digest} was designed by \citeauthor{shrivastavaMediansNewAggregation2004}\cite{shrivastavaMediansNewAggregation2004}. It deals with the quantile problem as a histogram problem using $O (\frac{1}{\epsilon}\log U)$ space, where $U$ is the size of the fixed universe from which the input is drawn.


\subsection{Randomised algorithms}
\label{randomised}

Estimating quantile from a random sample of data stream input is the basic idea for most randomized algorithms, and it reduces space usage to a considerable extend.
A simple randomized algorithm designed by \citeauthor{floydExpectedTimeBounds1975}\cite{floydExpectedTimeBounds1975} uses $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon \delta})$ space, for an $\epsilon$-approximate $\phi$-quantile of N elements with probability at least $1 - \delta$, where $0 < \delta < 1$.
Later, \citeauthor{mankuRandomSamplingTechniques1999}\cite{mankuRandomSamplingTechniques1999} proposed a randomized quantile estimation algorithm based on their previous framework in \cite{mankuApproximateMediansOther1998}. The new method requires space of size 
$O(
    \frac{1}{\epsilon} \log^2 
    (\frac{1}{\epsilon} 
        \log \frac{1}{\epsilon \delta}
    )
)
$
, being the first practical algorithm that does not require advance knowledge of data stream size $N$.

The \textit{turnstile model} was first taken into consideration by \citeauthor{gilbertChapter40How2002}\cite{gilbertChapter40How2002}, who designed the \textit{random subset sum} sketch with space complexity 
$O(\frac{1}{\epsilon^2} \log^2 U \log (\frac{\log U}{\epsilon}))$, where $U$ is the size of the universe. 
The key idea is based on their findings that insertions or deletions of quantiles can be reduced to finding range sums.
The space requirement is greatly improved by another algorithm for turnstile model -- the \textit{Count-Min sketch} designed by \citeauthor{cormodeImprovedDataStream2005}\cite{cormodeImprovedDataStream2005}. Only 
$O(\frac{1}{\epsilon} \log^2 N \log (\frac{\log N}{\phi \delta}))$ space is needed for an $\epsilon$-approximate $\phi$-quantile. Although the space requirement is worse than the deterministic algorithms \textit{GK} and \textit{Q-Digest}, it has a noticeable advantage on dealing with both insertions and deletions to streams.

Another significant improvement on randomised algorithms were made in 2017, when \citeauthor{felberRandomizedOnlineQuantile2017}\cite{felberRandomizedOnlineQuantile2017} developed an algorithm that uses $O(\frac{1}{\epsilon}\log \frac{1}{\epsilon})$ of memory space. The algorithm not only beats the previous best upper bound of $O(\frac{1}{\epsilon}\log^{\frac{3}{2}}\frac{1}{\epsilon})$ by \citeauthor{agarwalMergeableSummaries2013}\cite{agarwalMergeableSummaries2013}, but it also beats the lower bound for any deterministic comparison-based algorithm, according to the work of \citeauthor{hungLogSpaceLower2010}\cite{hungLogSpaceLower2010}. The estimation ensures the return of the $\epsilon$-approximate quantile summary, with probability at least $1-e^{-poly(1/\epsilon)}$.


\subsection{Algorithms for other requirements}
\label{other}
For specific estimation accuracy requirements, space and time limitations vary. For example, the biased quantiles problem requires higher accuracy for more extreme quantile values. \citeauthor{cormodeSpaceTimeefficientDeterministic2006} \cite{cormodeSpaceTimeefficientDeterministic2006} propose a deterministic algorithm that takes only space $O(\frac{\log {U}}{\epsilon} \log {\epsilon N})$ for a biased quantiles with $\epsilon$ approximation, where $U$ is the size of universe from which the samples are drawn. 

Manku: Approcimate counts and quantiles over sliding windows

Some other applications require algorithms work for distributed system where big data stream is processed by different processors.
The Streaming Parallel Decision Tree (SPDT) algorithm \cite{ben-haimStreamingParallelDecision2010} introduces an on-line histogram building method % from streaming data at parallel processors.
in which histogram boundaries are estimated quantile values.
In this method, multiple histograms are built from streaming data in parallel, which are then merged into a summary histogram of the entire dataset. The summary histogram is a set of sorted real numbers that represents the interval boundaries such that all the intervals have approximately the same size. Specifically, for a summary histogram with $N$ intervals, the set of real numbers is approximately the set of $\tau$-quantiles ($\tau = \frac{1}{N}, \frac{2}{N}, ..., \frac{N-1}{N}$) for the input data stream.

 It also works well for huge amount of data because the computation complexity is not affected by the size of dataset.
    % This summary histogram is notable for its evenly distributed intervals sizes, as each interval has the same number of data points. To interpret the histogram into quantiles, 

% The stochastic gradient descent algorithm, however, is proposed under the assumption that the si` zze of data stream samples is unknown. \textbf{i dont know what to write for the accuracy/convergence part QAQ}
%----------------------------------------------------------------------------------------
%   Single quantile estimation
%----------------------------------------------------------------------------------------

\pagebreak

\iffalse
\section{Single quantile estimation with non-growing memory}
\label{singlequantile}
Memory storage restrictions from data stream can be stronger for some applications, which asks for algorithms with memory space not affected by sample size. 
\textbf{examples/explanations}
Several algorithms have been proposed to deal with this challenges, and two of them are discussed in detail later in this research.


\textbf{
\citeauthor{mankuApproximateMediansOther1998} \cite{mankuApproximateMediansOther1998} uses $bk$ memories, where $b$ is the number of buffer that each can store $k$ elements.
\\
\citeauthor{dunningComputingExtremelyAccurate2019} \cite{dunningComputingExtremelyAccurate2019} can have constant or weakly growing memory.
}

\citeauthor{maFrugalStreamingEstimating2014}\cite{maFrugalStreamingEstimating2014} introduce the randomized algorithms frugal streaming that requires even less memory.
In Frugal-1U, the algorithm requires only one unit of memory to record the current quantile estimate. On arrival of each sample, the quantile estimate either stays unchanged or changed by a constant value. 
% An improved version is Frugal-2U, which needs another memory unit for a better convergence rate. 

To attack the same problem, \citeauthor{yazidiQuantileEstimationDynamic2016}\cite{yazidiQuantileEstimationDynamic2016} resort to the idea of online learning and achieves a similar method. 
One distinction is step size. In this algorithm, each step the new estimate is updated by a variable related with the current estimate rather than a constant.
Their work is inspired by \citeauthor{tierneySpaceEfficientRecursiveProcedure1983}\cite{tierneySpaceEfficientRecursiveProcedure1983}, who introduce stochastic learning method to quantile estimation problems. 
% In The noticeable similarity and difference between \citeauthor{yazidiQuantileEstimationDynamic2016}'s work and the Frugal-1U is analysed in \textbf{section ??}.  

In \textbf{section ?}, we provide a theoretical analysis and experiment comparisons on the noticeable similarity between the Frugal-1U and \citeauthor{yazidiQuantileEstimationDynamic2016}'s work . 


\pagebreak 

%----------------------------------------------------------------------------------------
%   Multi- quantile estimation
%----------------------------------------------------------------------------------------

\section{Multi-quantile estimation}
\label{multiquantile}
When more quantile numbers are required, for example, two quantiles each identifying the upper and lower outlier of a distribution, the single quantile estimation methods become inefficient and less applicable.
For only a few quantiles, this problem can be solved by running parallel single quantile estimation processes for each quantile. But the issue remains when the number of processes excesses the computer's parallel capacity.
The algorithm which estimates several quantile values in one process, as a general solution to this problem, is then brought forward.
Multiple quantile estimation is the simultaneous estimation on different quantile values from streaming data. 
In general, it is the estimator that estimates $k$ quantiles (the $\tau_1$-, $\tau_2$-, $...$, $\tau_k$-quantiles) at the same time.
Given the relationship between 
It has been an issue targeted by different algorithms.
        % excluded: \cite{pebayFormulasRobustOnepass2008}: online learning, not quantile\\\\
    \subsection{Other Method and Advantages of simultaneous estimation\cite{mcdermottDataSkeletonsSimultaneous2007}}

    Another single-pass low-memory methods for simultaneous multi-quantile estimation is the Data Skeleton(DS)\cite{mcdermottDataSkeletonsSimultaneous2007} algorithm, which is derived from the method proposed by \citeauthor{liechtySinglepassLowstorageArbitrary} \cite{liechtySinglepassLowstorageArbitrary}. For an estimation of $k$ quantiles, the algorithm requires the first $km$ data points($m$ is a constant) being sorted, and updates this tracking array on each new observation. Instead of the $k$ estimates of quantiles, it returns a total of $km$ estimates due to the redundancy of computation. This feature is considered an advantage for certain applications like density estimation, when extra quantile estimates is useful in accuracy improvement.


    Over the comparison with \citeauthor{liechtySinglepassLowstorageArbitrary}'s algorithm, \citeauthor{mcdermottDataSkeletonsSimultaneous2007}\cite{mcdermottDataSkeletonsSimultaneous2007} find simultaneous estimation on multiple quantiles has two main advantages over single quantile estimation methods. First is the save in computation time as it does not need to estimate quantiles separately. The second advantage, according to the experiments, is the accuracy improvement in simultaneous quantile estimation.  


    
\textbf{
    Parallel quantile estimation using Stochastic approximation \cite{hammerSmoothEstimatesMultiple2019}
}

\textbf{P2 algorithm and latter algorithms that based on it}


% \section{Anomaly Detection and Outlier}
\fi