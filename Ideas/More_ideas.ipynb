{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Heuristics for active learning for regression\n",
    "\n",
    "We consider the linear/nonlinear regression settings where we would like to learn a regressor h, which is a function that maps some feature space $\\mathcal{X} \\in \\mathbb{R}^d$ to a probability distribution over a label space $\\mathcal{Y}:$\n",
    "$$h: \\mathcal{X} \\rightarrow p(\\mathcal{Y})$$\n",
    "\n",
    "The goal for active learning to come up with a rule $s(x; h)$ that gives each unlabeled example a score based only on their feature vector $x$ and the current regressor $h$. Recall that the regressor produces $p(\\mathcal{Y})$, a probability estimate for each label. We use these probability estimates from the regressor over the unlabeled examples to calculate the scores:\n",
    "$$s:p(\\mathcal{Y})â†’\\mathbb{R}$$\n",
    "\n",
    "The value of $s(x; h)$ indicates the informativeness of example $x$, where bigger is better. We would then label the example with the largest value of $s(x; h)$. This will be our active learning rule r:\n",
    "$$r(U;h) = {argmax}_{x \\in U} s(x;h)$$\n",
    "where U is the unlabeled set $U \\subset \\mathcal{X}$.  \n",
    "\n",
    "Then the idea is that if $p(\\mathcal{Y})$ can provide not only a probablity for a specific x, but also the variance/error/uncertainty for that x, then we can utilise the uncertainty to indicate the informativeness of example $x$ directly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 1: \n",
    "\n",
    "Consider the Bayesian linear regression setting, we obtain the (posterior) predictive distribution (assume Gaussian prior and likelihood):\n",
    "\n",
    "$$p(y_\\ast | \\mathcal{X,Y}, x_\\ast) = \\int p(y_\\ast | x_ast, \\theta) p(\\theta | \\mathcal{X,Y}) d\\theta = \\mathcal{N} (y_\\ast | \\mu_\\ast, \\sigma_\\ast)$$\n",
    "\n",
    "where $x_\\ast$ is test input and $y_\\ast$ is corresponding prediction. $\\sigma_\\ast$ reflects the posterior uncertainty associated with the parameter $\\theta$ and data noise. The predictive mean $\\mu_\\ast$ coincides with the MAP estimate.\n",
    "\n",
    "Then the active learning rule r could be:\n",
    "\n",
    "$$r_{BLR1} (U; h) = {argmax}_{x \\in U} \\sigma_{\\ast} \\tag{1}$$\n",
    "\n",
    "Further more, say if we care about the unlabeled instances with prediction values around a certain value $m$. We can choose the instances with predictive mean close to $m$ but with relative large uncertainty.\n",
    "\n",
    "$$r_{BLR2} (U; h) = {argmax}_{x \\in U} \\{- (\\mu_\\ast - m)^2 + \\beta \\sigma_{\\ast}\\} \\tag{2}$$\n",
    "\n",
    "where $\\beta$ indicates how much we care about uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 2:\n",
    "\n",
    "Consider the Gaussian Process regression setting, which can be regarded as a special case for Bayesian linear regression setting (with the prior as Gaussian process). According to the assumption in GP modelling, where our data can be represented as a sample from a multivariate Gaussian distribution, we have \n",
    "\n",
    "$$\n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   y  \\\\\n",
    "   y_\\ast  \\\\\n",
    "  \\end{matrix}\n",
    "  \\right] \n",
    "\\sim\n",
    "\\mathcal{N}(0, \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "   K, K_\\ast^T  \\\\\n",
    "   K_\\ast, K_{\\ast\\ast}  \\\\\n",
    "  \\end{matrix}\n",
    "  \\right] )\n",
    "  \\tag{3}\n",
    "$$\n",
    "\n",
    "where K is the covariance matrix for observed data pair (x, x'), $K_\\ast  = [k(x_\\ast, x_1) \\quad k(x_\\ast, x_2) \\quad ... \\quad k(x_\\ast, x_n)]$ and $K_{\\ast\\ast} = k(x_\\ast, x_\\ast)$\n",
    "\n",
    "And we will have a gaussian predictive distribution for the condition probability $p(y_\\ast | y)$:\n",
    "$$p(y_\\ast | y) = \\mathcal{N}(\\mu_\\ast, \\sigma_\\ast)$$\n",
    "\n",
    "The active learning rule is same as Equation (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 3:\n",
    "\n",
    "Consider quantile regression setting, for $x_\\ast$ we can have the median prediction $u_\\ast$ (i.e. 50%), along with the upper bound as q-quantile prediction $q_\\ast$(e.g. 80%) and the lower bound as p-quantile prediction $p_\\ast$(e.g. 20%). Then the active learning rule is:\n",
    "\n",
    "$$r_{QR} (U; h) = {argmax}_{x \\in U} \\{q_{\\ast} - p_\\ast \\tag{4}\\}$$\n",
    "\n",
    "Again if we care about the prediction value around a certain value m, \n",
    "\n",
    "$$r_{QR} (U; h) = {argmax}_{x \\in U} \\{- (u_\\ast - m)^2 + \\beta (q_{\\ast} - p_\\ast)\\} \\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heuristics can be easily extended to the classification setting, as long as the predictive probablity distribution $p(\\mathcal{Y})$ can provide the uncertainty for the unlabelled instances.\n",
    "\n",
    "Since linear regression problem can be well generalized to matrix factorization problems, these heuristics can also be used for tensors (knowledge population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Active learning\n",
    "\n",
    "We already get the informativeness score s(x; h) for all unlabeled instances x. Now we consider what if we want to recommend n instances (n > 1) for one iteration, what will be the active learning rule $r_n(U; h)? We can simply use the top n score as recommendation. However, the top n instances may provide similar information as each other, say we can inferring the label as the one instance by knowing the label of another instance.\n",
    "\n",
    "#### Kmeans++ \n",
    "\n",
    "For one batch we want to recommend m instances to be labelled, then we run kmeans++ algorithm on the top n informative instances and pick m instances which are furthest to each other (the distance should depend on the features).\n",
    "\n",
    "#### Gaussian process\n",
    "\n",
    "As indicated in Equation (3), the similarity for all data points have already been presented by the covirance matrix (kernel function). \n",
    "\n",
    "The first option is to use the approaches in Kmeans++ with the distance between data points as the k(x,x'). We first pick the top 1 instance $x_{1*}$ and then find the instance $x_{t*}$ which has the smallest similarity with $x_{1* :(t-1)*}$ (t >= 2). \n",
    "\n",
    "The second option is:   \n",
    "From all unlabeled set U, we want to recommend m data points. First we pick the top n data points (len(U) > n > m, e.g. n = 30 and m = 10). \n",
    "1. From s(x; h) we get the $\\textbf{Uncertainty Rank}$ for n data points.\n",
    "1. Get the $n \\times n$ matrix $K_n$ with entries as the similarities k(x, x'), then marginalise each row in {K_n} and sort the summation to get the $\\textbf{Distance Rank}$.\n",
    "3. sort the sum of Uncertainty Rank and Distance Rank, get the total rank.\n",
    "\n",
    "Example:\n",
    "\n",
    "|   | 1   | 2   | 3   | Sum | Distance Rank | Uncertainty Rank | Total rank |\n",
    "|---|-----|-----|-----|-----|---------------|------------------|------------|\n",
    "| 1 | 1   | 0.5 | 0.3 | 1.8 | 1             | 3                | 2          |\n",
    "| 2 | 0.5 | 1   | 0.2 | 1.7 | 2             | 1                | 1          |\n",
    "| 3 | 0.3 | 0.2 | 1   | 1.5 | 3             | 2                | 3          |\n",
    "\n",
    "Or instead of mapping the similarity and uncertainty into ranking, we can normalize both distance sum and uncertainty to [0,1] (represented as $\\mathbf{d}$ and $\\mathbf{u}$):\n",
    "\n",
    "$$r(U;h) = {argmax}_{x \\in U} \\{\\mathbf{u} - \\beta \\mathbf{d}\\}$$\n",
    "\n",
    "Then get the top m based that rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
