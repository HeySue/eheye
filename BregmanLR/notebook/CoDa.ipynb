{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this notebook create a new conda environment with:\n",
    "# conda create -n coda_env python=3.6 scipy numpy scikit-bio\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from skbio.stats.composition import *\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$ = number of samples\n",
    "\n",
    "$d$ = number of parts in composition\n",
    "\n",
    "$k$ = number of independent variables (compositions) in regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model\n",
    "\n",
    "Let $V, U^{(1)}, \\ldots, U^{(k)}$ be in $S^d_n.$\n",
    "\n",
    "The regression model is given by\n",
    "\n",
    "$$ \\hat{V} = \\beta_1 \\otimes U^{(1)} \\oplus \\ldots \\oplus \\beta_k \\otimes U^{(k)}$$\n",
    "\n",
    "After ilr-transform, this becomes\n",
    "\n",
    "$$ \\hat{Y} = \\beta_1 \\times X^{(1)} + \\ldots + \\beta_k \\times X^{(k)}$$\n",
    "\n",
    "Note: Since there is no obvious way to include a constant term in this model, H. Wang et al first centralizes the observed data, and un-centers the predicted values to give the final fitted values. \n",
    "\n",
    "The $\\beta = (\\beta_1,\\ldots, b_k)$ which minimizes the sum of squared errors between $Y, \\hat{Y}$ is given in eq (58) (H. Wang et al, 2013).\n",
    "\n",
    "Similarly to eq 9 of (M. Avalos et al, Representation learning of Composition Data, NeurIPS 2018), we can instead consider the gauged-KL loss \n",
    "\n",
    "$$ \\ell = D_{\\exp} \\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W, clr(V) \\right) $$\n",
    "\n",
    "$$ = D_{KL} \\left( \\tilde{V}, \\exp\\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) \\right) $$\n",
    "\n",
    "$$ = ( \\mathbf{1}_{n\\times 1} )^T \\exp\\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) \\mathbf{1}_{d\\times 1} - \\operatorname{trace}\\left( \\tilde{V}^T \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) + const$$\n",
    "\n",
    "Gradient: Let $g$ and $h$ denote the first and second term in the gauged-KL-loss respectively.\n",
    "\n",
    "$$ g(\\beta) = \\sum_{i=1}^d \\sum_{j=1}^n \\exp \\left( \\beta_1 C^{(1)}_{ij} + \\ldots + \\beta_k C^{(k)}_{ij} \\right)$$\n",
    "$$ h(\\beta) = \\operatorname{trace}\\left( \\tilde{V}^T \\left( \\sum_k \\beta_k C^{(k)} \\right)  \\right) = \\sum_k \\beta_k \\operatorname{trace}\\left( \\tilde{V}^T C^{(k)} \\right) $$\n",
    "We have $$ \\frac{ \\partial g}{\\partial \\beta_r } = \\sum_{i=1}^d \\sum_{j=1}^n C^{(r)}_{ij} \\exp \\left( \\beta_1 C^{(1)}_{ij} + \\ldots + \\beta_k C^{(k)}_{ij} \\right)$$\n",
    "\n",
    "$$ \\frac{ \\partial h}{\\partial \\beta_r } = \\operatorname{trace}\\left( \\tilde{V}^T C^{(r)}\\right)$$\n",
    "\n",
    "This loss function is convex in $\\beta$ and we can compute the gradient, so we can find the optimal $\\beta$ by standard methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation - Both the sum of squared errors and the gauged-KL loss produce $\\beta$ which seem to produce fairly similar results under a variety of evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_coeff(V, U_vec):\n",
    "    k = len(U_vec)\n",
    "    V_tilde = np.exp(clr(V))\n",
    "    C_vec = [clr(U) for U in U_vec]\n",
    "    \n",
    "    def kl_loss(beta):\n",
    "        # beta in R^k\n",
    "        ilr_estimator = sum(beta[i] * C_vec[i] for i in range(k))\n",
    "        h = sum(beta[i] * np.trace(V_tilde.T @ C_vec[i]) for i in range(k))\n",
    "        return np.sum(np.exp(ilr_estimator)) - h\n",
    "    \n",
    "    def kl_loss_grad(beta):\n",
    "        ilr_estimator = sum(beta[i] * C_vec[i] for i in range(k))\n",
    "        return np.array([np.sum(C_vec[i] * np.exp(ilr_estimator)) - np.trace(V_tilde.T@C_vec[i]) for i in range(k)])\n",
    "    \n",
    "    return minimize(kl_loss, np.array([0.0]*k), method='BFGS', jac=kl_loss_grad, tol=1e-16,\n",
    "                    options={'gtol': 1e-012, 'disp':False}).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_coeff(V, U_vec):\n",
    "    k = len(U_vec)\n",
    "    Y = ilr(V)\n",
    "    X_vec = [ilr(U) for U in U_vec]\n",
    "    A = np.array([ [np.trace(X_vec[i].T @ X_vec[j]) for i in range(k)] for j in range(k)])\n",
    "    b = np.array( [np.trace(X_vec[i].T@Y) for i in range(k)]).T\n",
    "    return np.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated directly in evaluate_XX_model functions below.\n",
    "#def fitted_outputs(beta, U_vec, V_mean):\n",
    "#    return clr_inv( sum(beta[i]* clr(U_vec[i]) for i in range(len(U_vec))) + clr(V_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_error(V, V_):\n",
    "    return np.sum(np.absolute(V - V_))\n",
    "\n",
    "def L2_error(V, V_):\n",
    "    return np.sum(np.sqrt(np.sum(np.square(V - V_), axis=1)))\n",
    "\n",
    "def Fisher_error(V, V_):\n",
    "    return 2*np.trace(np.arccos(np.sqrt(V)@np.sqrt(V_.T)))\n",
    "\n",
    "def symKL_error(V, V_):\n",
    "    return np.sum((V-V_) * np.log(np.divide(V, V_)))\n",
    "\n",
    "def report_evaluation(V_obs, V_fit):\n",
    "    print(\"Fisher-Rao:   \", Fisher_error(V_obs, V_fit))\n",
    "    print(\"Symmetric KL: \", symKL_error(V_obs, V_fit))\n",
    "    print(\"L2 error:     \", L2_error(V_obs, V_fit))\n",
    "    print(\"L1 error:     \", L1_error(V_obs, V_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions 'eval_XX_model' employs a train/test split as usual and is what we use in evaluating the effectiveness of the models. To reproduce the results in H. Wang (2013) where they both fit and test their model over the entire dataset, we can use 'eval_XX_model(V, V, U_vec, U_vec)'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_KL_model(V_train, V_test, U_train_vec, U_test_vec, center_data = True):\n",
    "    k = len(U_train_vec)\n",
    "    if center_data:\n",
    "        V_train_centered = centralize(V_train)\n",
    "        V_train_mean = scipy.stats.gmean(V_train, axis=0)\n",
    "        U_train_centered = [centralize(U) for U in U_train_vec]\n",
    "        U_test_centered = [centralize(U) for U in U_test_vec]\n",
    "        \n",
    "        beta_KL = KL_coeff(V_train_centered, U_train_centered)\n",
    "        V_prediction = clr_inv( sum(beta_KL[i]* clr(U_test_centered[i]) for i in range(k)) \n",
    "                               + clr(np.array([V_train_mean]*V_test.shape[0])) )\n",
    "        report_evaluation(V_test, V_prediction)\n",
    "    else: # Do not center the data\n",
    "        beta_KL = KL_coeff(V_train, U_train_vec)\n",
    "        V_prediction = clr_inv( sum(beta_KL[i]* clr(U_test_vec[i]) for i in range(k)))\n",
    "        report_evaluation(V_test, V_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_L2_model(V_train, V_test, U_train_vec, U_test_vec, center_data = True):\n",
    "    k = len(U_train_vec)\n",
    "    if center_data:\n",
    "        V_train_centered = centralize(V_train)\n",
    "        V_train_mean = scipy.stats.gmean(V_train, axis=0)\n",
    "        U_train_centered = [centralize(U) for U in U_train_vec]\n",
    "        U_test_centered = [centralize(U) for U in U_test_vec]\n",
    "        \n",
    "        beta_L2 = L2_coeff(V_train_centered, U_train_centered)\n",
    "        V_prediction = clr_inv( sum(beta_L2[i]* clr(U_test_centered[i]) for i in range(k)) \n",
    "                               + clr(np.array([V_train_mean]*V_test.shape[0])) )\n",
    "        report_evaluation(V_test, V_prediction)\n",
    "    else: # Do not center the data\n",
    "        beta_L2 = L2_coeff(V_train, U_train_vec)\n",
    "        V_prediction = clr_inv( sum(beta_L2[i]* clr(U_test_vec[i]) for i in range(k)))\n",
    "        report_evaluation(V_test, V_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of results below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It generally appears that the L2 model performs slightly better than the KL model when we center-uncenter the data, and that the KL model slightly outperforms the L2 model when we only use the raw data. \n",
    "\n",
    "For the real world datasets (the first two), the Centered models greatly outperform the Uncentered models.\n",
    "Very strangely, for the artifical dataset created below the Uncentered models appears to greatly outperform the Centered models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Data from (H. Wang et al, Multiple linear regression for Compositional Data, 2013)\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0925231213005808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np. array([\n",
    "       [0.024, 0.568, 0.408],\n",
    "       [0.023, 0.540, 0.437],\n",
    "       [0.021, 0.516, 0.463],\n",
    "       [0.019, 0.493, 0.488],\n",
    "       [0.018, 0.474, 0.508],\n",
    "       [0.016, 0.463, 0.521],\n",
    "       [0.015, 0.461, 0.524],\n",
    "       [0.014, 0.457, 0.529],\n",
    "       [0.012, 0.479, 0.509],\n",
    "       [0.010, 0.482, 0.508],\n",
    "       [0.010, 0.474, 0.516],\n",
    "       [0.009, 0.470, 0.521],\n",
    "       [0.008, 0.446, 0.546],\n",
    "       [0.008, 0.432, 0.560],\n",
    "       [0.007, 0.399, 0.594],\n",
    "       [0.007, 0.420, 0.573]])\n",
    "\n",
    "U1 = np.array([\n",
    "       [0.09853, 0.54467, 0.35680],\n",
    "       [0.12044, 0.52255, 0.35701],\n",
    "       [0.12708, 0.49105, 0.38187],\n",
    "       [0.12443, 0.46027, 0.41530],\n",
    "       [0.11414, 0.46458, 0.42128],\n",
    "       [0.10772, 0.44310, 0.44918],\n",
    "       [0.11004, 0.39875, 0.49121],\n",
    "       [0.10154, 0.39671, 0.50175],\n",
    "       [0.08626, 0.40765, 0.50609],\n",
    "       [0.06878, 0.45352, 0.47770],\n",
    "       [0.06296, 0.42429, 0.51275],\n",
    "       [0.05504, 0.41676, 0.52820],\n",
    "       [0.05243, 0.41251, 0.53505],\n",
    "       [0.04688, 0.40272, 0.55040],\n",
    "       [0.05119, 0.37391, 0.57490],\n",
    "       [0.03930, 0.37573, 0.58496]])\n",
    "\n",
    "U2 = np.array([\n",
    "       [0.00350, 0.29244, 0.70406],\n",
    "       [0.00380, 0.30635, 0.68985],\n",
    "       [0.00192, 0.30276, 0.69531],\n",
    "       [0.00117, 0.31051, 0.68832],\n",
    "       [0.00297, 0.30802, 0.68901],\n",
    "       [0.00308, 0.28961, 0.70732],\n",
    "       [0.00377, 0.30051, 0.69573],\n",
    "       [0.00264, 0.30085, 0.69651],\n",
    "       [0.00192, 0.29305, 0.70503],\n",
    "       [0.00078, 0.30147, 0.69775],\n",
    "       [0.00176, 0.25639, 0.74185],\n",
    "       [0.00420, 0.26713, 0.72867],\n",
    "       [0.00220, 0.25783, 0.73997],\n",
    "       [0.00209, 0.24018, 0.75773],\n",
    "       [0.00235, 0.22705, 0.77060],\n",
    "       [0.00360, 0.24734, 0.74906]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01268806, 0.47162267, 0.51065353])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking V_mean as stated in H.Wang2013 is correct.\n",
    "# NOTE - This differs slightly from the value published in their paper:\n",
    "# [0.01275, 0.47401, 0.51324]\n",
    "# Potentially an error in their calculations\n",
    "scipy.stats.gmean(V, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[::2], V[1::2]\n",
    "U1_train, U1_test = U1[::2], U1[1::2]\n",
    "U2_train, U2_test = U2[::2], U2[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    0.3938388560361861\n",
      "Symmetric KL:  0.0240752766083486\n",
      "L2 error:      0.25747135004291116\n",
      "L1 error:      0.3722438869854835\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, [U1_train, U2_train], [U1_test, U2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    0.38970540860620667\n",
      "Symmetric KL:  0.023850161075651598\n",
      "L2 error:      0.2555180413915952\n",
      "L1 error:      0.37014941154737036\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, [U1_train, U2_train], [U1_test, U2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    1.141862963697338\n",
      "Symmetric KL:  0.20761975708178956\n",
      "L2 error:      0.7489706273498107\n",
      "L1 error:      1.0825563215510952\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, [U1_train, U2_train], [U1_test, U2_test], center_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    1.1966443661220107\n",
      "Symmetric KL:  0.23603413243385934\n",
      "L2 error:      0.8081024870243325\n",
      "L1 error:      1.157215326120599\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, [U1_train, U2_train], [U1_test, U2_test], center_data = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D17 Aitchison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 17 Aitchison (with V[13] adjusted due to error in book)\n",
    "V = np.array([\n",
    "[0.27,0.28,0.45],\n",
    "[0.02,0.03,0.95],\n",
    "[0.12,0.16,0.72],\n",
    "[0.83,0.02,0.15],\n",
    "[0.24,0.22,0.54],\n",
    "[0.16,0.20,0.64],\n",
    "[0.31,0.08,0.61],\n",
    "[0.05,0.85,0.10],\n",
    "[0.06,0.06,0.88],\n",
    "[0.08,0.31,0.61],\n",
    "[0.18,0.20,0.62],\n",
    "[0.17,0.19,0.64],\n",
    "[0.04,0.17,0.79],\n",
    "[0.08,0.25,0.67],\n",
    "[0.11,0.34,0.55]])\n",
    "\n",
    "U1 = np.array([\n",
    "[0.70,0.07,0.23],\n",
    "[0.19,0.16,0.65],\n",
    "[0.18,0.26,0.54],\n",
    "[0.02,0.02,0.96],\n",
    "[0.08,0.16,0.76],\n",
    "[0.14,0.18,0.68],\n",
    "[0.16,0.11,0.73],\n",
    "[0.04,0.06,0.90],\n",
    "[0.06,0.54,0.40],\n",
    "[0.12,0.22,0.66],\n",
    "[0.06,0.02,0.92],\n",
    "[0.16,0.04,0.80],\n",
    "[0.27,0.17,0.56],\n",
    "[0.21,0.51,0.28],\n",
    "[0.15,0.15,0.70] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[:12], V[12:]\n",
    "U1_train, U1_test = U1[:12], U1[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    1.243822297977255\n",
      "Symmetric KL:  0.534509779806563\n",
      "L2 error:      0.5575393699095036\n",
      "L1 error:      0.8881891786261503\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, [U1_train], [U1_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    1.1716720959817062\n",
      "Symmetric KL:  0.4766113979085447\n",
      "L2 error:      0.5141937028565229\n",
      "L1 error:      0.7943933063157911\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, [U1_train], [U1_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    2.087125906258782\n",
      "Symmetric KL:  1.6253037903519312\n",
      "L2 error:      1.1026764719736475\n",
      "L1 error:      1.7690094548744115\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, [U1_train], [U1_test], center_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    2.090729162208426\n",
      "Symmetric KL:  1.6301387682323807\n",
      "L2 error:      1.1049098924621183\n",
      "L1 error:      1.7726035913451252\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, [U1_train], [U1_test], center_data = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial  Dataset\n",
    "\n",
    "Created by generating a dataset in $\\mathbb{R}^{d-1}$ and then using the inverse ilr map to produce compositional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d, k = 30, 10, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_vec = [ ilr_inv(np.random.randn(n,d-1)) for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_true = [(-1)**j * 0.5 * k for j in range(1,k+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = ilr_inv(sum( beta_true[i] * ilr(U_vec[i]) for i in range(k)) + 0.2* np.random.randn(n,d-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[:20], V[20:]\n",
    "U_train_vec = [U[:20] for U in U_vec]\n",
    "U_test_vec = [U[20:] for U in U_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    8.799269154289348\n",
      "Symmetric KL:  11.838319859552247\n",
      "L2 error:      3.610562371001399\n",
      "L1 error:      5.976266269180193\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    8.804243155122535\n",
      "Symmetric KL:  11.802893034877115\n",
      "L2 error:      3.6160491777625943\n",
      "L1 error:      5.981286409423493\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    0.7251876828898074\n",
      "Symmetric KL:  0.06514220981605431\n",
      "L2 error:      0.2840975782455637\n",
      "L1 error:      0.48217691582155114\n"
     ]
    }
   ],
   "source": [
    "eval_KL_model(V_train, V_test, U_train_vec, U_test_vec, center_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fisher-Rao:    0.8254282569572294\n",
      "Symmetric KL:  0.08732074352649755\n",
      "L2 error:      0.34703830889367143\n",
      "L1 error:      0.5673855557828801\n"
     ]
    }
   ],
   "source": [
    "eval_L2_model(V_train, V_test, U_train_vec, U_test_vec, center_data = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skbio_env]",
   "language": "python",
   "name": "conda-env-skbio_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
