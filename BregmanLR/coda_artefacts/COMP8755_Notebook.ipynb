{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP8755 Project Artefacts\n",
    "## Ragib Zaman\n",
    "## u6341578\n",
    "\n",
    "This notebook (and several helper python files) supplements the report submitted for COMP8755. References cited here are listed in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement 8 methods of performing Aitchion Simplex Regression. Aitchison Simplex Regression is the problem of finding a function which takes as input $m$ independent compositional variables and estimates a target compositional variable, where all of the compositional variables lie within the same Aitchision simplex $S^d.$ \n",
    "\n",
    "We generally maintain the following notation:\n",
    "\n",
    "$n$ = number of samples, indexed by $i$\n",
    "\n",
    "$d$ = number of parts in the compositions, index by $j$\n",
    "\n",
    "$m$ = number of independent compositional variables, indexed by $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for Aitchision Simplex Regression\n",
    "\n",
    "We are interesting in finding functions $f$ that take $m$ d-part compositions $u_1,\\ldots, u_m \\in S^d$ and return an estimate of a corresponding target d-part composition $v\\in S^d.$\n",
    "\n",
    "Wang et al. (2013, https://www.sciencedirect.com/science/article/pii/S0925231213005808) proposed the following model for this problem:\n",
    "\n",
    "$$ f( u_1, \\ldots, u_m) = \\bigoplus_{k=1}^m \\beta_k \\otimes u_k$$\n",
    "\n",
    "They provide a closed form solution for the parameters $\\beta_k$ which minimize the sum of the squared Aitchison distances between the model predictions $f(\\ldots)$ and the true labels $v,$ which is equivalent to minimizing the sum of the squared L2 distances between the ilr-transformed coordinates. Their model does not include a bias term, but instead they first centralize all of their data and apply a corresponding uncentering to produce their final predictions. \n",
    "\n",
    "We considered 7 new algorithms for this problem. First, we consider models which include a bias term $\\beta_0 \\in S^d$ in addition to the real parameters $\\beta_1,\\ldots \\beta_m:$\n",
    "\n",
    "$$f( u_1, \\ldots, u_m) = \\beta_0 \\ \\oplus \\ \\bigoplus_{k=1}^m \\beta_k \\otimes u_k$$\n",
    "\n",
    "Second, in addition to the L2 loss we also consider a KL-loss function motivated by arguments in a similar vein to Avalos-Fernandez et al. (2018). For example, in the unbiased case the loss function takes the form\n",
    "\n",
    "$$ \\ell = D_{\\exp} \\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W, clr(V) \\right) $$\n",
    "\n",
    "$$ = D_{KL} \\left( \\tilde{V}, \\exp\\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) \\right) $$\n",
    "\n",
    "$$ = ( \\mathbf{1}_{n\\times 1} )^T \\exp\\left( \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) \\mathbf{1}_{d\\times 1} - \\operatorname{trace}\\left( \\tilde{V}^T \\left( \\sum_k \\beta_k X^{(k)} \\right) W \\right) + const$$\n",
    "\n",
    "which is a convex function in the parameters $\\beta_k.$ Our report contains a computation of the gradient for this loss function, so we can find the desired parameters efficiently through out-of-the-box first-order optimisation procedures. In our implementation we have used Scipy's BFGS optimiser. \n",
    "\n",
    "Finally, we consider the the option of applying or not applying the centering-uncentering procedure. \n",
    "\n",
    "The options of (no bias / bias) , (L2 loss / KL loss) and (center-uncenter / no-centering) forms a total of 8 possibilities. Wang et al. (2013) studied the case where the first choice was chosen for these 3 options. \n",
    "\n",
    "To evaluate these models, we applied them to several datasets and measured their accuracy through 4 metrics. The Fisher-Rao and Symmetric KL metrics are considered to be more 'geometric' from the viewpoint of information geometry (Amari, 2016) and apply between to points on the Aitchison simplex $S^d.$ Forgetting the compositional structure and simply regarding the compositions as points in the ambient space $\\mathbb{R}^d,$ we can also compute the L2 and L1 metrics, which are less 'geometric'. A discussion of the results printed below is contained in the report.\n",
    "\n",
    "\n",
    "Please refer to the report for more background on the models, transformations, loss functions, gradient computations etc. used and/or implemented before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this notebook create a new conda environment with:\n",
    "# conda create -n coda_env python=3.6 scipy numpy scikit-bio\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from skbio.stats.composition import *\n",
    "from skbio.stats.composition import _gram_schmidt_basis\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import compositional_datasets as cd\n",
    "from error_metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets of $n$ samples of targets in $S^d$ as expressed as n x d matrices V. U_vec is a list of length m, where the k-th entry is the size n x d data matrix of the k-th independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model coefficients\n",
    "The following function computes Wang et al's (2013) closed form solution for the parameters $\\beta_k$ which minimizes the L2 error of their model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_coeff(V, U_vec):\n",
    "    m = len(U_vec)\n",
    "    Y = ilr(V)\n",
    "    X_vec = [ilr(U) for U in U_vec]\n",
    "    A = np.array([ [np.trace(X_vec[i].T @ X_vec[j]) for i in range(m)] for j in range(m)])\n",
    "    b = np.array( [np.trace(X_vec[k].T@Y) for k in range(m)]).T\n",
    "    return np.linalg.solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions compute the parameters which minimize the KL-loss for an unbiased and biased model respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_coeff(V, U_vec):\n",
    "    m = len(U_vec)\n",
    "    V_tilde = np.exp(clr(V))\n",
    "    C_vec = [clr(U) for U in U_vec]\n",
    "    \n",
    "    def kl_loss(beta):\n",
    "        # beta in R^m\n",
    "        clr_estimator = sum(beta[k] * C_vec[k] for k in range(m))\n",
    "        g = np.sum(np.exp(clr_estimator))\n",
    "        h = sum(beta[k] * np.trace(V_tilde.T @ C_vec[k]) for k in range(m))\n",
    "        return g - h\n",
    "    \n",
    "    def kl_loss_grad(beta):\n",
    "        clr_estimator = sum(beta[k] * C_vec[k] for k in range(m))\n",
    "        g_grad = np.array([np.sum(C_vec[k] * np.exp(clr_estimator)) for k in range(m)])\n",
    "        h_grad = np.array([np.trace(V_tilde.T@C_vec[k]) for k in range(m)])\n",
    "        return g_grad - h_grad\n",
    "\n",
    "    return minimize(kl_loss, np.array([0.0]*m), method='BFGS', jac=kl_loss_grad, tol=1e-16,\n",
    "                    options={'gtol': 1e-012, 'disp':False}).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biasKL_coeff(V, U_vec):\n",
    "    m = len(U_vec)\n",
    "    V_tilde = np.exp(clr(V))\n",
    "    C_vec = [clr(U) for U in U_vec]\n",
    "    n, d = V.shape\n",
    "    W = _gram_schmidt_basis(d)\n",
    "    \n",
    "    def bias_kl_loss(params):\n",
    "        #params =(alpha, beta), length d-1+m\n",
    "        # alpha in R^(d-1)\n",
    "        # beta in R^m\n",
    "        params = params.ravel() # Scipy's minimize method can return params as [[a,b,c]], so make it [a,b,c]\n",
    "        alpha = params[:d-1].reshape((1,d-1))\n",
    "        beta = params[d-1:]\n",
    "        bias_term = np.ones((n,1)) @ alpha @ W\n",
    "        clr_estimator = bias_term + sum(beta[k] * C_vec[k] for k in range(m))\n",
    "        g = np.sum(np.exp(clr_estimator))\n",
    "        h = alpha @ W @ V_tilde.T @ np.ones((n,1)) + sum(beta[k] * np.trace(V_tilde.T @ C_vec[k]) for k in range(m))\n",
    "        return g - h\n",
    "    \n",
    "    def bias_kl_loss_grad(params):\n",
    "        params = params.ravel() # Scipy's minimize method can return params as [[a,b,c]], so make it [a,b,c] \n",
    "        alpha = params[:d-1].reshape((1,d-1))\n",
    "        beta = params[d-1:]\n",
    "        bias_term = np.ones((n,1)) @ alpha @ W\n",
    "        clr_estimator = bias_term + sum(beta[k] * C_vec[k] for k in range(m))\n",
    "        \n",
    "        g_beta_grad = np.array([np.sum(C_vec[k] * np.exp(clr_estimator)) for k in range(m)])\n",
    "        h_beta_grad = np.array([np.trace(V_tilde.T@C_vec[k]) for k in range(m)])\n",
    "        \n",
    "        g_alpha_grad = np.array([np.sum((np.ones((n,1)) @ W[p].reshape((1,d))) * np.exp(clr_estimator)) for p in range(d-1)])\n",
    "        h_alpha_grad = (W @ V_tilde.T @ np.ones((n,1))).T.reshape((d-1,))\n",
    "        \n",
    "        g_grad = np.concatenate([g_alpha_grad, g_beta_grad])\n",
    "        h_grad = np.concatenate([h_alpha_grad, h_beta_grad])\n",
    "        \n",
    "        return g_grad - h_grad\n",
    "    \n",
    "    return minimize(bias_kl_loss, np.array([0.0]*(d-1+m)), method='BFGS', jac=bias_kl_loss_grad, tol=1e-16,\n",
    "                    options={'gtol': 1e-012, 'disp':False}).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "The following function trains on training data and then on test data evaluates Wang. et al's (2013) model on our 4 chosen metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(coeff_func, V_train, V_test, U_train_vec, U_test_vec, center_data = True):\n",
    "    m = len(U_train_vec)\n",
    "    if center_data:\n",
    "        V_train_centered = centralize(V_train)\n",
    "        V_train_mean = closure(scipy.stats.gmean(V_train, axis=0))\n",
    "        U_train_centered = [centralize(U) for U in U_train_vec]\n",
    "        U_test_centered = [centralize(U) for U in U_test_vec]\n",
    "        \n",
    "        beta = coeff_func(V_train_centered, U_train_centered)\n",
    "        V_prediction = clr_inv( sum(beta[k]* clr(U_test_centered[k]) for k in range(m)) \n",
    "                               + clr(np.array([V_train_mean]*V_test.shape[0])) )\n",
    "        report_evaluation(V_test, V_prediction)\n",
    "    else: # Do not center the data\n",
    "        beta = coeff_func(V_train, U_train_vec)\n",
    "        V_prediction = clr_inv( sum(beta[k]* clr(U_test_vec[k]) for k in range(m)))\n",
    "        report_evaluation(V_test, V_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains on training data and evaluates the biased models the KL-loss on our 4 metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_biased_model(coeff_func, V_train, V_test, U_train_vec, U_test_vec, center_data = True):\n",
    "    m = len(U_train_vec)\n",
    "    n, d = V_test.shape\n",
    "    W = _gram_schmidt_basis(d)\n",
    "    \n",
    "    if center_data:\n",
    "        V_train_centered = centralize(V_train)\n",
    "        V_train_mean = closure(scipy.stats.gmean(V_train, axis=0))\n",
    "        U_train_centered = [centralize(U) for U in U_train_vec]\n",
    "        U_test_centered = [centralize(U) for U in U_test_vec]\n",
    "        \n",
    "        params = coeff_func(V_train_centered, U_train_centered)\n",
    "        alpha = params[:d-1].reshape((1,d-1))\n",
    "        \n",
    "        print('\\nbias : ', alpha[0], '\\n')\n",
    "        \n",
    "        beta = params[d-1:]\n",
    "        \n",
    "        bias_term = np.ones((n,1)) @ alpha @ W\n",
    "        V_prediction = clr_inv( bias_term + sum(beta[k] * clr(U_test_centered[k]) for k in range(m))\n",
    "                                 + clr(np.array([V_train_mean]*V_test.shape[0]))  )\n",
    "        \n",
    "        report_evaluation(V_test, V_prediction)\n",
    "    else: # Do not center the data\n",
    "        params = coeff_func(V_train, U_train_vec)\n",
    "        alpha = params[:d-1].reshape((1,d-1))\n",
    "        \n",
    "        print('\\nbias : ', alpha[0], '\\n')\n",
    "        \n",
    "        beta = params[d-1:]\n",
    "        bias_term = np.ones((n,1)) @ alpha @ W\n",
    "        \n",
    "        V_prediction = clr_inv(bias_term + sum(beta[k]* clr(U_test_vec[k]) for k in range(m)))\n",
    "        report_evaluation(V_test, V_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data trains on training data and evaluates a model with the L2 loss on our 4 metrics. Note that if we do not include a bias term in this model (i.e. set fit_bias = False), then this model is identical to Wang et al's (2013) model. We have used this as a check of correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_L2_model(V_train, V_test, U_train_vec, U_test_vec, center_data = True, fit_bias = True):\n",
    "    m = len(U_train_vec)\n",
    "    n, d = V_test.shape\n",
    "    \n",
    "    if center_data:\n",
    "        V_train_centered = centralize(V_train)\n",
    "        V_train_mean = closure(scipy.stats.gmean(V_train, axis=0))\n",
    "        U_train_centered = [centralize(U) for U in U_train_vec]\n",
    "        U_test_centered = [centralize(U) for U in U_test_vec]\n",
    "        \n",
    "        Y_train = ilr(V_train_centered)\n",
    "        X_train_vec = [ilr(U) for U in U_train_centered]\n",
    "        X_test_vec = [ilr(U) for U in U_test_centered]\n",
    "\n",
    "        if fit_bias:\n",
    "            #The following prepadding of the X_vec allows us to learn the bias term\n",
    "            n_train = V_train.shape[0]\n",
    "            train_bias_pad = [np.zeros((n_train,d-1)) for p in range(d-1)]\n",
    "            for p, M in enumerate(train_bias_pad):\n",
    "                M[:,p]=1\n",
    "\n",
    "            X_train_vec = train_bias_pad + X_train_vec\n",
    "\n",
    "            test_bias_pad = [np.zeros((n,d-1)) for p in range(d-1)]\n",
    "            for p, M in enumerate(test_bias_pad):\n",
    "                M[:,p]=1\n",
    "            X_test_vec = test_bias_pad + X_test_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        y_train = Y_train.ravel()\n",
    "        X_train = np.array([X.ravel() for X in X_train_vec]).T\n",
    "\n",
    "        X_test = np.array([X.ravel() for X in X_test_vec]).T\n",
    "\n",
    "        reg = LinearRegression(fit_intercept = False).fit(X_train, y_train)\n",
    "        if fit_bias:\n",
    "            print('\\nbias : ', reg.coef_[:d-1], '\\n')\n",
    "        ilr_predictions = reg.predict(X_test).reshape((n,d-1)) + ilr(np.array([V_train_mean]*V_test.shape[0]))\n",
    "        V_prediction = ilr_inv(ilr_predictions)\n",
    "\n",
    "        report_evaluation(V_test, V_prediction)\n",
    "        \n",
    "    else: # Do not center the data\n",
    "        Y_train = ilr(V_train)\n",
    "        X_train_vec = [ilr(U) for U in U_train_vec]\n",
    "        X_test_vec = [ilr(U) for U in U_test_vec]\n",
    "        \n",
    "        \n",
    "        if fit_bias:\n",
    "            #The following prepadding of the X_vec allows us to learn the bias term\n",
    "            n_train = V_train.shape[0]\n",
    "            train_bias_pad = [np.zeros((n_train,d-1)) for p in range(d-1)]\n",
    "            for p, M in enumerate(train_bias_pad):\n",
    "                M[:,p]=1\n",
    "\n",
    "            X_train_vec = train_bias_pad + X_train_vec\n",
    "\n",
    "            test_bias_pad = [np.zeros((n,d-1)) for p in range(d-1)]\n",
    "            for p, M in enumerate(test_bias_pad):\n",
    "                M[:,p]=1\n",
    "            X_test_vec = test_bias_pad + X_test_vec\n",
    "\n",
    "            \n",
    "            \n",
    "        y_train = Y_train.ravel()\n",
    "        X_train = np.array([X.ravel() for X in X_train_vec]).T\n",
    "\n",
    "        X_test = np.array([X.ravel() for X in X_test_vec]).T\n",
    "\n",
    "        reg = LinearRegression(fit_intercept = False).fit(X_train, y_train)\n",
    "        if fit_bias:\n",
    "            print('\\nbias : ', reg.coef_[:d-1], '\\n')\n",
    "        ilr_predictions = reg.predict(X_test).reshape((n,d-1))\n",
    "        V_prediction = ilr_inv(ilr_predictions)\n",
    "\n",
    "        report_evaluation(V_test, V_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all(V_train, V_test, U_train_vec, U_test_vec):\n",
    "    print('=========    CENTERED    =========')\n",
    "    print('\\n-----  Unbiased  -----\\n')\n",
    "    \n",
    "    print('----L2----\\n')\n",
    "    eval_model(L2_coeff, V_train, V_test, U_train_vec, U_test_vec)\n",
    "    \n",
    "    print('\\n----KL----\\n')\n",
    "    eval_model(KL_coeff, V_train, V_test, U_train_vec, U_test_vec)\n",
    "    \n",
    "    print('\\n------  Biased  ------\\n')\n",
    "    print('----L2----')\n",
    "    eval_L2_model(V_train, V_test, U_train_vec, U_test_vec)\n",
    "    print('\\n----KL----')\n",
    "    eval_biased_model(biasKL_coeff, V_train, V_test, U_train_vec, U_test_vec)\n",
    "    \n",
    "    print('\\n\\n=======    NOT CENTERED    =======')\n",
    "    print('\\n-----  Unbiased  -----\\n')\n",
    "    \n",
    "    print('----L2----]n')\n",
    "    eval_model(L2_coeff, V_train, V_test, U_train_vec, U_test_vec, center_data = False)\n",
    "    \n",
    "    print('\\n----KL----\\n')\n",
    "    eval_model(KL_coeff, V_train, V_test, U_train_vec, U_test_vec, center_data = False)\n",
    "    \n",
    "    print('\\n------  Biased  ------\\n')\n",
    "    print('----L2----')\n",
    "    eval_L2_model(V_train, V_test, U_train_vec, U_test_vec, center_data = False)\n",
    "    print('\\n----KL----')\n",
    "    eval_biased_model(biasKL_coeff, V_train, V_test, U_train_vec, U_test_vec, center_data = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Data from (H. Wang et al, Multiple linear regression for Compositional Data, 2013)\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0925231213005808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, U1, U2 = cd.Wang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[::2], V[1::2]\n",
    "U1_train, U1_test = U1[::2], U1[1::2]\n",
    "U2_train, U2_test = U2[::2], U2[1::2]\n",
    "U_train_vec = [U1_train, U2_train]\n",
    "U_test_vec = [U1_test, U2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========    CENTERED    =========\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----\n",
      "\n",
      "Fisher-Rao:   0.38971\n",
      "Symmetric KL: 0.02385\n",
      "L2 error:     0.25552\n",
      "L1 error:     0.37015\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   0.39384\n",
      "Symmetric KL: 0.02408\n",
      "L2 error:     0.25747\n",
      "L1 error:     0.37224\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-2.34549485e-16  1.11022302e-16] \n",
      "\n",
      "Fisher-Rao:   0.38971\n",
      "Symmetric KL: 0.02385\n",
      "L2 error:     0.25552\n",
      "L1 error:     0.37015\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [0.01081073 0.00458697] \n",
      "\n",
      "Fisher-Rao:   0.39469\n",
      "Symmetric KL: 0.02389\n",
      "L2 error:     0.25629\n",
      "L1 error:     0.37141\n",
      "\n",
      "\n",
      "=======    NOT CENTERED    =======\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----]n\n",
      "Fisher-Rao:   1.19664\n",
      "Symmetric KL: 0.23603\n",
      "L2 error:     0.80810\n",
      "L1 error:     1.15722\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   1.14186\n",
      "Symmetric KL: 0.20762\n",
      "L2 error:     0.74897\n",
      "L1 error:     1.08256\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-1.04067439 -0.50696972] \n",
      "\n",
      "Fisher-Rao:   0.37744\n",
      "Symmetric KL: 0.02114\n",
      "L2 error:     0.25523\n",
      "L1 error:     0.36988\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [-1.22145815 -0.63834078] \n",
      "\n",
      "Fisher-Rao:   0.29758\n",
      "Symmetric KL: 0.01365\n",
      "L2 error:     0.19894\n",
      "L1 error:     0.28965\n"
     ]
    }
   ],
   "source": [
    "eval_all(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D17 Aitchison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 17 Aitchison (with V[13] adjusted due to error in book)\n",
    "V, U1 = cd.A17_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[:12], V[12:]\n",
    "U1_train, U1_test = U1[:12], U1[12:]\n",
    "U_train_vec = [U1_train]\n",
    "U_test_vec = [U1_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========    CENTERED    =========\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----\n",
      "\n",
      "Fisher-Rao:   1.17167\n",
      "Symmetric KL: 0.47661\n",
      "L2 error:     0.51419\n",
      "L1 error:     0.79439\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   1.24382\n",
      "Symmetric KL: 0.53451\n",
      "L2 error:     0.55754\n",
      "L1 error:     0.88819\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-3.36387723e-17  2.77555756e-17] \n",
      "\n",
      "Fisher-Rao:   1.17167\n",
      "Symmetric KL: 0.47661\n",
      "L2 error:     0.51419\n",
      "L1 error:     0.79439\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [0.06051754 0.20292682] \n",
      "\n",
      "Fisher-Rao:   1.44485\n",
      "Symmetric KL: 0.72773\n",
      "L2 error:     0.66173\n",
      "L1 error:     1.06088\n",
      "\n",
      "\n",
      "=======    NOT CENTERED    =======\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----]n\n",
      "Fisher-Rao:   2.09073\n",
      "Symmetric KL: 1.63014\n",
      "L2 error:     1.10491\n",
      "L1 error:     1.77260\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   2.08713\n",
      "Symmetric KL: 1.62530\n",
      "L2 error:     1.10268\n",
      "L1 error:     1.76901\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-0.00201023 -1.31974866] \n",
      "\n",
      "Fisher-Rao:   1.16504\n",
      "Symmetric KL: 0.46014\n",
      "L2 error:     0.51615\n",
      "L1 error:     0.80451\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [ 0.07677281 -1.97680232] \n",
      "\n",
      "Fisher-Rao:   1.35250\n",
      "Symmetric KL: 0.67391\n",
      "L2 error:     0.61189\n",
      "L1 error:     0.93279\n"
     ]
    }
   ],
   "source": [
    "eval_all(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDP vs Employment by Sector for 158 Countries\n",
    "### Source: en.wikipedia.org/wiki/List_of_countries_by_GDP_sector_composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, U1 = cd.GDPwiki_data\n",
    "V_train, V_test = V[:120], V[120:]\n",
    "U1_train, U1_test = U1[:120], U1[120:]\n",
    "U_train_vec = [U1_train]\n",
    "U_test_vec = [U1_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========    CENTERED    =========\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----\n",
      "\n",
      "Fisher-Rao:   9.79281\n",
      "Symmetric KL: 3.92245\n",
      "L2 error:     4.87165\n",
      "L1 error:     7.52357\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   9.86570\n",
      "Symmetric KL: 3.98067\n",
      "L2 error:     4.91057\n",
      "L1 error:     7.59648\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-6.62252891e-16 -2.22044605e-16] \n",
      "\n",
      "Fisher-Rao:   9.79281\n",
      "Symmetric KL: 3.92245\n",
      "L2 error:     4.87165\n",
      "L1 error:     7.52357\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [0.00116505 0.01462628] \n",
      "\n",
      "Fisher-Rao:   9.91331\n",
      "Symmetric KL: 3.99530\n",
      "L2 error:     4.94072\n",
      "L1 error:     7.64811\n",
      "\n",
      "\n",
      "=======    NOT CENTERED    =======\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----]n\n",
      "Fisher-Rao:   17.09993\n",
      "Symmetric KL: 11.02420\n",
      "L2 error:     8.54415\n",
      "L1 error:     13.52680\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   17.15592\n",
      "Symmetric KL: 11.22285\n",
      "L2 error:     8.63693\n",
      "L1 error:     13.69218\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [-0.86329099 -0.5252872 ] \n",
      "\n",
      "Fisher-Rao:   9.78037\n",
      "Symmetric KL: 3.94411\n",
      "L2 error:     4.86933\n",
      "L1 error:     7.49494\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [-0.91568625 -0.49034554] \n",
      "\n",
      "Fisher-Rao:   9.99490\n",
      "Symmetric KL: 4.09867\n",
      "L2 error:     4.95856\n",
      "L1 error:     7.59209\n"
     ]
    }
   ],
   "source": [
    "eval_all(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial  Dataset\n",
    "\n",
    "Created by generating a dataset in $\\mathbb{R}^{d-1}$ and then using the inverse ilr map to produce compositional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, U_vec = cd.artificial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train, V_test = V[:20], V[20:]\n",
    "U_train_vec = [U[:20] for U in U_vec]\n",
    "U_test_vec = [U[20:] for U in U_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========    CENTERED    =========\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----\n",
      "\n",
      "Fisher-Rao:   8.80424\n",
      "Symmetric KL: 11.80289\n",
      "L2 error:     3.61605\n",
      "L1 error:     5.98129\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   8.79927\n",
      "Symmetric KL: 11.83832\n",
      "L2 error:     3.61056\n",
      "L1 error:     5.97627\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [ 4.46856148e-16  1.33226763e-15 -1.11022302e-15  7.74328490e-16\n",
      " -8.14792772e-16 -1.34765133e-15 -2.14250857e-15  1.36800942e-15\n",
      "  1.08794302e-15] \n",
      "\n",
      "Fisher-Rao:   8.80424\n",
      "Symmetric KL: 11.80289\n",
      "L2 error:     3.61605\n",
      "L1 error:     5.98129\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [-0.03272113 -0.06001659 -0.00839264  0.01281397  0.03260636  0.13333889\n",
      " -0.02286804 -0.14949615 -0.04539334] \n",
      "\n",
      "Fisher-Rao:   8.87079\n",
      "Symmetric KL: 12.11430\n",
      "L2 error:     3.67002\n",
      "L1 error:     6.06674\n",
      "\n",
      "\n",
      "=======    NOT CENTERED    =======\n",
      "\n",
      "-----  Unbiased  -----\n",
      "\n",
      "----L2----]n\n",
      "Fisher-Rao:   0.82543\n",
      "Symmetric KL: 0.08732\n",
      "L2 error:     0.34704\n",
      "L1 error:     0.56739\n",
      "\n",
      "----KL----\n",
      "\n",
      "Fisher-Rao:   0.72519\n",
      "Symmetric KL: 0.06514\n",
      "L2 error:     0.28410\n",
      "L1 error:     0.48218\n",
      "\n",
      "------  Biased  ------\n",
      "\n",
      "----L2----\n",
      "\n",
      "bias :  [ 0.0393406   0.06338479  0.08323587  0.04518627  0.03684464 -0.11985787\n",
      " -0.02607753  0.04321831  0.06816841] \n",
      "\n",
      "Fisher-Rao:   0.75383\n",
      "Symmetric KL: 0.07176\n",
      "L2 error:     0.30148\n",
      "L1 error:     0.49898\n",
      "\n",
      "----KL----\n",
      "\n",
      "bias :  [ 0.03119272  0.00277384  0.04651418  0.05451039  0.0964517  -0.01703819\n",
      " -0.0074332  -0.08701425  0.14907634] \n",
      "\n",
      "Fisher-Rao:   0.81066\n",
      "Symmetric KL: 0.08315\n",
      "L2 error:     0.31459\n",
      "L1 error:     0.53088\n"
     ]
    }
   ],
   "source": [
    "eval_all(V_train, V_test, U_train_vec, U_test_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:skbio_env]",
   "language": "python",
   "name": "conda-env-skbio_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
