\documentclass[BSc]{usydthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Ragib Zaman}
\title{Regression Models for Compositional Data}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% causes equations to be numbered as section.equation
\numberwithin{equation}{chapter}

\theoremstyle{remark}

\newtheorem{Definition}[equation]{Definition}
\newtheorem{Theorem}[equation]{Theorem}
\newtheorem{Proposition}[equation]{Proposition}
\newtheorem{Lemma}[equation]{Lemma}
\newtheorem{Corollary}[equation]{Corollary}

\newtheorem{Remark}[equation]{Remark}
\newtheorem{Example}[equation]{Example}


\usepackage{amsmath,amsfonts,amsthm,bm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% personal macros

% common spaces
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sb}{\mathbb{S}}
\newcommand{\Hb}{\mathbb{H}}
\newcommand{\U}{\mathbb{U}}

\newcommand{\Lag}{\mathcal{L}}
\newcommand{\M}{\mathbf{\mu}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\x}{\mathbf{x}}

\DeclareMathOperator{\im}{im}
% for f:X -> Y; the default spacing isn't great
\newcommand{\map}[2]{\,{:}\,#1\!\longrightarrow\!#2}
\newcommand{\B}[1]{\mathbf{#1}}
% example \varphi \map{X}{Y}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}  

% roman page numbers for the initial pages
\pagenumbering{roman}

\maketitle          % creates the title page

\chapter*{Acknowledgments}
First and foremost I would like to thank Dr. Cheng Soon Ong. I greatly value the wisdom he has shared with me, helping me identify the core ideas in machine learning and the links between. His willingness to give his time so generously and with such patience has been very much appreciated.\\

I would also like to thank Dr. Weifa Liang and Dr. Stephen Gould for taking the responsibilities of course convener and examiner respectively. Without their contribution I would not have been able to have the pleasure of completing this course. 

\chapter*{Abstract}
In this report we explore regression models involving compositional data. We provide some background of the de-facto standard methods of analysing compositional data originally developed primarily by Aitchison (1982, 1986) and Egozcue (2003). After surveying regression problems involving compositions and models for studying them, we give particular focus to a problem where all variables are compositions with the same number of parts and a model for that problem introduced by Wang et al. (2013). We propose new models for this problem and, motivated by ideas from Information Geometry, consider alternate loss functions for these models. Finally, we evaluate these approaches across a variety of datasets and performance metrics.\\ 



\tableofcontents    % creates the table of contents

% reset the page numbering and change to arabic numbers
\newpage\setcounter{page}{1}\pagenumbering{arabic}

\chapter{Introduction}
Compositions are data which contain the information of the relative proportions of multiple parts which together form a whole. They commonly arise in fields such as chemistry, biology, geology and election forecasting, where samples are taken from an object due to it being impractical or unnecessary to take measurement of the entire object. In compositions the relative proportions of the parts contain the meaningful information rather than the actual values of the parts themselves. Thus, compositions are invariant under scaling of all the components by some positive factor.\\

In machine learning we attempt to analyse data to perform tasks such as regression or dimensionality reduction, however standard methods such as linear regression and principal component analysis (PCA) often require that the data naturally resides in some Euclidean space $\mathbb{R}^d,$ which compositions do not. The most naive approach to modelling compositions is to simply forget their special structure and use whichever raw numbers are provided in the original data. This is problematic, since the models in standard methods are not invariant under positive scaling of the input data, so we may get different results depending on how the original data was chosen to be represented. Although there are obvious approaches to get around this (such as normalising the data so the components always sum to $1$), there are still several issues.\\

 \begin{itemize}
  \item The model functions of standard methods often require us to add points together and multiply by real coefficients. The usual vector space operations of $\mathbb{R}^d$ do not apply naturally to compositions. For example, scalar multiplication reduces to the identity map. We require a natural vector space structure on the probablity simplex to use similar model functions to the standard ones.\\
 \item To minimise the standard loss functions we require a suitable notion of distance for compositions. While we could forget the special structure of compositions and, for example, use the Euclidean metric on normalised points, it is unclear whether this is appropriate.\\
  \item Compositions lie in a strict subspace of Euclidean space, as their components always sum to some fixed constant. This poses issues in models which require the data matrix to have full rank for issues related to uniqueness and/or numerical stability.\\ 
 \end{itemize}
 
Even if one could directly apply a standard method, it is natural to expect that a model which specifically addresses and/or incorporates the special structure of compositions will outperform a model which ignores it. In the first systematic treatment of the statistics of compositional data, Aitchison (1982, 1986) defined a Hilbert space structure on compositions and introduced the centered log-ratio transform which naturally maps compositions to a subspace of Euclidean space subject to a constraint. Egozcue et al. (2003) extended this by introducing the isometric log-ratio transform which removed the constraint and gave a natural way to derive Aitchison's Hilbert space structure. Although this allows us to transform compositions into Euclidean data, it may not always be most appropriate to measure the distance between compositions by the Euclidean distance between the transformed points. Avalos-Fernandez et al. (2018) studied PCA on compositional data, measuring quality of fit of a subspace by a general Bregman divergence and giving evidence that using the Kullback-Liebler divergence may be a desirable alternative in some applications.\\

After setting up some required background, we investigate regression problems involving compositional data. Of particular interest is modelling a dependent compositional variable as a function of multiple independent compositional variables. We review a model introduced by Wang et al. (2013) before introducing some new models for this problem. 
 
 
 
\chapter{Background}

In this chapter we define compositions and the spaces and operations that are commonly used to manipulate them. 

\section{Definition and Examples}
\begin{Definition}
Let $d$ be a positive integer. Let $\sim$ be the equivalence relation on the positive orthant $\mathbb{R}^d_{\geq 0}$ where for $x, x' \in \mathbb{R}^d_{\geq 0}$ we have $x \sim x'$ if and only if $x = \lambda x'$ for some $\lambda > 0.$ A $d$-part composition is an element of the quotient set $\mathbb{R}^d_{\geq 0}/\sim.$ (See Appendix V).
\end{Definition}


\begin{Example}
In a two party election we may survey members of various electorates for their preferred party and collect the following results.

\begin{table}[hbt!]
\centering
\begin{tabular}{|l|l|l|} 
\hline
Electorate   & Party A & Party B  \\ 
\hline
Electorate 1 &   23    &   31     \\
Electorate 2 &   34    &   27     \\
Electorate 3 &   13    &   15     \\
\hline
\end{tabular}
\end{table}

The data points for each electorate may be regarded as a 2-part composition. For Electorate 1 the point is $[(23,31)].$ If the total votes in Electorate 1 for Party A and Party B were instead 46 and 62 respectively, our data point would be the same composition since $[(23,31)] = [(46,62)].$ 
\end{Example}

\begin{Example}

Four common varieties of steel have approximate elemental compositions as below. If we took a 10 gram sample of a piece of steel whose variety is not known and it contained 0.015 grams of Carbon, 0.035 grams of Manganese and 9.95 grams of Iron, we may classify it as Mild Carbon, as $[(0.015, 0.035, 9.95)] = [(0.15, 0.35, 99.5)].$

\begin{table}[hbt!]
\centering
\begin{tabular}{|l|l|l|l|} 
\hline
Steel Variant    & \% Carbon & \% Manganese & \% Iron  \\ 
\hline
Mild Carbon      & 0.15      & 0.35         & 99.50    \\
Medium Carbon    & 0.4       & 0.9          & 98.7     \\
High Carbon      & 0.7       & 0.8          & 98.5     \\
Very High Carbon & 1.5       & 1.0          & 97.5     \\
\hline
\end{tabular}
\end{table}
\end{Example}

\begin{Example}
Three super funds have invested their assets into economic sectors as below (amounts are MM \$AUD). Fund A and Fund C have the same composition and we may expect their Return on Investment (ROI) to be similar. 
 \begin{table}[hbt!]
\centering
\begin{tabular}{|l|l|l|l|} 
\hline
Super Fund & Technology & Real Estate & Other Sectors   \\ 
\hline
Fund A     & 233.3      & 45.2        & 432.3           \\
Fund B     & 154.3      & 64.6        & 356.7           \\
Fund C     & 699.9      & 135.6       & 1296.9          \\
\hline
\end{tabular}
\end{table}
\end{Example}
\begin{Remark}
When we understand that a point is a composition we usually omit the brackets. 
\end{Remark}

\section{Coordinates and transformations for compositions}

We now present a set of coordinate systems and maps between them which help in the analysis of compositions. Many operations on compositions try to map compositions bijectively over to spaces that are closer to Euclidean space, so that we can apply known techniques and avoid complications of quotient spaces. These operations usually require that all the components of a composition are non-zero, and would lead to infinte values otherwise. Thus, most treatments of compositional data proceed with this assumption of strictly positive components in their analysis. In practice most measurements have some amount of error associated to them and the target they try to analyse varies continuously with respect to the components, which they use to justify replacing any zero components with a small positive value. This value is often chosen somewhat arbitrarily (such as by Avalos-Fernandez et al. (2016) and Wang et al. (2013)). Martin-Fernandez (2003) studied a systemic approach to choosing this value and gave some arguments towards its suitabiltiy, and this method is now offered in statistical computing packages (such as Scikit-Bio's 'Compositions' package for Python (Scikit-Bio 2019), and the 'compositions' package for R (CRAN 2019)) as the primary way to dealing with zero values. Some studies involving compositional data use techniques which can deal with zero components directly such as that by Scealy \& Welsh (2014). For the rest of this report we assume our compositions have no zero valued components. 

\begin{Definition}
 We define the following spaces.
 \begin{align*}
  S^d &= \bigg\{ x \in \mathbb{R}_{>0}^d \ \bigg| \ \sum_{i=1}^d x_i = 1 \bigg\} \\
  H^d &= \bigg\{ x \in \mathbb{R}_{>0}^d \ \bigg| \ \prod_{i=1}^d x_i = 1 \bigg\} \\
  U^d &= \bigg\{ x \in \mathbb{R}^d \ \ \ \bigg| \ \sum_{i=1}^d x_i = 0 \bigg\} \\
 \end{align*}
The space $S^d$ is the interior of the probability simplex, and in the context of compositional data analysis is sometimes called the 'Aitchison space/complex/geometry' although we reserve this term for when this space is endowed with the Hilbert space structure described in the next section. The space $H^d$ is generally not named, though we may say a composition in this space is given in hyperbolic coordinates. The hyperplane $U^d$ is sometimes called the clr-plane. 
\end{Definition}

The oblique hyperplane $U^d$ has dimension $d-1,$ and by applying a rotation we can isometrically map $U^d$ to the $d-1$ dimensional subspace of $\mathbb{R}^{d}$ defined by $x_d=0.$ We fix one particular rotation, a $(d-1) \times d$ matrix $W$ such that $WW^T = I_{d-1}$ and $W^TW = I_d - \frac{1}{d} \mathbf{1}_{d\times d},$ which is obtained through a Gram-Scmidt process described by Egozcue (2003). Any other rotation $W'$ with these properties could be used in place of $W$ however $W$ is simple to compute and using other $W'$ can be seen as applying $W$ and then further flipping, rotating or shifting the space $\mathbb{R}^{d-1},$ which provides no additional benefite for data analysis (Egozcue 2003). 


\begin{Definition}
 Define $g(x)$ to be the geometric mean of the components of $x,$ and interpret $\exp$ and $\log$ functions on a vector as component-wise application. We define a sequence of maps between the coodinate spaces.   \\

$ \scalebox{1.5}{$ \R^d_{> 0}/\sim \ \  \xrightarrow[{[x]\mapsto \frac{1}{\| x\|_1} x }]{{\mathcal{C}}} S^d \  \xrightarrow[{x \mapsto \frac{x}{g(x)}}]{{\tilde{\cdot}}} \ H^d \xrightarrow[{x \mapsto \log x}]{{\log}}\ U^d \ \xrightarrow[{x\mapsto Wx}]{{W}}\ \R^{d-1} $}$
\\
\\
\\
\\
$ \scalebox{1.5}{$ \ \ \ \  \R^d_{> 0}/\sim \ \ \ \ \xleftarrow[{x\mapsto [x] }]{\text{quotient}} \ \ \  S^d \xleftarrow[{x \mapsto \frac{1}{\| x\|_1} x}]{{\mathcal{C}'}} H^d \xleftarrow[{x \mapsto \exp x}]{{\exp}} U^d \xleftarrow[{x\mapsto W^Tx}]{{W^T}}\R^{d-1} $}$
\end{Definition}

All maps in the above definition are well-defined, continuous and bijective (with their inverse function given by the map in the opposite direction). Through these maps, we can transform a composition into coordinates expressed in any of the above spaces. The centered log-ratio map 
\begin{align*}
 clr : S^d &\to U^d \\
    x &\mapsto \log \frac{x}{g(x)}
\end{align*} 
was introduced by Aitchison (1982) and the isometric log-ratio map 
\begin{align*}
 ilr : S^d &\to \mathbb{R}^{d-1} \\
    x &\mapsto W \log \frac{x}{g(x)}
\end{align*}
was introduced by Egozcue (2003). Both of these maps (and their inverses) can be seen as compositions of consecutive maps in the definition above. Since these maps have been introduced, the standard approach to analysing compositional data has been to apply classical machine learning techniques to the transformed coordinates. Some of those techniques can be given an interpretation strictly within the Aitchison simplex $S^d.$


\section{Hilbert Space Structure of the Aitchison Simplex}
Recall that if $X$ and $Y$ are two topological spaces, a map $f:X\to Y$ is called a homeomorphism if it is a continuous bijective map whose inverse is also continuous. The existence of a homeomorphism between $X$ and $Y$ indicates that $X$ and $Y$ are equivalent in terms of topological structure. The isometric log-ratio function provides a homeomorphism from $S^d$ to $\mathbb{R}^{d-1},$ and through this map we can also transport the Hilbert space structure of $\mathbb{R}^{d-1}$ onto $S^d.$ 

\begin{Proposition}
 Let $X$ be a topological space and $H$ be a Hilbert space with addition, scalar multiplication and the inner product of $H$ denoted by $+,\  \cdot$ and $ \langle \cdot, \cdot \rangle$ respectively. Let $f: X \to H$ be a homeomorphism. Define the maps
\begin{align*}
 \odot : \mathbb{R} \times X &\to X \\
    \alpha \odot x &= f^{-1}( \alpha \cdot f(x) )\\
    \ \\
 \oplus : X \times X &\to X \\
    x \oplus x' &= f^{-1}( f(x) + f(x'))\\
    \ \\
 \langle \cdot, \cdot \rangle_X : X \times X &\to \mathbb{R}_{\geq 0} \\
    \langle x, x' \rangle_X &= \langle f(x), f(x') \rangle
\end{align*}
Then, equipped with those maps, $X$ is a Hilbert space and $f$ is an isometry between $X$ and $H.$
\end{Proposition}

\begin{proof}
 It is trivial to verify that the defined maps satisfy the axioms of an inner product space. Now we check that $X$ is a Hilbert space, that is we verify that $X$ is a complete metric space under the metric induced by the inner product. Let $(x_n)$ be a Cauchy sequence in $X,$ and fix an arbitrary value $\epsilon > 0.$ By the continuity of $f,$ there exists a $\delta>0$ such that $\| f(x) - f(x') \| < \epsilon$ for all $x, x' \in X$ such that $\| x\ominus x'\|_X < \delta.$ Since $(x_n)$ is a Cauchy sequence, there exists a positive integer $N$ such that $\| x_n \ominus x_m \|_X < \delta$ for all $n,m > N.$ Hence we have $\| f(x_n) - f(x_m) \| < \epsilon$ for all $n,m > N,$ proving that $f(x_n)$ is a Cauchy sequence in $H.$ Therefore, $f(x_n) \to z$ for some $z\in H.$ By the definition of the inner product of $X,$ we have $\| f^{-1}(z) \ominus x_n \|_X = \| z - f(x_n) \| \to 0$ so $(x_n)$ converges in $X,$ proving the claim that $X$ is a Hilbert space. The fact that $f$ is an isometry between $X$ and $H$ is essentially a tautology given how we have defined the inner product of $X.$
\end{proof}

Applying this to the isometric log-ratio transform, equipping the following operations onto the Aitchison simplex $S^d$ produces a Hilbert space.  
\begin{align*}
 \odot : \mathbb{R} \times S^d &\to S^d \\
    \alpha \odot x &= \frac{1}{\sum_{i} x_i^{\alpha}} [x_i^{\alpha}] \\
    \ \\
 \oplus : S^d \times S^d &\to S^d \\
    x \oplus x' &= \frac{1}{\sum_i x_i y_i} [x_i y_i] \\
    \ \\
 \langle \cdot, \cdot \rangle_a : S^d \times S^d &\to \mathbb{R}_{\geq 0} \\
    \langle x, x' \rangle_a &= \sum_{i=1}^d \log \left( \frac{x_i}{g(x)} \right)\log \left( \frac{y_i}{g(y)} \right)
\end{align*}

\begin{Example}
Suppose for example that $s, s' \in S^d$ with ilr coordinates $x, x' \in \mathbb{R}^{d-1}.$ A learning model may make predictions by linear combinations of the Euclidean vectors, say something of the form $\beta \cdot x + \beta' \cdot x',$ and then inverting the ilr transform. With the Hilbert space structure of the Aitchison simplex, this can instead be interpreted entirely as operations within the simplex - $\beta \odot s \oplus \beta' \odot.$ 
\end{Example}

In $\mathbb{R}^{d-1}$ we can define the center of points $x_1,\ldots, x_n \in \mathbb{R}^{d-1}$ to be the mean $\frac{1}{n} \sum_{i=1}^n x_i.$ We can transfer this to define a center of compositions.

\begin{Definition}
 Suppose $u_1, \ldots, u_n \in S^d$ are compositions. The center of these points is given by 
 
$$ \overline{u} = \frac{1}{n} \odot \bigoplus_{i=1}^n u_i$$
To centralize a set of compositions, we subtract the center from each element of the set. 
\end{Definition}

The following method is easily verified to be equivalent to but more computationally efficient than computing the center from the above definition. First form the vector in $\mathbb{R}^d$ where the $i$-th component is the geometric mean of the $i$-th components of all the compositions in the set. Then normalize this vector so that it lies in $S^d.$ 


\section{Information Geometry of the Probability Simplex}
[Expand background on statistical manifolds, FIM, KL-divergence, distance on Riemannian manifolds as geodesic distance...]
Each point in the probability simplex $\Delta^d$ corresponds naturally to a discrete probability distribution over $d$ states, so $\Delta^d$ can be viewed as a $d-1$ dimensional statistical manifold. A natural Riemannian metric for statistical manifolds is the Fisher information metric. The Fisher-Rao distance between two points on a statistical manifold as the distance between those points on the Riemannian manifold with metric given by the FIM. \
\\
\begin{Proposition}
The Fisher-Rao distance between $x,y \in \Delta^d$ is given by $$d_{FR}(x,y) = \arccos \left( \sum_{i=1}^d \sqrt{x_i y_i} \right).$$
\end{Proposition}

\begin{proof}
\end{proof}

\begin{Proposition}
As $p \to q$ in $\Delta^d,$ we have $D_{KL}(p, q) \sim 2 d_{FR}^2 (p,q).$ Thus, the KL divergence provides a local approximation to the Fisher-Rao distance.
\end{Proposition}

\begin{proof}
\end{proof}

\begin{Remark}
As $p\to e_i$ and $q\to e_j,$ $i\neq j,$ the KL divergence $D_{KL}(p,q)$ tends to infinity. Under the same conditions, $2 d_{FR}^2(p,q)$ approaches a finite value.  
\end{Remark}




\chapter{Regression with Compositions}

We now briefly define the possible forms of a regression problem involving compositions before we discuss the particular problem that was our focus. \\
\\

\section{General regression}
In general, we may wish to estimate a continuous target (either a real number or a composition) given a set of real numbers and a set of compositions. More precisely, we may have 

\begin{itemize}
 \item A continuous target $y$ with either $y\in \mathbb{R}$ or $y\in S^{d_0}.$
 \item Independent compositional variables $s_1,\ldots, s_m$ where $s_i$ has $d_i$ parts.
 \item Independent real variables $x_1, \ldots, x_{m'}$.
\end{itemize}
 
A regression problem is to learn a function $f$ such that $f(s_1,\ldots, s_m, x_1, \ldots, x_{m'})$ gives a good estimate of the target variable $y,$ where the quality of an estimate is given by a predetermined loss function. Unfortunately there is no accepted convention for naming the various special cases of this regression problem.

\begin{Example}
 A common case is the problem of predicting a real value given a single independent compositional variable. For example, Aitchison (1986) studies the problem of using the relative sand/silt/clay composition of a sediment sample taken from an Artic lake to estimate the depth in the lake from which the sample was taken. 
\end{Example}

\begin{Example}
 Another common case is the problem of predicting a composition given $m$ independent real variables. An example would be modelling the adult/child composition of a household, given their annual expenditure on 10 categories of retail goods. One model commonly used for this problem is the following:
 
 $$ b_0 \oplus \left( \bigoplus_{k=1}^m x_k \odot b_k \right)$$
 
 Here the learned parameters $b_i$ are compositions within the same simplex as the target, and the loss between the estimate and label is given by the distance within the Aitchison simplex. This model was first introduced by Aitchision \& Shen (1980) and the algorithm for computing the parameters has been greatly improved by Aitchision \& Egozcue (2005).
\end{Example}




\section{Aitchison Simplex Regression}

Another special case, and the case which is the main focus on this report, is when we wish to model a target $y \in S^d$ given $m$ independent compositional variables $x_1,\ldots, x_m \in S^d.$ In this case, all of the inputs and outputs of the model lie within the same Aitchison simplex $S^d,$ and we call this problem Aitchison Simplex Regression. \\

Wang et al. (2013) proposed the following model for this problem:

$$ f(x_1, \ldots, x_m) = \bigoplus_{i=1}^m \beta_i \odot x_i $$ and provide a closed form solution for the parameters $\beta_i$ which minimize the error when the loss between an estimate and the true label is given by the Aitchison distance. Their derivation relies on the property that, when transformed by the ilr function, this model has a very particular form which reduces to the form of linear regression in Euclidean space. Note that this form does not accomodate learning a bias term. As a compromise, their method is to first preprocess all of their data by centralizing all of the variables (see Section 2.3), forming their model on this centered data, and then uncentering (adding back the centers) to form a prediction.  

\section{KL loss for Aitchison Simplex Regression}
Wang et al. (2013) fit a model which minimizes the average Aitchison distance between the true values and their estimates, which is equivalent to minimizing the L2 error of the ilr transformed coordinates. As discussed in Section 2.4, although this loss is mathematically convenient to work with it may not best capture the information geometric properties of the Aitchison simplex. \\

PRESENT NEW LOSS AND GRADIENT, so rewrite the equations below in new notation.

More specifically, their model (which we call clr-LR) has the following form. Suppose we have labels $V$ and features $U^{(k)}, k=1,\ldots, m$ in $S^d_n,$ assumed to be centralized. Their model is given by $$ \hat{V} = \oplus_{k=1}^m \beta_k \otimes U^{(k)} $$

Let $Y = ilr(V), \hat{Y} = ilr(\hat{V}), X^{(k)} = ilr( U^{(k)} ).$  Since $ilr$ is an isometric isomorphism, we have $$ \hat{Y} = \sum_{k=1}^m \beta_k X^{(k)}.$$

To find $\beta \in \mathbb{R}^m,$ the authors minimize the Frobenius norm $ \| Y - \hat{Y} \|_F.$ [Derive closed form solution]. Following (Section 4.2 of) (M Avalos-Fernandez et al,  Representation Learning of Compositional Data, 2018), we expect that the model may have better information geometry if we consider minimizing the following loss instead:

$$ l_{CoDA} := D_{exp} \left( \sum_k \beta_k clr(U^{(k)} ), clr(V) \right)$$
$$ = (\mathbf{1}_{n \times 1})^T \exp \left( (\sum_i \beta_i X^{(i)})W \right) \mathbf{1}_{d\times 1} - \operatorname{trace}(\tilde{V}^T (\sum_i \beta_i X^{(i)})W )$$

Let $g$ and $h$ denote the first and second term in the gauged-KL-loss respectively. 
$$ g(\beta) = \sum_{i=1}^d \sum_{j=1}^n \exp \left( \beta_1 C^{(1)}_{ij} + \ldots + \beta_k C^{(k)}_{ij} \right)$$
$$ h(\beta) = \operatorname{trace}\left( \tilde{V}^T \left( \sum_k \beta_k C^{(k)} \right)  \right) = \sum_k \beta_k \operatorname{trace}\left( \tilde{V}^T C^{(k)} \right) $$

We have $$ \frac{ \partial g}{\partial \beta_r } = \sum_{i=1}^d \sum_{j=1}^n C^{(r)}_{ij} \exp \left( \beta_1 C^{(1)}_{ij} + \ldots + \beta_k C^{(k)}_{ij} \right)$$
$$ \frac{ \partial h}{\partial \beta_r } = \operatorname{trace}\left( \tilde{V}^T C^{(r)}\right)$$

This loss function is convex in $\beta$ and we can compute the gradient, so we can find the optimal $\beta$ by standard methods. In our experiments we used Scipy's BFGS optimizer and evaluated the results against metrics on $\Delta^d$ such as Fisher-Rao distance, symmetric KL distance, L1 distance and L2 distance.

On a variety of datasets these two loss functions produce quite similar results. CoDA-LR usually has 1 to 2 percent higher error than clr-LR, although there are instances where CoDA-LR has lower error than clr-LR. The similarity between the results is perhaps unsurprising given the relatively small number of learnable paramaters ($m$) in clr-LR. 




\section{Bias learning model for Aitchison Simplex Regression}

Wang et al's (2013) method for computing the closed form of the learnable parameters breaks down if one extends the model to directly learn a bias composition $\bm{\beta}_0 \in S^d$ in addition to the real parameters $\beta_1,\ldots, \beta_m$ as in the form below:

$$ f(x_1, \ldots, x_m) = \bm{\beta}_0 \oplus \ \bigoplus_{i=1}^m \beta_i \odot x_i $$

Here we describe methods of computing the parameters for this model which minimize the L2-loss and the KL-loss.

\section{Matrix Coefficient Model for Aitchison Simplex Regression}

In the previously introduced models for Aitchison Simplex Regression, although the models could be expressed purely as operations within the Aitchision simplex it was also convenient to look at the models in the ilr-transformed domain. Suppose $w\in \mathbb{R}^{d-1}$ denotes an ilr-transformed estimate of the target and $z_1, \ldots, z_m$ denotes the ilr-transformed independent variables. The following model generalizes the previously models:

$$ w = \bm{\beta}_0 + w_1 \bm{\beta}_1 + \ldots + w_m \bm{\beta}_m$$

where $\bm{\beta}_0$ is a row vector of length $d-1,$ and $\bm{\beta}_1, \ldots, \bm{\beta}_m$ are $(d-1) \times (d-1)$ matrices.  
\\
This model has $m(d-1)^2 + d-1$ learnable paramaters. This general model may be able to learn complex relations between compositional variables, however many compositional datasets do not have enough samples to learn these parameters with good generalisation. As a compromise, one may consider the special case where $\bm{\beta}_1, \ldots, \bm{\beta}_m$ are diagonal matrices, in which case there are $(m+1)(d-1)$ learnable parameters.
\\
A drawback to these models are that they have no simple formulation in the original domain of the Aitchison simplex, and the learnable parameters become more difficult to interpret. \\

Present way to compute the learnable parameters.

\chapter{Experimental Methodology}
TO DO:

\begin{itemize}
 \item Brief description of datasets, including how I generated artificial datasets
 \item Software platform
 \item Hardware platform
 \item Definition of the metrics used to evaluate models
\end{itemize}

\chapter{Results and Discussion}

\chapter{Conclusion}


%% Bibliography %%
\begin{thebibliography}{0}

\bibitem{Aitchison82}
{\sc Aitchison, J 1982}, 'The statistical analysis of compositional data (with discussion)', Journal of the Royal Statistical Society B, Volume 44(2), pp. 139-177.

\bibitem{Aitchision86}
{\sc Aitchison, J 1986}, 'The Statistical Analysis of Compositional Data', Chapman and Hall, New York.

\bibitem{AitchisionEgozcue}
{\sc Aitchision, J \& Egozcue, J.J 2005}, 'Compositional data analysis: where are we and where should we be heading?', Mathematical Geology, Volume 37(7), pp. 829-850.

\bibitem{AitchisionShen}
{\sc Aitchison, J \& Shen, S.M 1980}, 'Logistic-normal distributions: some properties and uses', Biometrika, Volume 67(2), pp. 261-272.

\bibitem{Amari16}
{\sc Amari, S.I 2016}, 'Information Geometry and its Applications', Spinger-Verlag, Berlin.

\bibitem{Avalos16}
{\sc Avalos-Fernandez, M, Nock, R, Ong, C.S, Rouar, J, \& Sun, K 2016}, 'Representation Learning of Compositional data', NIPS'18 Proceedings of the 32nd International Conference on Neural Information Processing Systems. 


\bibitem{Egozcue03}
{\sc Egozcue, J.J, Pawlowsky-Glahn, V, Mateu-Figueras, G \& Barcelo-Vidal, C 2003}, 'Isometric logratio transformations for compositional data analysis', Mathematical Geology Journal, Volume 35, pp. 279-300.

\bibitem{Nock18}
{\sc Nock, R, Menon, A \& Ong, C.S 2018}, 'A scaled Bregman theorem with applications', NIPS'16 Proceedings of the 30th International Conference on Neural Information Processing Systems. 

\bibitem{Wang13}
{\sc Wang, H, Shangguan, L, Wu, J, \& Guan, R, 2013}, 'Multiple linear regression modeling for compositional data', J. Neurocomputing, Volume 122, pp. 490-500.






\end{thebibliography}

\chapter*{Appendix}
.

\end{document}

