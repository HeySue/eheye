\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage{color}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{amsthm}

\theoremstyle{plain}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\newtheorem{lemma}{Lemma}
\newtheorem{lemmaproof}{Proof of Lemma}

\newtheorem{theo}{Theorem}
\newtheorem{theoproof}{Proof of Theorem}

\newtheorem{prop}{Proposition}

\newtheorem{defi}{Definition}

\title{QuantUCB for Order Statistics}
\author{u6015325 }
\date{\today}
\bibliography{ref.bib}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Problem Statement}

    For a stochastic K-armed bandit problem, there is a set of distributions $v = (P_i: i \in \mathcal{K})$, where $\mathcal{K}$ is the set of available actions with size K. In each round $t \in \{1, ..., n\}$, a learner chooses an action $A_t = i \in \mathcal{K}$ according to a policy $\pi$, and gets a reward which is sampled from $P_i$. The goal of designing such a policy is usually to maximize expected cumulative reward (i.e. minimize expected cumulative regret). For this work, instead of caring about the cumulative reward, we focus on the order statistics of the reward, for example, median or more generally quantiles. We give the definitions and notations in the following section. 
    
\subsection{Definitions and Notations}
    
    Let $T_i(t)$ be the number of times machine i has been played during first t plays. 
    \begin{align}
        %T_i(t) = \sum_{s = 1}^t \mathbb{I} \{A_s = i\},
        T_i(t) = \sum_{s = 1}^t \mathbb{I}_{\{i\}} (A_s),
    \end{align}
    where $\mathbb{I}_A: \Omega\rightarrow \{0,1\}$ is the indicator function of $A \subseteq \Omega$, which is defined as, 
    \begin{align}
        \mathbb{I}_A(w) = \begin{cases}
                            1 & w \in A;\\
                            0 & \text{otherwise}.
                            \end{cases}
    \end{align}
    Then $X_{i,T_i(t)}$ is defined as the reward i.i.d. sampled from $P_i$ in round $t$ (when arm i has been played $T_i(n)$ times). \\
    
    Assume the cumulative distribution function $F_{X_i}: \mathbb{R} \rightarrow [0,1]$ of  is continuous, strictly monotonic and differential. Then the quantile function $Q_{i}:  [0,1]\rightarrow \mathbb{R}$ is defined as,
\begin{align}
    Q_{i}(\alpha) &= F_{X_i}^{-1}(\alpha)\\
    &= \inf \{x \in \mathbb{R}| F_{X_i}(x) \geq \alpha\},
\end{align}

The largest $\alpha-$quantile of all arms is then 
\begin{align}
    Q^\ast(\alpha) = \max_{i\in \mathcal{K}} Q_{i}(\alpha).
\end{align}
The empirical quantile function relies on the the sorted samples of arm i up to round t, assume $Y_{i,1} \leq Y_{i,2} ... \leq Y_{i,T_i(t)}$, where $(Y_{i,1} \leq Y_{i,2} ... \leq Y_{i,T_i(t)})$ is the permutation of the samples $X_{i,1} \leq X_{i,2} ... \leq X_{i,T_i(t)}$, then the empirical quantile function is defined as,
\begin{align}
    \hat{Q}_{i, T_i(t)}(\alpha) = Y_{i,\lceil \alpha T_i(t) \rceil}
\end{align}

When $\alpha = \frac{1}{2}$, $Q_i(\alpha)$ is the median of the arm i's reward distribution. We define $m_i = Q_i(1/2)$, similarly we define the largest median of all arms as $m^*$, and the empirial median is $\hat{m}_{i, T_i(t)}$.
    
    The expected regret for total n rounds is the loss due to the policy $\pi$ does not always play the best arm (the arm with maximum order statistics). We define the expected regret in terms of $\alpha-$quantile as 
    \begin{align}
        \label{regret}
        \mathbb{E}[R_n] = Q^\ast(\alpha) n -  \sum_{i=1}^K Q_{i}(\alpha) \mathbb{E}[T_i(n)],
    \end{align}
    where the expectation is taken with respect to the measure on outcomes induced by the interaction of $\pi$ and $v$. If we define the difference between the largest $\alpha-$quantile and $\alpha-$quantile of arm i as $\triangle_{i,\alpha} = Q^\ast(\alpha) - Q_{i}(\alpha)$, we can rewrite the expected regret as
    \begin{align}
        \mathbb{E}[R_n] = \sum_{i = 1}^K \triangle_{i, \alpha} \mathbb{E}[T_i(n)].
    \end{align}
    
\section{Policy and Regret Bound}

\textbf{Proposed policy for arbitrary distribution}\\
In the (t+1) round, pick an arm with index 
\begin{align}
   \argmax_{i \in \mathcal{K}} \hat{Q}_{i, T_i(t)}(\alpha) + \frac{p}{\lambda} - \frac{q}{2} \mathbb{E}[d_{q} (\exp\{\lambda d_{q}\} - 1)]
\end{align}
where $p = 4 \log t$, $q = T_i(t) - \lceil \alpha T_i(t) \rceil$, $\lambda = \sup \{ \lambda \geq 0 | \lambda p - \log \mathbb{E}[\exp\{\lambda (\hat{Q}_{i, T_i(t)}(\alpha) - \mathbb{E}\hat{Q}_{i, T_i(t)}(\alpha))\}]\}$ And $d_q$ is the difference between the $\{T_i(t) - q\}^{th}$ and $\{T_i(t) - (q + 1)\}^{th}$ order statistics,
\begin{align}
    d_q = Y_{i,T_i(t) - q} - Y_{i,T_i(t) - (q + 1)} = Y_{i, \lceil \alpha T_i(t) \rceil} - Y_{i, \lceil \alpha T_i(t) \rceil - 1}
\end{align}
For simplicity, we denote $B_i(t, T_i(t), \alpha) = \frac{p}{\lambda} - \frac{q}{2} \mathbb{E}[d_{q} (\exp\{\lambda d_{q}\} - 1)]$\\

\begin{defi}
(Hazard Rate)\cite{boucheron2012}. The hazard rate of an absolutely continuous probability
distribution with distribution function F is: $h = f /\bar{F}$ where f and $\bar{F} = 1 - F$ are respectively the density and the survival function associated with F.\\
\end{defi}

\begin{theo}
(Regret bound). \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$ having non-decreasing hazard rate, then its expected regret after any number of n plays is at most ... \\
\end{theo}

\begin{proof}

\begin{lemma}
Let $X_1,..., X_n$ be independently distributed according to cumulative density distribution F, $Y_{1} \leq \ldots \leq Y_{n}$ be the order statistics and let $d_q = Y_{n-q} - Y_{n-q-1}$ be the $q^{th}$ spacing. Then with $q = n - \lceil \alpha n \rceil (i.e. 1 - \frac{q+1}{n} \leq \alpha \leq 1 - \frac{q}{n})$, if F has a non-decreasing hazard rate h, then for $\lambda \geq 0$, and $1\leq k \leq n/2$,
\begin{align}
    \label{Boucheron theorem 9-2}
    \log \mathbb{E}e^{\lambda\left(\Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)]\right)} \leq \lambda \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]
\end{align}
And tail bound is 
\begin{align}
    \label{Lemma 1-2}
   P\{\Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)] \geq \frac{\varepsilon}{\lambda} - \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]\} \leq e^{-\varepsilon}\\
   \label{Lemma 1-3}
   P\{\mathbb{E}[ \Hat{Q}(\alpha)] - \Hat{Q}(\alpha) \geq \frac{\varepsilon}{\lambda} - \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]\} \leq e^{-\varepsilon}
\end{align}
\end{lemma}

\begin{proof}
(\ref{Boucheron theorem 9-2}) can be derived from \cite{boucheron2012}(Theorem 2.9).  Let $S = \Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)]$, then according to Markov's inequality,
\begin{align}
    P \{S \geq t\} \leq \frac{\mathbb{E}[e^{\lambda S}]}{e^{\lambda t}} \leq \exp\{\lambda \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right] - \lambda t\},
\end{align}
from which we can derive (\ref{Lemma 1-2}). The lower tail (\ref{Lemma 1-3}) can be similarly proved (\textcolor{red}{how??}).
\end{proof}

\begin{lemma}
Let $I_t$ represent the arm we chose in the round t and $l$ as an arbitrary positive integer. For $t \geq 1$, the number of arm r is chosen is bounded by
\begin{align}
    T_i(n) \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}
\end{align}
\end{lemma}
 
\begin{proof}
    \begin{align}
        T_i(n) =& 1 + \sum_{t = k + 1}^n \{I_t = i\}\\
               \leq & l + \sum_{t = k + 1}^n (\{I_t = i\} \cup \{T_i(t-1) \geq l\})\\
               \leq & l + \sum_{t = k + 1}^n (\{ \hat{Q}_{*, T_*(t-1)}(\alpha) + B_*(t, T_i(t-1), \alpha) \leq \hat{Q}_{i, T_i(t-1)}(\alpha) + B_i(t, T_i(t-1), \alpha)\} \cup \{ T_i(t-1) \geq l\})\\
               \label{proof: minmax}
               \leq & l + \sum_{t = k + 1}^n \{ \mathop{min}\limits_{0 < s < t}\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq \mathop{max}\limits_{l < s_i < t}\hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}\}\\
               \label{proof: union bound}
               \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}
    \end{align}
    where the symbols with $*$ representing the properties of the best arm (i.e. the arm with maximum $\alpha-$quantiles $Q_i(\alpha)$). From step (\ref{proof: minmax}) to (\ref{proof: union bound}), we use the union bound. And in the step (\ref{proof: union bound}), t is summed from 1 to infinity to make sure the arm i can be played for s times.\\
\end{proof}

\begin{lemma}
\label{lemma 3}
    $\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)$ implies that at least one of the following must hold\\
    \begin{align}
        \label{lemma 1.1}
        \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]\\
        \label{lemma 1.2}
        \hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)]\\
        \label{lemma 1.3}
        \mathbb{E}[\hat{Q}_{*, s}(\alpha)] < \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] + 2 B_i(t, s_i, \alpha)
    \end{align}
\end{lemma}

\begin{proof}
Assume all of the three inequalities are not true, then we have
    \begin{align}
        \label{proof lemma 1.1}
         \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) >  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]\\
        \label{proof lemma 1.2}
        \hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) < \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)]\\
        \label{proof lemma 1.3}
        \mathbb{E}[\hat{Q}_{*, s}(\alpha)] \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] + 2 B_i(t, s_i, \alpha)
    \end{align}
    (\ref{proof lemma 1.1}) - (\ref{proof lemma 1.2}) we get, 
    \begin{align}
         \mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] &< 
         \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) - (\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha)) \\
        & \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha) - (\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha)) \\
        &= 2 B_i(t, s_i, \alpha),
    \end{align}
    which is contradicted to (\ref{proof lemma 1.3}), the assumption that all of the three inequalities are not true doesn't hold. Lemma 1 is proved to be true.\\
\end{proof}

\begin{lemma}
    The expected number of arm i is chosen is bounded by 
    \begin{align}
        \mathbb{E}[T_i(n)] \leq ?
    \end{align}
\end{lemma}

\begin{proof}
To give a bound for $\mathbb{E}[T_i(n)]$, we only need to manage give a bound of $\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)$. From Lemma \ref{lemma 3} we know, if we make (\ref{lemma 1.3}) false, then 
\begin{align}
    & P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)) \\
    \leq &  P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]) + P(\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)])
\end{align}
According to Lemma 1, we bound the probability of (\ref{lemma 1.1})(\ref{lemma 1.2}) as
    \begin{align}
        P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]) \leq  e^{-4lnt} = t^{-4}\\
        P(\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)])  \leq  e^{-4lnt} = t^{-4}
    \end{align}
    
    For $l = ...$, (\ref{lemma 1.3}) is false.
    \begin{align}
        & \mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] - 2 B_i(t, s_i, \alpha) \\
        = &\mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] - 2 ... \\
        \geq & \mu^\ast - \mu_i - \triangle_i = 0
    \end{align}
    (\textcolor{red}{Cannot derive a value of l to satisfy that?})
\end{proof}
   
\end{proof}


\section{Policy and Regret Bound for Gaussian Reward}

\textbf{Proposed policy for absolute value of standard normal distribution}\\
In the (t+1) round, pick an arm with index 
\begin{align}
\label{policy normal}
   \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}
\end{align}
where $\hat{m}_{i, T_i(t)}$ is the empirical median for arm i at the round t, $\varepsilon = 4 \log t$, $v_t = \frac{8}{T_i(t) log2}$. $T_i(t)$ is the number of times arm i has been played until round t.  

The policy includes two parts, where the first part is the empirical median of the rewards, the second part is the confidence bound and we denote it as,

\begin{align}
\label{B_i(i, T_i(t)) definition}
B_i(t, T_i(t)) &= \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}\\
&= \sqrt{2 \times \frac{8}{T_i(t) \log 2} \times 4 \log t} + 2 \times 4 \log t \times \sqrt{\frac{8}{T_i(t) \log 2} \times \frac{1}{T_i(t)}}\\
&= \sqrt{\frac{64 \log t}{T_i(t) \log 2}} + \sqrt{\frac{64 \log^2 t \times 8}{T_i^2(t) \log 2}}\\
&= \frac{\sqrt{64 \log t T__i(t)} + 64 \log^2 t \times 8}{\sqrt{T_i^2(t) \log 2}}\\
\label{B_i(i, T_i(t)) definition 2}
&= \frac{8 \sqrt{\log t} (\sqrt{ T_i(t)} + \sqrt{8\log t})}{T_i(t)  \sqrt{\log 2}}
\end{align}

Thus, the policy in (\ref{policy normal}) can be written as,
\begin{align}
    \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \frac{8 \sqrt{\log t} (\sqrt{ T_i(t)} + \sqrt{8\log t})}{T_i(t)  \sqrt{\log 2}}
\end{align}

\begin{theo}
(Regret bound for policy of absolute value of standard normal distribution). \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$ having non-decreasing hazard rate, then its expected regret after any number of n plays is at most 

\begin{align}
    \sum_{i: m_i < m_\ast} \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
where $m_i$ is the median of $P_i$, $m_\ast$ is the median of the best arm (i.e. the arm with maximum median). $\triangle_{j, 1/2} = m_\ast - m_j$, $\beta = (64 + 32 \sqrt{2 \log 2} \triangle_{i,1/2} )\log n$. 
\end{theo}

\begin{proof}

\begin{lemma}
Let $\hat{m}_{i, T_i(t)}$ is the empirical median of arm i in round t, where $T_i(n)$ represents the number of samples for arm i in the $t^{th}$ round. Let $v_t = 8/(T_i(t) \log 2)$. For all $ 0 \leq \lambda \leq T_i(n)/(2 \sqrt{v_t})$,

\begin{align}
    \label{Boucheron propo 4.6}
    \log \mathbb{E} e^{\lambda\left(\hat{m}_{i, T_i(t)}-\mathbb{E}[ \hat{m}_{i, T_i(t)}]\right)} \leq \frac{v_t \lambda^2}{2(1-2\lambda \sqrt{v_t/T_i(t)})} 
\end{align}
And for all $t > 0$, the tail bound is 
\begin{align}
    \label{Lemma 5 tail bound 1}
   P\left( \hat{m}_{i, T_i(t)}-\mathbb{E}[ \hat{m}_{i, T_i(t)}] \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq e^{-\varepsilon}\\
   \label{Lemma 5 tail bound 2}
   P\left( \mathbb{E}[ \hat{m}_{i, T_i(t)}] - \hat{m}_{i, T_i(t)} \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq e^{-\varepsilon}\\
\end{align}
\end{lemma}

\begin{proof}
(\ref{Lemma 5 tail bound 1}) can be proved from \cite{boucheron2012}(Proposition 4.6). If the $\hat{m}_{i, T_i(t)} \in \mathbb{R}$, the lower tail (\ref{Lemma 5 tail bound 2}) holds because of the symmetry (central limit theory). If the $\hat{m}_{i, T_i(t)} \in \mathbb{R^+}$ is positive, then we still have 
\begin{align}
    P\left( \mathbb{E}[ \hat{m}_{i, T_i(t)}] - \hat{m}_{i, T_i(t)} \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq P\left( \hat{m}_{i, T_i(t)}-\mathbb{E}[ \hat{m}_{i, T_i(t)}] \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq e^{-\varepsilon}
\end{align}
In this case, (\ref{Lemma 5 tail bound 2}) is loose. 
\end{proof}

\begin{lemma}
Let $l$ be an arbitrary positive integer, $\hat{m}_{*, s}$ is the empirical median of the reward samples of the best arm (i.e. the arm with maximum median) when it has been played s times, similarly $\hat{m}_{i, s_i}$ is the empirical median of the reward samples of arm i when it has been played $s_i$ times. $B_i(t, s_i)$ is defined as (\ref{B_i(i, T_i(t)) definition}). For $t \geq 1$, the number of arm i is chosen ($T_i(n)$) is bounded by
\begin{align}
    T_i(n) \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)\}
\end{align}
\end{lemma}
 
\begin{proof}
Let $I_t$ represent the arm we chose in the round t
    \begin{align}
        T_i(n) =& 1 + \sum_{t = k + 1}^n \{I_t = i\}\\
       \leq & l + \sum_{t = k + 1}^n (\{I_t = i\} \cup \{T_i(t-1) \geq l\})\\
       \leq & l + \sum_{t = k + 1}^n \left(\{ \hat{m}_{*, T_*(t-1)} + B_*(t, T_*(t-1)) \leq \hat{m}_{i, T_i(t-1)} + B_i(t, T_i(t-1)\} \cup \{ T_i(t-1) \geq l\}\right)\\
       \label{proof: minmax}
       \leq & l + \sum_{t = k + 1}^n \{ \mathop{min}\limits_{0 < s < t}\hat{m}_{*, s} + B_*(t, s) \leq \mathop{max}\limits_{l < s_i < t}\hat{m}_{i, s_i} + B_i(t, s_i)\}\}\\
       \label{proof: union bound}
       \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)\}
    \end{align}
    where the symbols with $*$ representing the properties of the best arm (i.e. the arm with maximum median). From step (\ref{proof: minmax}) to (\ref{proof: union bound}), we use the union bound. And in the step (\ref{proof: union bound}), t is summed from 1 to infinity to make sure the arm i can be played for s times.\\
\end{proof}

\begin{lemma}
\label{lemma 7}

Let $\hat{m}_{*, s}$ is the empirical median of the reward samples of the best arm (i.e. the arm with maximum median) when it has been played s times, $\hat{m}_{i, s_i}$, similarly, is the empirical median of the reward samples of arm i when it has been played $s_i$ times. $B_i(t, s_i)$ is defined as (\ref{B_i(i, T_i(t)) definition}).\\

    $\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)$ implies that at least one of the following must hold\\
    \begin{align}
        \label{lemma 7.1}
        \hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\\
        \label{lemma 7.2}
        \hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\\
        \label{lemma 7.3}
        \mathbb{E}[\hat{m}_{*, s}] < \mathbb{E}[\hat{m}_{i, s_i}] + 2 B_i(t, s_i)
    \end{align}
\end{lemma}

\begin{proof}
Assume all of the three inequalities are not true, then we have
    \begin{align}
        \label{proof lemma 7.1}
         \hat{m}_{*, s} + B_*(t, s) >  \mathbb{E}[\hat{m}_{*, s}]\\
        \label{proof lemma 7.2}
        \hat{m}_{i, s_i} - B_i(t, s_i) < \mathbb{E}[\hat{m}_{i, s_i}]\\
        \label{proof lemma 7.3}
        \mathbb{E}[\hat{m}_{*, s}] \geq \mathbb{E}[\hat{m}_{i, s_i}] + 2 B_i(t, s_i)
    \end{align}
    (\ref{proof lemma 7.1}) - (\ref{proof lemma 7.2}) we get, 
    \begin{align}
         \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}] &< 
         \hat{m}_{*, s} + B_*(t, s) - (\hat{m}_{i, s_i} - B_i(t, s_i)) \\
        & \leq \hat{m}_{i, s_i} + B_i(t, s_i) - (\hat{m}_{i, s_i} - B_i(t, s_i)) \\
        &= 2 B_i(t, s_i),
    \end{align}
    which is contradicted to (\ref{proof lemma 7.3}), the assumption that all of the three inequalities are not true doesn't hold. Lemma \ref{lemma 7} is proved to be true.\\
\end{proof}

\begin{lemma}
\label{Lemma 8: bound for E[T_i(n)]}
    The expected number of arm i is chosen for totally n rounds is bounded by 
    \begin{align}
        \mathbb{E}[T_i(n)] \leq  \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}^2} + 1 + \frac{\pi^2}{3}
    \end{align}
    where $\triangle_{i, 1\2} = m_\ast - m_i = \mathbb{E}[\hat{m}_\ast] - \mathbb{E}[\hat{m}_i]$, $\beta = (64 + 32 \sqrt{2 \log 2} \triangle_{i,1/2} )\log n$.
\end{lemma}

\begin{proof}
To give a bound for $\mathbb{E}[T_i(n)]$, we only need to manage give a bound of $\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)$. From Lemma \ref{lemma 7} we know, if we make (\ref{lemma 7.3}) false, then 
\begin{align}
    & P(\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)) \\
    \leq &  P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) + P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])
\end{align}
According to Lemma 1, we bound the probability of (\ref{lemma 7.1})(\ref{lemma 7.2}) as
    \begin{align}
        P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) \leq  e^{-4\log t} = t^{-4}\\
        P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])  \leq  e^{-4\log t} = t^{-4}
    \end{align}
    
    Then our goal is to find the value of l to make (\ref{lemma 7.3}) false, i.e. 
    \begin{align}
    \label{goal to find l}
        B_i(t, s_i) \leq  \frac{1}{2}(\mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}])\\
    \end{align}
    Remind that $\triangle_{i, 1/2} = m_* - m_i = \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}]$, then (\ref{goal to find l}) can be written as $B_i(t, s_i) \leq  \frac{1}{2} \triangle_{i, 1/2}$. According to (\ref{B_i(i, T_i(t)) definition 2}) and with $t \leq n, s_i \geq l \geq 1$, 
    \begin{align}
        B_i(t, s_i) &= \frac{8 \sqrt{\log t} (\sqrt{ s_i} + \sqrt{8\log t})}{s_i  \sqrt{\log 2}}\\
        & \leq \frac{8 \sqrt{\log n}}{\sqrt{\log 2}} \times \frac{\sqrt{s_i} + \sqrt{8\log n}}{s_i}\\
        & \leq \frac{8 \sqrt{\log n}}{\sqrt{\log 2}} \times \frac{s_i + \sqrt{8\log n}}{s_i}
    \end{align}
    By solving $\frac{8 \sqrt{\log n}}{\sqrt{\log 2}} \times \frac{s_i + \sqrt{8\log n}}{s_i} \leq \frac{1}{2} \triangle_{i, 1/2}$, when $\triangle_{i, 1/2} \sqrt{\log 2} \geq 16 \sqrt{\log n}$ , 
    
    \begin{align}
        s_i \geq \frac{32 \sqrt{2} \log n}{\sqrt{\log 2} \triangle_{i, 1/2} - 16 \sqrt{\log n}} 
    \end{align}
    
    However this assumption doesn't hold in most of the cases, so we need to solve the inequality by directly solving $\frac{8 \sqrt{\log n}}{\sqrt{\log 2}} \times \frac{\sqrt{s_i} + \sqrt{8\log n}}{s_i} \leq 1/2 \triangle_{i, 1/2}$, let $\beta = (64 + 32 \sqrt{2 \log 2} \triangle_{i,1/2} )\log n$,
    \begin{align}
        s_i \geq \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}^2} 
    \end{align}
    
    Then we have pairwise $l'$ which makes (\ref{lemma 7.3}) false,
   \begin{align}
       l' = \lceil \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}^2} \rceil = \lceil \frac{(128 + 32 \sqrt{2 \log 2} \triangle_{i, 1/2} + 64 \sqrt{4 + 2 \sqrt{2 \log 2} \triangle_{i, 1/2}}) \log n}{\log 2 \triangle_{i, 1/2}^2} \rceil
   \end{align} 
   
    So we get the bound of $\mathbb{E}[T_i(n)]$ as,
    \begin{align}
        \mathbb{E}[T_i(n)] &\leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l'}^{t-1} P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) + P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])\\
        & \leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = 1}^{t-1} 2 t^{-4}\\
        & \leq \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}^2} + 1 + \frac{\pi^2}{3}
    \end{align}
   
\end{proof}

According to Lemma \ref{Lemma 8: bound for E[T_i(n)]}, we derive the upper bound for expected regret,
\begin{align}
    \mathbb{E}[R_n] \leq 
          \sum_{i: m_i < m_\ast} \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
   
\end{proof}


\section{Extensions to Section 3 Gaussian Case}

\subsection{Why absolute?}

In \cite{boucheron2012} Proposition 4.6,  they use
\begin{align}
    \Delta_{n / 2} \leq \frac{\sqrt{2} \lambda E_{n / 2}}{(n / 2) \sqrt{\log 2+Y_{(n / 2+1)}}} \leq \frac{\sqrt{2} E_{n / 2}}{(n / 2) \sqrt{\log 2}}=\sqrt{\frac{v_{n}}{n}} E_{n / 2}
\end{align}
to get rid of $\Delta_{n/2}$. 
The inequality is derived from \cite{boucheron2012} Proposition 4.1: 
\begin{align}
    y>0, \phi(\tilde{U}(\exp (y))) / \bar{\Phi}(\tilde{U}(\exp (y))) \geq \sqrt{\kappa_{1}(y+\log 2)} \text { where } \kappa_{1} \geq 1 / 2
\end{align} 

As the hazard rate $\phi(x) / \bar{\Phi}(x)$ of the Gaussian distribution tends to 0 as x tends to $- \infty$ , the approach in  \cite{boucheron2012} Proposition 4.6 does not work when dealing with order statistics of Gaussian samples (If the hazard rate tends to 0, then Prop 4.1 cannot hold anymore).

Another possible reason might be, hazard rate is defined on positive values.

Not sure whether we can extend the proof into non-absolute values.

\subsection{How about non-standard Gaussian?}
The absolute values of Gaussian setting makes the extension to non-standard Gaussian non-trivial. To show this, we first show how can we using the CDF of standard Gaussian distribution $\Phi$ to represent the 1 - 1/t quantile of the distribution of the absolute value of a standard Gaussian random variable. We give the general form of the CDF of Gaussian $\Phi$ and Folded Gaussian distribution (absolute values) $\tilde{\Phi}$ in the following.

\begin{align}
    \Phi(x) &= \frac{1}{2}\left(1 + \text{erf}(\frac{x - \mu}{\sigma \sqrt{2}})\right)\\
    \tilde{\Phi}(x) &= \frac{1}{2}\left(\text{erf}(\frac{x + \mu}{\sigma \sqrt{2}}) + \text{erf}(\frac{x - \mu}{\sigma \sqrt{2}})\right) 
\end{align}

For the case of standard Gaussian, $\mu = 0, \sigma = 1$, thus we have 
\begin{align}
    \tilde{\Phi}(x) &= 2 \Phi(x) - 1\\
    \tilde{\Phi}^{-1}(x) &= \Phi^{-1}(\frac{x+1}{2})\\
    \tilde{\Phi}^{-1}(1 - 1/t) &= \Phi^{-1}(1 - 1/(2t))
\end{align}
Thus let $\tilde{U} : ] 1, \infty ) \rightarrow[0, \infty)$ be defined by $\widetilde{U}(t)=\Phi^{-1}(1-1 /(2 t))$, $\widetilde{U}(t)$ is the 1 - 1/t quantile of the distribution of the absolute value of a standard Gaussian random variable, or the 1 - 1/(2t) quantile of the standard Gaussian distribution. 

However, if we consider the Gaussian distribution with non-zero mean (which is a necessary assumption of bandit settings), we cannot get rid of the term $\text{erf}(\frac{x + \mu}{\sigma \sqrt{2}})$ easily, which means it's hard to represent the quantile of the absolute non-standard Gaussian with $\Phi$, and thus cannot direct make use of the fact of $p \sqrt{\kappa_{1} \log 1 / p} \leq \phi \circ \Phi^{-1}(p)$.

\subsection{Other distributions?}
To figure out how to proof for other distributions, we first show the detailed proof of \cite{boucheron2012} Proposition 4.6. 

\begin{prop}
\label{prop 4.6}
(\cite{boucheron2012} Proposition 4.6) Note here we use the notations which are consistent to \cite{boucheron2012}, but slightly different as the above notations in this note (TODO: make it consistent?). 

Let $v_{n}=8 /(n \log 2)$. $X_1, ..., X_n$ are independent random variables, distributed according to a certain probability distribution F, and $X_{(1)} \geq X_{(2)} \geq ... \geq X_{(n)}$ denote the corresponding order statistics. $X_{n/2}$ is the median (we assume n is even). For all $0 \leq \lambda<n /\left(2 \sqrt{v_{n}}\right)$, 
\begin{align}
    \log \mathbb{E}e^{\lambda\left(X_{(n / 2)}-\mathrm{E} X_{(n / 2)}\right)} \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}.
\end{align}
where the expectation is in terms of the randomness of the environment. For all $t > 0$,
\begin{align}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} t}+2 t \sqrt{v_{n} / n}\right\} \leq e^{-t}
\end{align}
\end{prop}

\begin{proof}
By \cite{boucheron2012} Theorem 2.9, 
\begin{align}
    \log \operatorname{E} e^{\lambda\left(X_{(n / 2)}-\mathbb{E} X_{(n / 2)}\right)} \leq \frac{n}{4} \lambda \mathbb{E}\left[\Delta_{n / 2}\left(e^{\lambda \Delta_{n / 2}}-1\right)\right]
\end{align}
where $\Delta_{n / 2}=X_{(n / 2)}-X_{(n / 2+1)} \sim \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)- \tilde{U}\left(e^{Y_{(n / 2+1)}}\right).$ where $E_{n/2}$ is standard exponentially distributed and independent of $Y_{(n/2 + 1)}$. $Y_{(1)} \geq Y_{(2)} \geq ... \geq Y_{(n)}$ is the order statistics of an independent sample of the standard exponential distribution. 

If hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $U(\exp (t+x))-U(\exp (t)) \leq x / h(U(\exp (t)))$. Thus, 
\begin{align}
    \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)- \tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \leq \frac{E_{n / 2} /(n / 2)}{h(\tilde{U}(\exp{(Y_{(n/2+1)})}))}
\end{align}

From \cite{boucheron2012} Proposition 4.1 (ii), 
\begin{align}
    h(U(\exp{(Y_{(n/2+1)})})) &= \frac{\phi(\widetilde{U}(\exp (Y_{(n/2+1)})))}{\overline{\Phi}(\widetilde{U}(\exp (Y_{(n/2+1)})))}\\
    &\geq \sqrt{k_1 (Y_{(n/2+1)} + \log 2)} \quad \text{where} \quad k_1 \geq 1/2,
\end{align}
so we get 
\begin{align}
    \triangle_{n/2} = \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) 
    &\leq \frac{E_{n / 2} /(n / 2)}{\sqrt{k_1 (Y_{(n/2+1)} + \log 2)}}\\
    &\leq \frac{ \sqrt{2} E_{n / 2} }{(n / 2)\sqrt{(Y_{(n/2+1)} + \log 2)}}\\
    &\leq \frac{ \sqrt{2} E_{n / 2} }{(n / 2)\sqrt{ \log 2}}\\
    &= \sqrt{\frac{v_{n}}{n}} E_{n / 2}
\end{align}

Then our goal is to show that $\frac{n}{4} \lambda \mathbb{E}\left[\Delta_{n / 2}\left(e^{\lambda \Delta_{n / 2}}-1\right)\right] \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}$, which can be shown in the following,

\begin{align}
    & \lambda \mathbb{E}[\left(\tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \right) \left(e^{\lambda \left (\tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \right)} - 1\right)]\\
    & \leq \lambda \mathbb{E}[\sqrt{\frac{v_{n}}{n}} E_{n / 2} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} E_{n / 2}} -1\right)]\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} \{ E_{n / 2}  = x \} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} \{E_{n / 2} = x\}} -1\right) f_{E_{n/2}}(x) d x\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} x \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} x } -1\right) e^{-x} d x\\
    & \leq \frac{2\lambda^2 v_n}{n (1 - 2 \lambda \sqrt{\frac{v_n}{n}})}
\end{align}
where the last step is because for $0 \leq \mu \leq 1/2$, $\int_{0}^{\infty} \mu x\left(e^{\mu x}-1\right) e^{-x} \mathrm{d} x=\frac{\mu^{2}(2-\mu)}{(1-\mu)^{2}} \leq \frac{2 \mu^{2}}{1-2 \mu}$.
\end{proof}

The above proof shows that the key point of change the absolute value of Gaussian random variable with other random variables is we can have some bound (positive) for the hazard rate (in terms of the U transformation). 

This also explains why Gaussian random variables cannot work for such kind of proof, when x tends to negative infinity, the hazard rate of the Gaussian distribution tends to 0, which makes $\Delta_{n/2}$ bounded by positive infinity. So only with the assumption of non-decreasing hazard rate, the above proof works bad. 
\subsubsection{Exponential Distribution}

Based on above analysis, the Exponential distribution can be a good candidate to try. Since its positive and it has the positive constant hazard rate (before the U transformation), and it can be easily represented by the U transformation.\\

For $\alpha > 0$, PDF of exponential distribution:
\begin{align}
\label{Expon PDF}
    f(x, \alpha) = \alpha e^{-\alpha x},
\end{align}

CDF of exponential distribution:
\begin{align}
\label{expon CDF}
    F(x, \alpha) = 1 - e^{-\alpha x},
\end{align}

hazard rate of exponential distribution:
\begin{align}
 h = f(x, \alpha)/ (1-F(x, \alpha)) = \alpha
\end{align}


\begin{prop}
\label{prop for expon hazard rate}
Exponential distribution has constant hazard rate, for it's CDF F and PDF f defined as (\ref{expon CDF})(\ref{Expon PDF}), for $y > 0$,
\begin{align}
    \frac{f(U(\exp{y}))}{\bar{F}(U(\exp{y}))} = \frac{y}{\alpha}
\end{align}
\end{prop}

\begin{proof}
\begin{align}
    \frac{f(U(e^{y}))}{\bar{F}(U(e^{y}))} &=
    \frac{f(F^{-1}(1 - e^{-y}))}{1-F\left({F}^{-1}\left(1 - e^{-y})\right)\right)}\\
    &= \frac{f(F^{-1}(1 - e^{-1}{y}))}{e^{-y}}
\end{align}
Note that $F^{-1}(y) = - \log (1 - y)/ \alpha$, thus $F^{-1}(1 - e^{-1}{y}) = y/ \alpha$. And $f(y/\alpha) = \frac{y}{\alpha} e^{-y}$,
so 
\begin{align}
    \frac{f(U(e^{y}))}{\bar{F}(U(e^{y}))} = \frac{\frac{y}{\alpha} e^{-y}}{e^{-y}} = \frac{y}{\alpha}.
\end{align}
 
\end{proof}

\begin{prop}
\label{prop 4.6}

$X_1, ..., X_n$ are independent random variables, distributed according to exponential distribution's CDF F, and $X_{(1)} \geq X_{(2)} \geq ... \geq X_{(n)}$ denote the corresponding order statistics. $X_{n/2}$ is the median (we assume n is even). For all $0 \leq \lambda<n /\left(2 \sqrt{v_{n}}\right)$, 
\begin{align}
    \log \mathbb{E}e^{\lambda\left(X_{(n / 2)}-\mathrm{E} X_{(n / 2)}\right)} \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}
\end{align}
where the expectation is in terms of the randomness of the environment. For all $t > 0$,
\begin{align}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} t}+2 t \sqrt{v_{n} / n}\right\}  \leq e^{-t}
\end{align}
\end{prop}

\begin{proof}
By \cite{boucheron2012} Theorem 2.9, 
\begin{align}
    \log \operatorname{E} e^{\lambda\left(X_{(n / 2)}-\mathbb{E} X_{(n / 2)}\right)} \leq \frac{n}{4} \lambda \mathbb{E}\left[\Delta_{n / 2}\left(e^{\lambda \Delta_{n / 2}}-1\right)\right]
\end{align}
where $\Delta_{n / 2}=X_{(n / 2)}-X_{(n / 2+1)} \sim U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right).$ where $E_{n/2}$ is standard exponentially distributed and independent of $Y_{(n/2 + 1)}$. $Y_{(1)} \geq Y_{(2)} \geq ... \geq Y_{(n)}$ is the order statistics of an independent sample of the standard exponential distribution. 

If hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $U(\exp (t+x))-U(\exp (t)) \leq x / h(U(\exp (t)))$. Thus, 
\begin{align}
    U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \leq \frac{E_{n / 2} /(n / 2)}{h(U(\exp{(Y_{(n/2+1)})}))}
\end{align}

From Proposition \ref{prop for expon hazard rate}, 
\begin{align}
    h(U(\exp{(Y_{(n/2+1)})})) &= \frac{f(U(\exp (Y_{(n/2+1)})))}{\overline{F}({U}(\exp (Y_{(n/2+1)})))} = \frac{Y_{(n/2+1)}}{\alpha} 
\end{align}
so we get 
\begin{align}
    \triangle_{n/2} = U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) 
    &\leq \frac{E_{n / 2} /(n / 2)}{\frac{Y_{(n/2+1)}}{\alpha} }\\
    &\leq \frac{ 2\alpha E_{n / 2} }{n Y_{(n/2+1)}}\\
    & \leq \frac{ 2\alpha E_{n / 2} }{n \gamma}
\end{align}
where the last line holds with the assumption $Y_{(n/2+1)} \geq \gamma$, $\gamma > 0$ is a constant. Let $v_n = \frac{4 \alpha^2}{n \gamma^2}$, we have $\triangle_{n/2} \leq \sqrt{\frac{v_n}{n}} E_{n/2}$ (TODO: release the assumption)\\

Then our goal is to show that $\frac{n}{4} \lambda \mathbb{E}\left[\Delta_{n / 2}\left(e^{\lambda \Delta_{n / 2}}-1\right)\right] \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}$, which can be shown in the following (same as the Gaussian case),

\begin{align}
    & \lambda \mathbb{E}[\left(U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \right) \left(e^{\lambda \left (U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \right)} - 1\right)]\\
    & \leq \lambda \mathbb{E}[\sqrt{\frac{v_{n}}{n}} E_{n / 2} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} E_{n / 2}} -1\right)]\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} \{ E_{n / 2}  = x \} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} \{E_{n / 2} = x\}} -1\right) f_{E_{n/2}}(x) d x\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} x \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} x } -1\right) e^{-x} d x\\
    & \leq \frac{2\lambda^2 v_n}{n (1 - 2 \lambda \sqrt{\frac{v_n}{n}})}
\end{align}
where the last step is because for $0 \leq \mu \leq 1/2$, $\int_{0}^{\infty} \mu x\left(e^{\mu x}-1\right) e^{-x} \mathrm{d} x=\frac{\mu^{2}(2-\mu)}{(1-\mu)^{2}} \leq \frac{2 \mu^{2}}{1-2 \mu}$.

\end{proof}

%------------------------------------------------------------------------------
\textbf{Proposed policy for exponential distribution (with assumption $Y_{(n/2 + 1)} \geq \gamma)$}\\

In the (t+1) round, pick an arm with index 
\begin{align}
\label{policy normal}
   \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}
\end{align}
where $\hat{m}_{i, T_i(t)}$ is the empirical median for arm i at the round t, $\varepsilon = 4 \log t$, $v_t = \frac{4 \alpha^2}{T_i(t) \gamma^2}$ with $0 < \gamma \leq 1$. $T_i(t)$ is the number of times arm i has been played until round t.  

The policy includes two parts, where the first part is the empirical median of the rewards, the second part is the confidence bound and we denote it as,

\begin{align}
\label{B_i(i, T_i(t)) definition}
B_i(t, T_i(t)) &= \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}\\
&= \sqrt{2 \times \frac{4 \alpha^2}{T_i(t) \gamma^2} \times 4 \log t} + 2 \times 4 \log t \times \sqrt{\frac{4 \alpha^2}{T_i(t) \gamma^2} \times \frac{1}{T_i(t)}}\\
&= \frac{4 \alpha \sqrt{\log t} ( \sqrt{ 2T_i(t)} + 4\sqrt{\log t})}{T_i(t) \gamma}
\end{align}

Thus, the policy in (\ref{policy normal}) can be written as,
\begin{align}
    \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} +  \frac{4 \alpha \sqrt{\log t} ( \sqrt{ 2T_i(t)} + 4\sqrt{\log t})}{T_i(t) \gamma}
\end{align}

\begin{theo}
(Regret bound for policy of exponential distribution) (with assumption $Y_{(n/2 + 1)} \geq \gamma)$. \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$ having non-decreasing hazard rate, then its expected regret after any number of n plays is at most 

\begin{align}
    \sum_{i: m_i < m_\ast} \frac{32 \alpha \log t (\sqrt{2 \alpha + \triangle_{i, 1/2} \gamma^2} + \sqrt{\alpha})^2}{ \triangle_{i, 1/2} \gamma^2} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
where $m_i$ is the median of $P_i$, $m_\ast$ is the median of the best arm (i.e. the arm with maximum median). $\triangle_{j, 1/2} = m_\ast - m_j$. $\alpha > 0$ is the parameter of exponential distribution. 
\end{theo}

The bound isn't ideal, since when $\gamma$ approaches to 0, the bound approaches to $\infty$, which means we have to release the assumption of Y. 
%-------------------------------------------------------------------------------

\section*{Appendix}

\textbf{Logic of Proposition 4.5}\\

Known
\begin{align}
    t \leq  \frac{\lambda}{2} \mathbb{E}_{X, Y}[g(X, Y)]
\end{align}

and define
\begin{align}
    \mathbb{E}_X[g(X, Y = y)| Y = y] \leq \varphi (y)\\
    \rho = \max_{\eta} \mathbb{E}_X[g(X, Y = \eta)|Y = \eta]
\end{align}

The conditional expectation $\mathbb{E}_X[g(X, Y)| Y]$ is a non-decreasing function of Y.\\

The goal is to prove the target t satisfies the inequality:\\
\begin{align}
    t \leq \lambda \varphi(\tau) + \frac{\lambda}{2} \rho \mathbb{P}(Y \leq \tau)
\end{align}
Proof:
\begin{align}
    t & \leq  \frac{\lambda}{2} \mathbb{E}_{X, Y}[g(X, Y)]\\
    & = \frac{\lambda}{2}\int _0^{\infty} \int_0^{\infty} p(X = x, Y = y) g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} \int_0^{\infty} p(X = x|Y = y) p(Y = y)g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} p(Y = y) \int_0^{\infty} p(X = x|Y = y) g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy\\
    & = \frac{\lambda}{2}\left(\int _0^{\tau} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy + \int _\tau^{\infty} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy\right)\\
    & \leq \frac{\lambda}{2}\left(\int _0^{\tau} p(Y = y) dy \times \mathbb{E}_X[g(X, Y = 0)| Y = 0] + \int _\tau^{\infty} p(Y = y) dy \times \mathbb{E}_X[g(X, Y = \tau)| Y = \tau] \right)\\
    & = \frac{\lambda}{2}\left(p(Y \leq \tau) \times \mathbb{E}_X[g(X, Y = 0)| Y = 0] + p(Y > \tau) \times \mathbb{E}_X[g(X, Y = \tau)| Y = \tau] \right)\\
    & \leq \frac{\lambda}{2} \mathbb{E}_X[g(X, Y = \tau)|Y = \tau] + \frac{\lambda}{2} \mathbb{P}(Y \leq \tau) \max_{\eta} \mathbb{E}_X[g(X, Y = \eta)|Y = \eta]\\
    & \leq \lambda \varphi(\tau) + \frac{\lambda}{2} \rho \mathbb{P}(Y \leq \tau)
\end{align}

From (122) to (123), we 

Note the corresponding relationships to the paper are:\\

$t = \log \mathbb{E} [e^{\lambda (X_{(1)} - \mathbb{E}[X_{(1)}])}]$, $X = E_1, Y = Y_{(2)}$ are both standard exponential random variables. 

\printbibliography
\end{document}
