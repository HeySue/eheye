\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\usepackage[dvipsnames]{xcolor}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{amsthm}

\theoremstyle{plain}

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\newtheorem{lemma}{Lemma}
\newtheorem{lemmaproof}{Proof of Lemma}

\newtheorem{theo}{Theorem}
\newtheorem{theoproof}{Proof of Theorem}

\newtheorem{prop}{Proposition}

\newtheorem{defi}{Definition}

\title{QuantUCB for Order Statistics}
\author{u6015325 }
\date{\today}
\bibliography{ref.bib}

\begin{document}

\maketitle

% Introduction----------------------------------------------------------------------------
\section{Introduction}

\subsection{Problem Statement}

    For a stochastic K-armed bandit problem, there is a set of distributions $v = (P_i: i \in \mathcal{K})$, where $\mathcal{K}$ is the set of available actions with size K. In each round $t \in \{1, ..., n\}$, a learner chooses an action $A_t = i \in \mathcal{K}$ according to a policy $\pi$, and gets a reward which is sampled from $P_i$. The goal of designing such a policy is usually to maximize expected cumulative reward (i.e. minimize expected cumulative regret). For this work, instead of caring about the cumulative reward, we focus on the order statistics of the reward, for example, median or more generally quantiles. We give the definitions and notations in the following section. 
    
\subsection{Definitions and Notations}
    
    Let $T_i(t)$ be the number of times machine i has been played during first t plays. 
    \begin{align}
        %T_i(t) = \sum_{s = 1}^t \mathbb{I} \{A_s = i\},
        T_i(t) = \sum_{s = 1}^t \mathbb{I}_{\{i\}} (A_s),
    \end{align}
    where $A_s$ is the arm chosen in the round s, $\mathbb{I}_A: \Omega\rightarrow \{0,1\}$ is the indicator function of $A \subseteq \Omega$, which is defined as, 
    \begin{align}
        \mathbb{I}_A(w) = \begin{cases}
                            1 & w \in A;\\
                            0 & \text{otherwise}.
                            \end{cases}
    \end{align}
    Then let $X_i$ be the random variable denotes the reward for arm i, and $X_{i,T_i(t)}$ is defined as the reward i.i.d. sampled from $P_i$ in round $t$ (when arm i has been played $T_i(n)$ times). \\
    
    Assume $X_i$ lies in interval [a, b] with $a \leq b$ and $a, b \in \mathbb{R}$, the cumulative distribution function $F_{X_i}: [a, b] \rightarrow [0,1]$ is continuous, strictly monotonic and differential. Then the quantile function is $Q_{i}:  [0,1] \rightarrow [a,b]$. For $\alpha \in [0,1]$, the $\alpha-$quantile of arm i is defined as,
    \begin{align}
        Q_{i}(\alpha) &= F_{X_i}^{-1}(\alpha)\\
        &= \inf \{x \in \mathbb{R}| F_{X_i}(x) \geq \alpha\},
    \end{align}

The largest $\alpha-$quantile of all arms is then 
\begin{align}
    Q^\ast(\alpha) = \max_{i\in \mathcal{K}} Q_{i}(\alpha).
\end{align}
The empirical quantile function relies on the the sorted samples of arm i up to round t, assume for arm i, the non-decreasing sorted samples is $X_{i,(1)} \geq X_{i,(2)} ... \geq X_{i,(T_i(t))}$, then the empirical quantile function is defined as,
\begin{align}
    \hat{Q}_{i, T_i(t)}(\alpha) = Y_{i,\lfloor (1 - \alpha) T_i(t) \rfloor}
\end{align}


The expected regret for total n rounds is the loss due to the policy $\pi$ does not always play the best arm (the arm with maximum order statistics). We define the expected regret in terms of $\alpha-$quantile as 
\begin{align}
    \label{regret}
    \mathbb{E}[R_n] = Q^\ast(\alpha) n -  \sum_{i=1}^K Q_{i}(\alpha) \mathbb{E}[T_i(n)],
\end{align}
where the expectation is taken with respect to the measure on outcomes induced by the interaction of $\pi$ and $v$ (i.e. expectation in terms of different experiments). If we define the difference between the largest $\alpha-$quantile and $\alpha-$quantile of arm i as $\triangle_{i,\alpha} = Q^\ast(\alpha) - Q_{i}(\alpha)$, we can rewrite the expected regret as
\begin{align}
    \mathbb{E}[R_n] = \sum_{i = 1}^K \triangle_{i, \alpha} \mathbb{E}[T_i(n)].
\end{align}

Specially, when $\alpha = \frac{1}{2}$, $Q_i(\alpha)$ is the median of the arm i's reward distribution. We define $m_i = Q_i(1/2)$, similarly we define the largest median of all arms as $m^*$, and the empirical median is $\hat{m}_{i, T_i(t)}$. From now on, we first concentrate on the median case. 

%---------------------------------------------------------------------------------
\section{Concentration inequalities for order statistics}
The UCB algorithms based on the concentration inequalities for order statistics, which was first proposed by \cite{boucheron2012}. In \cite{boucheron2012}, they derived Bernstein-like inequalities for absolute value of independent standard Gaussian random variables. We first extend their theorem into two distributions: absolute Gaussian distribution with zero mean, exponential distributions. To show that, we first show several key concepts which are needed to derive the inequalities. 

%--------------------------------------------------------
% definitions/theos from paper

\begin{defi} (U-transform.) The U-transform of a cumulative density distribution F is defined as a non-decreasing function on $(1, \infty)$ by $U = (1/(1-F))^{-1}$, $U(t) = \inf\{x: F(x) \geq 1 - 1/t\} = F^{-1}(1-t)$.
\end{defi}

\begin{theo} (Renyi's representation.) Let $X_{(1)} \geq \ldots \geq X_{(n)}$ be the order statistics of a sample from P with cumulative density distribution F, $Y_{(1)} \geq Y_{(2)} \geq \ldots \geq Y_{(n)}$ be the
order statistics of an independent sample of the standard exponential distribution, then
\begin{align}
    \left(Y_{(n)}, \ldots, Y_{(i)}, \ldots, Y_{(1)}\right) \sim\left(\frac{E_{n}}{n}, \ldots, \sum_{k=i}^{n} \frac{E_{k}}{k}, \ldots, \sum_{k=1}^{n} \frac{E_{k}}{k}\right)
\end{align}
where $E_{1}, \ldots, E_{n}$ are independent and identically distributed standard exponential random variables, and
\begin{align}
    \left(X_{(n)}, \ldots, X_{(1)}\right) \sim\left(U \circ \exp \left(Y_{(n)}\right), \ldots, U \circ \exp \left(Y_{(1)}\right)\right)
\end{align}
where $U=(1 /(1-F))^{-1}$ is the U-transform.
\end{theo}

\begin{defi} (Hazard rate.) The hazard rate of an absolutely continuous probability distribution with CDF F is: $h=f / \overline{F} \text { where } f \text { and } \overline{F}=1-F$ are respectively the density and the survival function associated with F .
\end{defi}

\begin{prop}
\label{prop non-increasing hazard rate}
Let F be an absolutely continuous distribution function with hazard
rate h, let $U=(1 /(1-F))^{-1}$. We get $(U \circ \exp )^{\prime}=1 / h(U \circ \exp )$. Then, 
\begin{itemize}
    \item h is non-decreasing if and only if $U \circ \mathrm{exp}$ is concave.
    \item if the hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $U(\exp (t+x))-U(\exp (t)) \leq x / h(U(\exp (t)))$.
\end{itemize}
\end{prop}
%-------------------------------------------------------

\subsection{Concentration inequalities for absolute value of Gaussian distribution}

\cite{boucheron2012} derived Bernstein-like inequalities for absolute value of independent standard Gaussian random variables. The absolute values of Gaussian setting makes the extension to non-standard Gaussian non-trivial. To show this, we first show how can we use the CDF of standard Gaussian distribution $\Phi$ to represent the 1 - 1/t quantile of the distribution of the absolute value of a standard Gaussian random variable. We give the general form of the CDF of Gaussian $\Phi$ and Folded Gaussian distribution (absolute values) $\tilde{\Phi}$ in the following.

\begin{align}
    \Phi(x) &= \frac{1}{2}\left(1 + \text{erf}(\frac{x - \mu}{\sigma \sqrt{2}})\right)\\
    \tilde{\Phi}(x) &= \frac{1}{2}\left(\text{erf}(\frac{x + \mu}{\sigma \sqrt{2}}) + \text{erf}(\frac{x - \mu}{\sigma \sqrt{2}})\right) 
\end{align}

For the case of standard Gaussian, $\mu = 0$,  we have 
\begin{align}
    \tilde{\Phi}(x) &= 2 \Phi(x) - 1\\
    \tilde{\Phi}^{-1}(x) &= \Phi^{-1}(\frac{x+1}{2})\\
    \tilde{\Phi}^{-1}(1 - 1/t) &= \Phi^{-1}(1 - 1/(2t))
\end{align}
Thus let $\tilde{U} : [ 1, \infty ) \rightarrow[0, \infty)$ be defined by $\widetilde{U}(t)=\Phi^{-1}(1-1 /(2 t))$, $\widetilde{U}(t)$ is the 1 - 1/t quantile of the distribution of the absolute value of a standard Gaussian random variable, or the 1 - 1/(2t) quantile of the standard Gaussian distribution. 

However, if we consider the Gaussian distribution with non-zero mean, we cannot get rid of the term $\text{erf}(\frac{x + \mu}{\sigma \sqrt{2}})$ easily, which means it's hard to represent the quantile of the absolute non-standard Gaussian with $\Phi$, and thus cannot direct make use of the fact of $p \sqrt{\kappa_{1} \log 1 / p} \leq \phi \circ \Phi^{-1}(p)$.


\begin{lemma}
\label{lemma mills' ratio}
    For $x >0$, $ \sigma^2 \phi(x) -x \bar{\Phi}(x) \geq 0$, where $\bar{\Phi}(x) = 1 - \Phi(x),  \phi, \Phi$ are respectively pdf and cdf for centered Gaussian distribution with variance $\sigma^2$.
\end{lemma}
\begin{proof}
\begin{align}
    0 &< \int_x^\infty \bar{\Phi}(t) dt\\
    & = t \overline{\Phi}\left.(t)\right|_{x} ^{\infty}-\int_x^{\infty} t(-\phi(t)) d t\\
    &= -x \overline{\Phi}(x)+ \sigma^2 \phi(x).
\end{align}
\end{proof}

\begin{lemma}
    $\phi \circ \Phi^{-1}(p) \geq \frac{p}{\sigma} \sqrt{\kappa_1 \log 1/p}$,  where $\kappa_{1} \geq 1 / 2, \phi, \Phi$ are respectively pdf and cdf for centered Gaussian distribution with variance $\sigma^2$.
\end{lemma}
\begin{proof}
Let's look the special case when $\sigma = 1$, which is proved in \cite{boucheron2012}. The fact that $\phi \circ \Phi^{-1}(p) \geq p \sqrt{\kappa_1 \log 1/p}$ follows from $\phi(x) - x \bar{\Phi(x)} \geq 0$. We can derive $\Phi^{-1}(p) \geq \sigma p/(1-p) \sqrt{\kappa_1 \log 1/p}$ for any $\sigma$. \\

From Lemma \ref{lemma mills' ratio}, we know $ \phi(x) \geq \frac{1}{\sigma^2}x \bar{\Phi}(x)$
\begin{align}
    \phi \circ \Phi^{-1}(p) \geq \frac{1}{\sigma^2} \Phi^{-1}(p) (1-p) \geq \frac{p}{\sigma} \sqrt{\kappa_1 \log 1/p}
\end{align}
\end{proof}


\begin{prop}
\label{prop: hazard bound for normal}
Absolute values of centered Gaussian random variables have a non-decreasing
hazard rate, for $y>0$,
\begin{align}
    \phi(\tilde{U}(\exp (y))) / \overline{\Phi}(\tilde{U}(\exp (y))) \geq \sigma \sqrt{\kappa_{1}(y+\log 2)}
\end{align}
 where $\kappa_{1} \geq 1 / 2, \phi, \Phi$ are respectively pdf and cdf for centered Gaussian distribution with variance $\sigma^2$. 
\end{prop}

\begin{proof}
\begin{align}
    \frac{\phi\left(\Phi^{-1}\left(1-e^{-y} / 2\right)\right)}{\overline{\Phi}\left(\Phi^{-1}\left(1-e^{-y} / 2\right)\right)}=\frac{\phi\left(\Phi^{-1}\left(e^{-y / 2}\right)\right)}{e^{-y} / 2} \geq \sigma \sqrt{\kappa_{1}(\log 2+y)}
\end{align}
\end{proof}


\begin{prop}
\label{prop 4.6}
Let $v_{n}=8 /(n \sigma^2 \log 2)$. $X_1, ..., X_n$ are independent random variables, distributed according to absolute value of Gaussian distribution with zero mean, and $X_{(1)} \geq X_{(2)} \geq ... \geq X_{(n)}$ denote the corresponding order statistics. $X_{n/2}$ is the median (we assume n is even). For all $0 \leq \lambda<n /\left(2 \sqrt{v_{n}}\right)$, 
\begin{align}
    \label{inequality Bernstein with log for normal}
    \log \mathbb{E}e^{\lambda\left(X_{(n / 2)}-\mathbb{E} X_{(n / 2)}\right)} \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}.
\end{align}
where the expectation is in terms of the randomness of the environment. For all $\varepsilon > 0$,
\begin{align} 
    \label{inequality Bernstein upper bound for normal}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right\} \leq e^{-\varepsilon}\\
    \label{inequality Bernstein lower bound for normal}
    \mathbb{P}\left\{\mathbb{E} X_{(n / 2)} - X_{(n / 2)}>\sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right\} \leq e^{-\varepsilon}
\end{align}
\end{prop}

\begin{proof}
By \cite{boucheron2012} Theorem 2.9, 
\begin{align}
    \log \mathbb{E} e^{\lambda\left(X_{(n / 2)}-\mathbb{E} X_{(n / 2)}\right)} \leq \frac{n}{4} \lambda \mathbb{E}\left[d_{n / 2}\left(e^{\lambda d_{n / 2}}-1\right)\right]
\end{align}
where $d_{n / 2}=X_{(n / 2)}-X_{(n / 2+1)} \sim \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)- \tilde{U}\left(e^{Y_{(n / 2+1)}}\right),$ where $E_{n/2}$ is standard exponentially distributed and independent of $Y_{(n/2 + 1)}$. $Y_{(1)} \geq Y_{(2)} \geq ... \geq Y_{(n)}$ is the order statistics of an independent sample of the standard exponential distribution. 

From Proposition \ref{prop non-increasing hazard rate} we know, if hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $U(\exp (t+x))-U(\exp (t)) \leq x / h(U(\exp (t)))$. Thus, 
\begin{align}
    \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)- \tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \leq \frac{E_{n / 2} /(n / 2)}{h(\tilde{U}(\exp{(Y_{(n/2+1)})}))}
\end{align}

From Proposition \ref{prop: hazard bound for normal}, 
\begin{align}
    h(U(\exp{(Y_{(n/2+1)})})) &= \frac{\phi(\widetilde{U}(\exp (Y_{(n/2+1)})))}{\overline{\Phi}(\widetilde{U}(\exp (Y_{(n/2+1)})))}\\
    &\geq \sigma \sqrt{k_1 (Y_{(n/2+1)} + \log 2)} \quad \text{where} \quad k_1 \geq 1/2,
\end{align}
so we get 
\begin{align}
    \triangle_{n/2} = \tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) 
    &\leq \frac{E_{n / 2} /(n / 2)}{\sigma \sqrt{k_1 (Y_{(n/2+1)} + \log 2)}}\\
    &\leq \frac{ \sqrt{2} E_{n / 2} }{(n\sigma / 2)\sqrt{(Y_{(n/2+1)} + \log 2)}}\\
    &\leq \frac{ \sqrt{2} E_{n / 2} }{(n \sigma / 2)\sqrt{ \log 2}}\\
    &= \sqrt{\frac{v_{n}}{n}} E_{n / 2},
\end{align}
where $v_{n}=8 /(n \sigma^2 \log 2)$.
Then our goal is to show that $\frac{n}{4} \lambda \mathbb{E}\left[\Delta_{n / 2}\left(e^{\lambda \Delta_{n / 2}}-1\right)\right] \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}$, which can be shown in the following,

\begin{align}
    & \lambda \mathbb{E}[\left(\tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \right) \left(e^{\lambda \left (\tilde{U}\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-\tilde{U}\left(e^{Y_{(n / 2+1)}}\right) \right)} - 1\right)]\\
    & \leq \lambda \mathbb{E}[\sqrt{\frac{v_{n}}{n}} E_{n / 2} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} E_{n / 2}} -1\right)]\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} \{ E_{n / 2}  = x \} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} \{E_{n / 2} = x\}} -1\right) f_{E_{n/2}}(x) d x\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} x \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} x } -1\right) e^{-x} d x\\
    & \leq \frac{2\lambda^2 v_n}{n (1 - 2 \lambda \sqrt{\frac{v_n}{n}})}
\end{align}
where the last step is because for $0 \leq \mu \leq 1/2$, $\int_{0}^{\infty} \mu x\left(e^{\mu x}-1\right) e^{-x} \mathrm{d} x=\frac{\mu^{2}(2-\mu)}{(1-\mu)^{2}} \leq \frac{2 \mu^{2}}{1-2 \mu}$.\\

(\ref{inequality Bernstein with log for normal}) is thus proved. Such a random variable satisfies a so-called Bernstien inequality \cite{boucheron2013}, for $\varepsilon > 0$, 
\begin{align}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right\} \leq e^{-\varepsilon}
\end{align}

If the $X_{n/2} \in \mathbb{R}$, the lower tail (\ref{inequality Bernstein lower bound for normal}) holds because of the symmetry (central limit theory). If the $X_{n/2} \in \mathbb{R^+}$ is positive, then we still have 
\begin{align}
    P\left( \mathbb{E}[ X_{n/2}] - X_{n/2} \geq \sqrt{2v_n \varepsilon} + 2 \varepsilon \sqrt{v_n/n} \right) \leq P\left( X_{n/2}-\mathbb{E}[ X_{n/2}] \geq \sqrt{2v_n \varepsilon} + 2 \varepsilon \sqrt{v_n/n} \right) \leq e^{-\varepsilon}
\end{align}
In this case, the bound for the lower tail is loose.\\ 

%\textcolor{red}{TODO}: discuss whether the above proof is correct. 
\end{proof}


\subsection{Concentration inequalities for Exponential distribution}


%Based on above analysis, the Exponential distribution can be a good candidate to try. Since its positive and it has the positive constant hazard rate (before the U transformation), and it can be easily represented by the U transformation.\\

For $\theta > 0$, PDF of exponential distribution:
\begin{align}
\label{Expon PDF}
    f(x, \theta) = \theta e^{-\theta x},
\end{align}

CDF of exponential distribution:
\begin{align}
\label{expon CDF}
    F(x, \theta) = 1 - e^{-\theta x},
\end{align}

hazard rate of exponential distribution:
\begin{align}
 h = f(x, \theta)/ (1-F(x, \theta)) = \theta
\end{align}


\begin{prop}
\label{prop for expon hazard rate}
Exponential distribution has constant hazard rate, for it's CDF F and PDF f defined as (\ref{expon CDF})(\ref{Expon PDF}), for $y > 0$,
\begin{align}
    \frac{f(U(\exp{y}))}{\bar{F}(U(\exp{y}))} = \frac{y}{\theta}
\end{align}
\end{prop}

\begin{proof}
\begin{align}
    \frac{f(U(e^{y}))}{\bar{F}(U(e^{y}))} &=
    \frac{f(F^{-1}(1 - e^{-y}))}{1-F\left({F}^{-1}\left(1 - e^{-y})\right)\right)}\\
    &= \frac{f(F^{-1}(1 - e^{-1}{y}))}{e^{-y}}
\end{align}
Note that $F^{-1}(y) = - \log (1 - y)/ \theta$, thus $F^{-1}(1 - e^{-1}{y}) = y/ \theta$. And $f(y/\theta) = \frac{y}{\theta} e^{-y}$,
so 
\begin{align}
    \frac{f(U(e^{y}))}{\bar{F}(U(e^{y}))} = \frac{\frac{y}{\alpha} e^{-y}}{e^{-y}} = \frac{y}{\theta}.
\end{align}
 
\end{proof}

\begin{prop}
\label{prop 4.6}

$X_1, ..., X_n$ are independent random variables, distributed according to exponential distribution's CDF F, and $X_{(1)} \geq X_{(2)} \geq ... \geq X_{(n)}$ denote the corresponding order statistics. $X_{n/2}$ is the median (we assume n is even). For all $0 \leq \lambda<n /\left(2 \sqrt{v_{n}}\right)$, 
\begin{align}
    \log \mathbb{E}e^{\lambda\left(X_{(n / 2)}-\mathrm{E} X_{(n / 2)}\right)} \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}
\end{align}
where the expectation is in terms of the randomness of the environment. For all $\varepsilon > 0$,
\begin{align} 
    \label{inequality Bernstein upper bound for exp}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right\} \leq e^{-\varepsilon}\\
    \label{inequality Bernstein lower bound for exp}
    \mathbb{P}\left\{\mathbb{E} X_{(n / 2)} - X_{(n / 2)}>\sqrt{2 v_{n} \varepsilon}+2 \varepsilon \sqrt{v_{n} / n}\right\} \leq e^{-\varepsilon}
\end{align}
\end{prop}

\begin{proof}
By \cite{boucheron2012} Theorem 2.9, 
\begin{align}
    \log \operatorname{E} e^{\lambda\left(X_{(n / 2)}-\mathbb{E} X_{(n / 2)}\right)} \leq \frac{n}{4} \lambda \mathbb{E}\left[d_{n / 2}\left(e^{\lambda d_{n / 2}}-1\right)\right]
\end{align}
where $d_{n / 2}=X_{(n / 2)}-X_{(n / 2+1)} \sim U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right).$ where $E_{n/2}$ is standard exponentially distributed and independent of $Y_{(n/2 + 1)}$. $Y_{(1)} \geq Y_{(2)} \geq ... \geq Y_{(n)}$ is the order statistics of an independent sample of the standard exponential distribution. 

From Proposition \ref{prop non-increasing hazard rate}, if hazard rate h is non-decreasing, then for all $t > 0$ and $x > 0$, $U(\exp (t+x))-U(\exp (t)) \leq x / h(U(\exp (t)))$. Thus, 
\begin{align}
    U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \leq \frac{E_{n / 2} /(n / 2)}{h(U(\exp{(Y_{(n/2+1)})}))}
\end{align}

From Proposition \ref{prop for expon hazard rate}, 
\begin{align}
    h(U(\exp{(Y_{(n/2+1)})})) &= \frac{f(U(\exp (Y_{(n/2+1)})))}{\overline{F}({U}(\exp (Y_{(n/2+1)})))} = \frac{Y_{(n/2+1)}}{\theta} 
\end{align}
so we get 
\begin{align}
    d_{n/2} = U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) 
    &\leq \frac{E_{n / 2} /(n / 2)}{\frac{Y_{(n/2+1)}}{\theta} }\\
    &\leq \frac{ 2\theta E_{n / 2} }{n Y_{(n/2+1)}}\\
    & \approx \frac{ 2\theta E_{n / 2} }{ n\log 2 }
\end{align}
where the last line holds with the assumption $Y_{(n/2+1)} \approx Y_{(n/2)} \approx \log2$. Let $v_n = \frac{4 \theta^2}{n (\log 2)^2}$, we have $d_{n/2} \leq \sqrt{\frac{v_n}{n}} E_{n/2}$. 

Then our goal is to show that $\frac{n}{4} \lambda \mathbb{E}\left[d_{n / 2}\left(e^{\lambda d_{n / 2}}-1\right)\right] \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}$, which can be shown in the following (same as the Gaussian case),

\begin{align}
    & \lambda \mathbb{E}[\left(U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \right) \left(e^{\lambda \left (U\left(e^{E_{n / 2} /(n / 2)+Y_{(n / 2+1)}} \right)-U\left(e^{Y_{(n / 2+1)}}\right) \right)} - 1\right)]\\
    & \leq \lambda \mathbb{E}[\sqrt{\frac{v_{n}}{n}} E_{n / 2} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} E_{n / 2}} -1\right)]\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} \{ E_{n / 2}  = x \} \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} \{E_{n / 2} = x\}} -1\right) f_{E_{n/2}}(x) d x\\
    & = \int_0^\infty \lambda  \sqrt{\frac{v_{n}}{n}} x \left(e^{\lambda \sqrt{\frac{v_{n}}{n}} x } -1\right) e^{-x} d x\\
    & \leq \frac{2\lambda^2 v_n}{n (1 - 2 \lambda \sqrt{\frac{v_n}{n}})}
\end{align}
where the last step is because for $0 \leq \mu \leq 1/2$, $\int_{0}^{\infty} \mu x\left(e^{\mu x}-1\right) e^{-x} \mathrm{d} x=\frac{\mu^{2}(2-\mu)}{(1-\mu)^{2}} \leq \frac{2 \mu^{2}}{1-2 \mu}$.

\end{proof}


    
% Policies
%----------------------------------------------------------------------------------------
    
\section{Policy and Regret Bound}

\subsection{Absolute Value for Gaussian Distribution}

\textbf{Proposed policy for absolute value of centered Gaussian distribution with variance $\sigma^2$}\\
In the (t+1) round, pick an arm with index 
\begin{align}
    \label{policy normal}
   \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}
\end{align}
where $\hat{m}_{i, T_i(t)}$ is the empirical median for arm i at the round t, $\varepsilon = 4 \log t$, $v_t = \frac{8}{T_i(t) \sigma^2 log2}$. $T_i(t)$ is the number of times arm i has been played until round t.  

The policy includes two parts, where the first part is the empirical median of the rewards, the second part is the confidence bound and we denote it as,

\begin{align}
\label{B_i(i, T_i(t)) definition}
B_i(t, T_i(t)) &= \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}\\
&= \sqrt{2 \times \frac{8}{T_i(t) \sigma^2 \log 2} \times 4 \log t} + 2 \times 4 \log t \times \sqrt{\frac{8}{T_i(t) \sigma^2 \log 2} \times \frac{1}{T_i(t)}}\\
&= \sqrt{\frac{64 \log t}{T_i(t) \sigma^2 \log 2}} + \sqrt{\frac{64 \log^2 t \times 8}{T_i^2(t)\sigma^2 \log 2}}\\
&= \frac{\sqrt{64 \log t T__i(t)} + 64 \log^2 t \times 8}{\sqrt{T_i^2(t) \sigma^2 \log 2}}\\
\label{B_i(i, T_i(t)) definition 2}
&= \frac{8 \sqrt{\log t} (\sqrt{ T_i(t)} + \sqrt{8\log t})}{\sigma T_i(t)  \sqrt{\log 2}}
\end{align}

Thus, the policy in (\ref{policy normal}) can be written as,
\begin{align}
    \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \frac{8 \sqrt{\log t} (\sqrt{ T_i(t)} + \sqrt{8\log t})}{\sigma T_i(t)  \sqrt{\log 2}}
\end{align}

\begin{theo}
(Regret bound for policy of absolute value of Gaussian distribution). \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$, where $P_i$ is Gaussian distribution with zero mean, then its expected regret after any number of n plays is at most 

\begin{align}
    \sum_{i: m_i < m_\ast} \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\sigma^2 \log 2 \times \triangle_{i, 1/2}} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
where $m_i$ is the median of $P_i$, $m_\ast$ is the median of the best arm (i.e. the arm with maximum median). $\triangle_{i, 1/2} = m_\ast - m_i$, $\beta = (64 + 32\sigma  \sqrt{2 \log 2} \triangle_{i,1/2} )\log n$. 
\end{theo}

\begin{proof}


\begin{lemma}
\label{Lemma 2 normal}
Let $l$ be an arbitrary positive integer, $\hat{m}_{*, s}$ is the empirical median of the reward samples of the best arm (i.e. the arm with maximum median) when it has been played s times, similarly $\hat{m}_{i, s_i}$ is the empirical median of the reward samples of arm i when it has been played $s_i$ times. $B_i(t, s_i)$ is defined as (\ref{B_i(i, T_i(t)) definition}). For $n \geq 1$, the number of arm i is chosen is bounded by
\begin{align}
    T_i(n) \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)\}
\end{align}
\end{lemma}
 
\begin{proof}
Let $I_t$ represent the arm we chose in the round t
    \begin{align}
        T_i(n) =& 1 + \sum_{t = k + 1}^n \{I_t = i\}\\
       \leq & l + \sum_{t = k + 1}^n (\{I_t = i\} \cup \{T_i(t-1) \geq l\})\\
       \label{proof: one step before minmax for normal}
       \leq & l + \sum_{t = k + 1}^n \left(\{ \hat{m}_{*, T_*(t-1)} + B_*(t, T_*(t-1)) \leq \hat{m}_{i, T_i(t-1)} + B_i(t, T_i(t-1)\} \cup \{ T_i(t-1) \geq l\}\right)\\
       \label{proof: minmax for normal}
       \leq & l + \sum_{t = k + 1}^n \{ \mathop{min}\limits_{0 < s < t}\hat{m}_{*, s} + B_*(t, s) \leq \mathop{max}\limits_{l < s_i < t}\hat{m}_{i, s_i} + B_i(t, s_i)\}\}\\
       \label{proof: union bound for normal}
       \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)\}
    \end{align}
    where the symbols with $*$ representing the properties of the best arm (i.e. the arm with maximum median). From step (\ref{proof: one step before minmax for normal}) to step (\ref{proof: minmax for normal}), we make use of the fact $l \leq T_i(t-1) < t$ and $0 < T_*(t-1) < t$. From step (\ref{proof: minmax for normal}) to (\ref{proof: union bound for normal}), we use the union bound. And in the step (\ref{proof: union bound for normal}), t is summed from 1 to infinity to make sure the arm i can be played for s times.\\
\end{proof}

\begin{lemma}
\label{lemma 3 normal}

Let $\hat{m}_{*, s}$ be the empirical median of the reward samples of the best arm (i.e. the arm with maximum median) when it has been played s times, $\hat{m}_{i, s_i}$, similarly, is the empirical median of the reward samples of arm i when it has been played $s_i$ times, where $s \geq 1, s_i \geq l$, l is an arbitrary integer,  $B_i(t, s_i)$ is defined as (\ref{B_i(i, T_i(t)) definition}).\\

    $\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)$ implies that at least one of the following must hold
    \begin{align}
        \label{lemma 3.1}
        \hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]\\
        \label{lemma 3.2}
        \hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}]\\
        \label{lemma 3.3}
        \mathbb{E}[\hat{m}_{*, s}] < \mathbb{E}[\hat{m}_{i, s_i}] + 2 B_i(t, s_i)
    \end{align}
\end{lemma}

\begin{proof}
Assume all of the three inequalities are not true, then we have
    \begin{align}
        \label{proof lemma 3.1}
         \hat{m}_{*, s} + B_*(t, s) >  \mathbb{E}[\hat{m}_{*, s}]\\
        \label{proof lemma 3.2}
        \hat{m}_{i, s_i} - B_i(t, s_i) < \mathbb{E}[\hat{m}_{i, s_i}]\\
        \label{proof lemma 3.3}
        \mathbb{E}[\hat{m}_{*, s}] \geq \mathbb{E}[\hat{m}_{i, s_i}] + 2 B_i(t, s_i)
    \end{align}
    (\ref{proof lemma 3.1}) - (\ref{proof lemma 3.2}) we get, 
    \begin{align}
         \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}] &< 
         \hat{m}_{*, s} + B_*(t, s) - (\hat{m}_{i, s_i} - B_i(t, s_i)) \\
        & \leq \hat{m}_{i, s_i} + B_i(t, s_i) - (\hat{m}_{i, s_i} - B_i(t, s_i)) \\
        &= 2 B_i(t, s_i),
    \end{align}
    which is contradicted to (\ref{proof lemma 3.3}), the assumption that all of the three inequalities are not true doesn't hold. Lemma \ref{lemma 3 normal} is proved to be true.\\
\end{proof}

\begin{lemma}
\label{Lemma 4: bound for E[T_i(n)]}
    The expected number of arm i is chosen for totally n rounds is bounded by 
    \begin{align}
        \mathbb{E}[T_i(n)] \leq  \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\log 2 \times \triangle_{i, 1/2}^2} + 1 + \frac{\pi^2}{3}
    \end{align}
    where $\triangle_{i, 1/2} = m_\ast - m_i = \mathbb{E}[\hat{m}_{\ast, T_i(n)}] - \mathbb{E}[\hat{m}_{i,T_i(n)}]$, $\beta = (64 + 32 \sqrt{2 \log 2} \triangle_{i,1/2} )\log n$.
\end{lemma}

\begin{proof}
To give a bound for $\mathbb{E}[T_i(n)]$, from Lemma \ref{Lemma 2 normal}, we know we only need to manage give a bound of $\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)$. From Lemma \ref{lemma 3 normal} we know, if we make (\ref{lemma 3.3}) false, then 
\begin{align}
    & P(\hat{m}_{*, s} + B_*(t, s)  \leq \hat{m}_{i, s_i} + B_i(t, s_i)) \\
    \leq &  P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) + P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])
\end{align}
According to Lemma 1, we bound the probability of (\ref{lemma 3.1})(\ref{lemma 3.2}) as
    \begin{align}
        P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) \leq  e^{-4\log t} = t^{-4}\\
        P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])  \leq  e^{-4\log t} = t^{-4}
    \end{align}
    
    Then our goal is to find the value of l to make (\ref{lemma 3.3}) false, i.e. 
    \begin{align}
    \label{goal to find l}
        B_i(t, s_i) \leq  \frac{1}{2}(\mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}])
    \end{align}
    Remind that $\triangle_{i, 1/2} = m_* - m_i = \mathbb{E}[\hat{m}_{*, s}] - \mathbb{E}[\hat{m}_{i, s_i}]$, then (\ref{goal to find l}) can be written as $B_i(t, s_i) \leq  \frac{1}{2} \triangle_{i, 1/2}$. According to (\ref{B_i(i, T_i(t)) definition 2}) and with $t \leq n, s_i \geq l \geq 1$, 
    \begin{align}
        B_i(t, s_i) &= \frac{8 \sqrt{\log t} (\sqrt{ s_i} + \sqrt{8\log t})}{\sigma s_i  \sqrt{\log 2}}\\
        & \leq \frac{8 \sqrt{\log n}}{ \sigma \sqrt{\log 2}} \times \frac{\sqrt{s_i} + \sqrt{8\log n}}{s_i}\\
        & \leq \frac{8 \sqrt{\log n}}{\sigma \sqrt{\log 2}} \times \frac{s_i + \sqrt{8\log n}}{s_i}
    \end{align}
    By solving $\frac{8 \sqrt{\log n}}{\sigma \sqrt{\log 2}} \times \frac{s_i + \sqrt{8\log n}}{s_i} \leq \frac{1}{2} \triangle_{i, 1/2}$, when $\sigma \triangle_{i, 1/2} \sqrt{\log 2} \geq 16 \sqrt{\log n}$ , 
    
    \begin{align}
        s_i \geq \frac{32 \sqrt{2} \log n}{\sigma \sqrt{\log 2} \triangle_{i, 1/2} - 16 \sqrt{\log n}} 
    \end{align}
    
    However this assumption doesn't hold in most of the cases, so we need to solve the inequality by directly solving $\frac{8 \sqrt{\log n}}{\sigma \sqrt{\log 2}} \times \frac{\sqrt{s_i} + \sqrt{8\log n}}{s_i} \leq 1/2 \triangle_{i, 1/2}$, let $\beta = (64 + 32 \sqrt{2 \log 2}\sigma \triangle_{i,1/2} )\log n$,
    \begin{align}
        s_i \geq \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\sigma^2 \log 2 \times \triangle_{i, 1/2}^2} 
    \end{align}
    
    Then we have $l'$ which makes (\ref{lemma 3.3}) false,
   \begin{align}
       l' = \lceil \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\sigma^2 \log 2 \times \triangle_{i, 1/2}^2} \rceil = \lceil \frac{(128 + 32 \sqrt{2 \log 2} \sigma \triangle_{i, 1/2} + 64 \sqrt{4 + 2 \sqrt{2 \log 2}\sigma \triangle_{i, 1/2}}) \log n}{\log 2 \triangle_{i, 1/2}^2} \rceil
   \end{align} 
   
    So we get the bound of $\mathbb{E}[T_i(n)]$ as,
    \begin{align}
        \mathbb{E}[T_i(n)] &\leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l'}^{t-1} P(\hat{m}_{*, s} + B_*(t, s) \leq  \mathbb{E}[\hat{m}_{*, s}]) + P(\hat{m}_{i, s_i} - B_i(t, s_i) \geq \mathbb{E}[\hat{m}_{i, s_i}])\\
        & \leq l' + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = 1}^{t-1} 2 t^{-4}\\
        & \leq \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\sigma^2 \log 2 \times \triangle_{i, 1/2}^2} + 1 + \frac{\pi^2}{3}
    \end{align}
   
\end{proof}

According to Lemma \ref{Lemma 4: bound for E[T_i(n)]}, we derive the upper bound for expected regret,
\begin{align}
    \mathbb{E}[R_n] \leq 
          \sum_{i: m_i < m_\ast} \frac{(\sqrt{\beta} + 8 \sqrt{\log n})^2}{\sigma^2 \log 2 \times \triangle_{i, 1/2}} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
   
\end{proof}
%----------------------------------------------------------------------------------------
\subsection{Exponential Distribution}

\textbf{Proposed policy for exponential distribution}\\

In the (t+1) round, pick an arm with index 
\begin{align}
   \label{policy expon}
   \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} + \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}
\end{align}
where $\hat{m}_{i, T_i(t)}$ is the empirical median for arm i at the round t, $\varepsilon = 4 \log t$, $v_t = \frac{4 \theta^2}{T_i(t) (\log 2)^2}$, where $\theta$ is the parameter of exponential distribution, $T_i(t)$ is the number of times arm i has been played until round t.  

The policy includes two parts, where the first part is the empirical median of the rewards, the second part is the confidence bound and we denote it as,

\begin{align}
%\label{B_i(i, T_i(t)) definition}
B_i(t, T_i(t)) &= \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)}\\
&= \sqrt{2 \times \frac{4 \theta^2}{T_i(t) (\log 2)^2} \times 4 \log t} + 2 \times 4 \log t \times \sqrt{\frac{4 \theta^2}{T_i(t) (\log 2)^2} \times \frac{1}{T_i(t)}}\\
&= \frac{4 \theta \sqrt{\log t} ( \sqrt{ 2T_i(t)} + 4\sqrt{\log t})}{T_i(t) \log 2}
\end{align}

Thus, the policy in (\ref{policy expon}) can be written as,
\begin{align}
    \argmax_{i \in \mathcal{K}} \hat{m}_{i, T_i(t)} +  \frac{4 \theta \sqrt{\log t} ( \sqrt{ 2T_i(t)} + 4\sqrt{\log t})}{T_i(t)\log 2}
\end{align}

\begin{theo}
(Regret bound for policy of exponential distribution). \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$ having non-decreasing hazard rate, then its expected regret after any number of n plays is at most 

\begin{align}
    \sum_{i: m_i < m_\ast} \frac{32 \theta \log t (\sqrt{2 \theta + \triangle_{i, 1/2} (\log 2)^2} + \sqrt{\theta})^2}{ \triangle_{i, 1/2} (\log 2)^2} + (1 + \frac{\pi^2}{3}) (\sum_{j=1}^K \triangle_{j, 1/2}) 
\end{align}
where $m_i$ is the median of $P_i$, $m_\ast$ is the median of the best arm (i.e. the arm with maximum median). $\triangle_{j, 1/2} = m_\ast - m_j$. $\theta > 0$ is the parameter of exponential distribution. 
\end{theo}

Some considerations:

\begin{itemize}

\item Is $Y_{(n/2 + 1)}$ a random variable or a scalar?\\
$Y_{(n/2 + 1)}$ is the order statistic of samples of standard exponential distribution, so scalar. However, it is just a sample, which means $Y_{(n/2)}$ is only the median of the samples, but not the median of standard exponential distribution. 

\item Can $Y_{(n/2 + 1)}$ be replaced by $Y_{(n/2)}$?\\
Technically, no. On the one hand, $Y_{(n/2 + 1)} \leq Y_{(n/2 )}$, which means  $d_{n/2} \leq \frac{ 2\theta E_{n / 2} }{n Y_{(n/2+1)}} \geq  \frac{ 2\theta E_{n / 2} }{n Y_{(n/2)}} $. 
On the other hand, $d_{n/2 - 1} \leq \frac{ 2\theta E_{n / 2 - 1} }{n Y_{(n/2)}} $, which eventually leads to the tail bound for $X_{(n/2 + 1)}$, not median anymore. 

\item Can we still make assumption for $Y_{n/2 + 1}$?\\
yes? we have $\lim_{n \rightarrow \infty} Y_{n/2} = \log 2$
then $\lim_{n \rightarrow \infty} Y_{n/2 + 1} \simeq \log 2$. 
So we can assume $Y_{n/2+1} = \log2$?
However, the assumption may not be true when n is small, which means at the first few rounds of bandit algorithm, the assumption may not hold.
\end{itemize}

%----------------------------------------------------------------------------------------
\subsection{Arbitrary Distribution}

\textbf{Proposed policy for arbitrary distribution}\\
In the (t+1) round, pick an arm with index 
\begin{align}
   \argmax_{i \in \mathcal{K}} \hat{Q}_{i, T_i(t)}(\alpha) + \frac{p}{\lambda} - \frac{q}{2} \mathbb{E}[d_{q} (\exp\{\lambda d_{q}\} - 1)]
\end{align}
where $p = 4 \log t$, $q = T_i(t) - \lceil \alpha T_i(t) \rceil$, $\lambda = \sup \{ \lambda \geq 0 | \lambda p - \log \mathbb{E}[\exp\{\lambda (\hat{Q}_{i, T_i(t)}(\alpha) - \mathbb{E}\hat{Q}_{i, T_i(t)}(\alpha))\}]\}$ And $d_q$ is the difference between the $\{T_i(t) - q\}^{th}$ and $\{T_i(t) - (q + 1)\}^{th}$ order statistics,
\begin{align}
    d_q = Y_{i,T_i(t) - q} - Y_{i,T_i(t) - (q + 1)} = Y_{i, \lceil \alpha T_i(t) \rceil} - Y_{i, \lceil \alpha T_i(t) \rceil - 1}
\end{align}
For simplicity, we denote $B_i(t, T_i(t), \alpha) = \frac{p}{\lambda} - \frac{q}{2} \mathbb{E}[d_{q} (\exp\{\lambda d_{q}\} - 1)]$\\

\begin{defi}
(Hazard Rate)\cite{boucheron2012}. The hazard rate of an absolutely continuous probability
distribution with distribution function F is: $h = f /\bar{F}$ where f and $\bar{F} = 1 - F$ are respectively the density and the survival function associated with F.\\
\end{defi}

\begin{theo}
(Regret bound). \\

For all K $>$ 1, if the proposed policy is run on K machines with absolute reward distribution $v = (P_i: i \in \mathcal{K})$ having non-decreasing hazard rate, then its expected regret after any number of n plays is at most ... \\
\end{theo}

\begin{proof}

\begin{lemma}
Let $X_1,..., X_n$ be independently distributed according to cumulative density distribution F, $Y_{1} \leq \ldots \leq Y_{n}$ be the order statistics and let $d_q = Y_{n-q} - Y_{n-q-1}$ be the $q^{th}$ spacing. Then with $q = n - \lceil \alpha n \rceil (i.e. 1 - \frac{q+1}{n} \leq \alpha \leq 1 - \frac{q}{n})$, if F has a non-decreasing hazard rate h, then for $\lambda \geq 0$, and $1\leq k \leq n/2$,
\begin{align}
    \label{Boucheron theorem 9-2}
    \log \mathbb{E}e^{\lambda\left(\Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)]\right)} \leq \lambda \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]
\end{align}
And tail bound is 
\begin{align}
    \label{Lemma 1-2}
   P\{\Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)] \geq \frac{\varepsilon}{\lambda} - \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]\} \leq e^{-\varepsilon}\\
   \label{Lemma 1-3}
   P\{\mathbb{E}[ \Hat{Q}(\alpha)] - \Hat{Q}(\alpha) \geq \frac{\varepsilon}{\lambda} - \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right]\} \leq e^{-\varepsilon}
\end{align}
\end{lemma}

\begin{proof}
(\ref{Boucheron theorem 9-2}) can be derived from \cite{boucheron2012}(Theorem 2.9).  Let $S = \Hat{Q}(\alpha)-\mathbb{E}[ \Hat{Q}(\alpha)]$, then according to Markov's inequality,
\begin{align}
    P \{S \geq t\} \leq \frac{\mathbb{E}[e^{\lambda S}]}{e^{\lambda t}} \leq \exp\{\lambda \frac{q}{2} \mathbb{E}\left[d_q\left(e^{\lambda d_q}-1\right)\right] - \lambda t\},
\end{align}
from which we can derive (\ref{Lemma 1-2}). The lower tail (\ref{Lemma 1-3}) can be similarly proved (\textcolor{red}{how??}).
\end{proof}

\begin{lemma}
Let $I_t$ represent the arm we chose in the round t and $l$ as an arbitrary positive integer. For $t \geq 1$, the number of arm r is chosen is bounded by
\begin{align}
    T_i(n) \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}
\end{align}
\end{lemma}
 
\begin{proof}
    \begin{align}
        T_i(n) =& 1 + \sum_{t = k + 1}^n \{I_t = i\}\\
               \leq & l + \sum_{t = k + 1}^n (\{I_t = i\} \cup \{T_i(t-1) \geq l\})\\
               \leq & l + \sum_{t = k + 1}^n (\{ \hat{Q}_{*, T_*(t-1)}(\alpha) + B_*(t, T_i(t-1), \alpha) \leq \hat{Q}_{i, T_i(t-1)}(\alpha) + B_i(t, T_i(t-1), \alpha)\} \cup \{ T_i(t-1) \geq l\})\\
               \label{proof: minmax for arb}
               \leq & l + \sum_{t = k + 1}^n \{ \mathop{min}\limits_{0 < s < t}\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq \mathop{max}\limits_{l < s_i < t}\hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}\}\\
               \label{proof: union bound for arb}
               \leq & l + \sum_{t = 1}^\infty \sum_{s = 1}^{t-1} \sum_{s_i = l}^{t-1} \{\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)\}
    \end{align}
    where the symbols with $*$ representing the properties of the best arm (i.e. the arm with maximum $\alpha-$quantiles $Q_i(\alpha)$).From step  From step (\ref{proof: minmax}) to (\ref{proof: union bound}), we use the union bound. And in the step (\ref{proof: union bound}), t is summed from 1 to infinity to make sure the arm i can be played for s times.\\
\end{proof}

\begin{lemma}
\label{lemma 3}
    $\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)$ implies that at least one of the following must hold\\
    \begin{align}
        \label{lemma 1.1}
        \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]\\
        \label{lemma 1.2}
        \hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)]\\
        \label{lemma 1.3}
        \mathbb{E}[\hat{Q}_{*, s}(\alpha)] < \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] + 2 B_i(t, s_i, \alpha)
    \end{align}
\end{lemma}

\begin{proof}
Assume all of the three inequalities are not true, then we have
    \begin{align}
        \label{proof lemma 1.1}
         \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) >  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]\\
        \label{proof lemma 1.2}
        \hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) < \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)]\\
        \label{proof lemma 1.3}
        \mathbb{E}[\hat{Q}_{*, s}(\alpha)] \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] + 2 B_i(t, s_i, \alpha)
    \end{align}
    (\ref{proof lemma 1.1}) - (\ref{proof lemma 1.2}) we get, 
    \begin{align}
         \mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] &< 
         \hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) - (\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha)) \\
        & \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha) - (\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha)) \\
        &= 2 B_i(t, s_i, \alpha),
    \end{align}
    which is contradicted to (\ref{proof lemma 1.3}), the assumption that all of the three inequalities are not true doesn't hold. Lemma 1 is proved to be true.\\
\end{proof}

\begin{lemma}
    The expected number of arm i is chosen is bounded by 
    \begin{align}
        \mathbb{E}[T_i(n)] \leq ?
    \end{align}
\end{lemma}

\begin{proof}
To give a bound for $\mathbb{E}[T_i(n)]$, we only need to manage give a bound of $\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)$. From Lemma \ref{lemma 3} we know, if we make (\ref{lemma 1.3}) false, then 
\begin{align}
    & P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha)  \leq \hat{Q}_{i, s_i}(\alpha) + B_i(t, s_i, \alpha)) \\
    \leq &  P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]) + P(\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)])
\end{align}
According to Lemma 1, we bound the probability of (\ref{lemma 1.1})(\ref{lemma 1.2}) as
    \begin{align}
        P(\hat{Q}_{*, s}(\alpha) + B_*(t, s, \alpha) \leq  \mathbb{E}[\hat{Q}_{*, s}(\alpha)]) \leq  e^{-4lnt} = t^{-4}\\
        P(\hat{Q}_{i, s_i}(\alpha) - B_i(t, s_i, \alpha) \geq \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)])  \leq  e^{-4lnt} = t^{-4}
    \end{align}
    
    For $l = ...$, (\ref{lemma 1.3}) is false.
    \begin{align}
        & \mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] - 2 B_i(t, s_i, \alpha) \\
        = &\mathbb{E}[\hat{Q}_{*, s}(\alpha)] - \mathbb{E}[\hat{Q}_{i, s_i}(\alpha)] - 2 ... \\
        \geq & \mu^\ast - \mu_i - \triangle_i = 0
    \end{align}
    %(\textcolor{red}{Cannot derive a value of l to satisfy that?})
\end{proof}
   
\end{proof}

%---------------------------------------------------------------------------------
\section{Considerations}

\subsection{Why absolute Gaussian?}

The hazard rate $\phi(x) / \bar{\Phi}(x)$ of the Gaussian distribution tends to 0 as x tends to $- \infty$ , the approach in  \cite{boucheron2012} Proposition 4.6 does not work when dealing with order statistics of Gaussian samples (If the hazard rate tends to 0, then Prop 4.1 cannot hold anymore).

Another possible reason might be, hazard rate is defined on positive values.

Not sure whether we can extend the proof into non-absolute values.

\subsection{How about non-standard Gaussian?}
We've shown the proof works for centered Gaussian with arbitrary variance. However, if we consider the Gaussian distribution with non-zero mean, we cannot get rid of the term $\text{erf}(\frac{x + \mu}{\sigma \sqrt{2}})$ easily, which means it's hard to represent the quantile of the absolute non-standard Gaussian with $\Phi$, and thus cannot direct make use of the fact of $p \sqrt{\kappa_{1} \log 1 / p} \leq \phi \circ \Phi^{-1}(p)$.

\subsection{Other distributions?}
The key point of change the absolute value of Gaussian random variable with other random variables is we can have some lower bound (positive) for the hazard rate (in terms of the U transformation). 

This also explains why Gaussian random variables cannot work for such kind of proof, when x tends to negative infinity, the hazard rate of the Gaussian distribution tends to 0, which makes $\Delta_{n/2}$ bounded by positive infinity. So only with the assumption of non-decreasing hazard rate, the above proof works bad. 


%-------------------------------------------------------------------------------

\section*{Appendix}

\textbf{Logic of Proposition 4.5}\\

Known
\begin{align}
    t \leq  \frac{\lambda}{2} \mathbb{E}_{X, Y}[g(X, Y)]
\end{align}

and define
\begin{align}
    \mathbb{E}_X[g(X, Y = y)| Y = y] \leq \varphi (y)\\
    \rho = \max_{\eta} \mathbb{E}_X[g(X, Y = \eta)|Y = \eta]
\end{align}

The conditional expectation $\mathbb{E}_X[g(X, Y)| Y]$ is a non-decreasing function of Y.\\

The goal is to prove the target t satisfies the inequality:\\
\begin{align}
    t \leq \lambda \varphi(\tau) + \frac{\lambda}{2} \rho \mathbb{P}(Y \leq \tau)
\end{align}
Proof:
\begin{align}
    t & \leq  \frac{\lambda}{2} \mathbb{E}_{X, Y}[g(X, Y)]\\
    & = \frac{\lambda}{2}\int _0^{\infty} \int_0^{\infty} p(X = x, Y = y) g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} \int_0^{\infty} p(X = x|Y = y) p(Y = y)g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} p(Y = y) \int_0^{\infty} p(X = x|Y = y) g(X = x, Y = y) dxdy\\
    & = \frac{\lambda}{2}\int _0^{\infty} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy\\
    & = \frac{\lambda}{2}\left(\int _0^{\tau} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy + \int _\tau^{\infty} p(Y = y) \mathbb{E}_X[g(X, Y = y)| Y = y]dy\right)\\
    & \leq \frac{\lambda}{2}\left(\int _0^{\tau} p(Y = y) dy \times \mathbb{E}_X[g(X, Y = 0)| Y = 0] + \int _\tau^{\infty} p(Y = y) dy \times \mathbb{E}_X[g(X, Y = \tau)| Y = \tau] \right)\\
    & = \frac{\lambda}{2}\left(p(Y \leq \tau) \times \mathbb{E}_X[g(X, Y = 0)| Y = 0] + p(Y > \tau) \times \mathbb{E}_X[g(X, Y = \tau)| Y = \tau] \right)\\
    & \leq \frac{\lambda}{2} \mathbb{E}_X[g(X, Y = \tau)|Y = \tau] + \frac{\lambda}{2} \mathbb{P}(Y \leq \tau) \max_{\eta} \mathbb{E}_X[g(X, Y = \eta)|Y = \eta]\\
    & \leq \lambda \varphi(\tau) + \frac{\lambda}{2} \rho \mathbb{P}(Y \leq \tau)
\end{align}

From (122) to (123), we 

Note the corresponding relationships to the paper are:\\

$t = \log \mathbb{E} [e^{\lambda (X_{(1)} - \mathbb{E}[X_{(1)}])}]$, $X = E_1, Y = Y_{(2)}$ are both standard exponential random variables. 

%---------------------------------------------------------------------------------
% To be deleted

\begin{prop}
Let $\hat{m}_{i, T_i(t)}$ be the empirical median of arm i in round t, sampled from the absolute value of Gaussian distribution with zero mean. $T_i(t)$ represents the number of samples for arm i in the $t$ round. Let $v_t = 8/(T_i(t) \log 2)$. For all $ 0 \leq \lambda \leq T_i(n)/(2 \sqrt{v_t})$,

\begin{align}
    \label{Boucheron propo 4.6}
    \log \mathbb{E} e^{\lambda\left(\hat{m}_{i, T_i(t)}-\mathbb{E}[ \hat{m}_{i, T_i(t)}]\right)} \leq \frac{v_t \lambda^2}{2(1-2\lambda \sqrt{v_t/T_i(t)})} ,
\end{align}
where the expectation is taken with respect to the measure on outcomes induced by the interaction of $\pi$ and $v$. For all $t > 0$, the tail bound is 
\begin{align}
    \label{Lemma 5 tail bound 1}
   P\left( \hat{m}_{i, T_i(t)}-\mathbb{E}[ \hat{m}_{i, T_i(t)}] \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq e^{-\varepsilon}\\
   \label{Lemma 5 tail bound 2}
   P\left( \mathbb{E}[ \hat{m}_{i, T_i(t)}] - \hat{m}_{i, T_i(t)} \geq \sqrt{2v_t \varepsilon} + 2 \varepsilon \sqrt{v_t/T_i(t)} \right) \leq e^{-\varepsilon}\\
\end{align}
\end{prop}

\printbibliography
\end{document}
