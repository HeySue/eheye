\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[mathscr]{eucal}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\urlstyle{same}
 

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\title{Takeaway points from papers}
\author{u6015325 }
\date{March 2019}
\bibliography{ref.bib}

\begin{document}

\maketitle

\textbf{Notes:} the notation among papers is not consistent, since I just copy the equation from papers to make the notes self-contained. And some of the definition of notations is not given in the notes (otherwise, it will make the notes too long), please refer to the paper if needed. 

\section{Auer 2002}
\href{https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf}{Finite-time Analysis of the Multiarmed Bandit Problem} \cite{auer2002finite}

\textbf{UCB1}: constructs the Upper
Confidence Bound (UCB) for arm k at time t by adding the bias factor
\begin{align}
    \label{ucb1 confidence bound}
    \sqrt{\frac{2 b^{2} \log t}{T_{k}(t-1)}}
\end{align}
to its sample-mean. They proved that the expected regret of this algorithm satisfies

\begin{align}
    \label{ucb1 regret bound}
    \mathbb{E}\left[\hat{R}_{n}\right] \leq 8\left(\sum_{k : \mu_{k}<\mu^{*}} \frac{b^{2}}{\Delta_{k}}\right) \log (n)+O(1)
\end{align}

\textbf{UCB-Tuned:}
First define an upper confidence bound for the variance of machine j. This means that machine j, which has been played s times during the first t plays, has a variance that is at most the sample variance plus $\sqrt{2 lnt/s}$
\begin{align}
    V_{j}(s) \stackrel{\mathrm{def}}{=}\left(\frac{1}{s} \sum_{\tau=1}^{s} X_{j, \tau}^{2}\right)-\overline{X}_{j, s}^{2}+\sqrt{\frac{2 \ln t}{s}}
\end{align}

Then replace (\ref{ucb1 confidence bound}) with 
\begin{align}
    \sqrt{\frac{\ln n}{n_{j}} \min \left\{1 / 4, V_{j}\left(n_{j}\right)\right\}}
\end{align}

The UCB-Tuned includes estimates of variance into the UCB1 algorithm, which performs better than UCB1 but \cite{auer2002finite} didn't prove a regret bound.\\

Comment:\\
However UCB-Tuned is only for Bernoulli random variables, where 1/4 is the upper bound on the variance of a Bernoulli random variable. For Gaussian, our experiment shows it performs worse than UCB1 ($ \min \left\{1 / 4, V_{j}\left(n_{j}\right)\right\}$ is turned to just $V_{j}\left(n_{j}\right)$). 

\section{Audibert 2009}

Exploration–exploitation tradeoff using variance estimates in multi-armed bandits \cite{AUDIBERT20091876}

\begin{enumerate}
    \item This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. 
    
    \textbf{UCB-V:}
    
    \begin{align}
        \sqrt{\frac{2 V_{k, T_{k}(t-1)} \mathcal{E}_{T_{k}(t-1), t}}{T_{k}(t-1)}}+c \frac{3 b \mathcal{E}_{T_{k}(t-1), t}}{T_{k}(t-1)}
    \end{align}
    
    where $V_{k,s}$ is the empirical variance estimate for arm k based on s samples,
    $\mathcal{E} = \mathcal{E}_{\cdot,\cdot}$ (viewed as a function of (s, t)) is the so-called exploration function. A typical choice for this function is $\mathcal{E}_{s,t} = \varsigma log(t)$. With this choice the algorithm’s behavior is controlled by the parameters $\varsigma, c > 0$.
    
    For c = 1 and $\varsigma$ = 1.2, 
    \begin{align}
        \mathbb{E}\left[\hat{R}_{n}\right] \leq 10 \sum_{k : \mu_{k}<\mu^{*}}\left(\frac{\sigma_{k}^{2}}{\Delta_{k}}+2 b\right) \log (n)
    \end{align}
    The main difference to the bound (\ref{ucb1 regret bound}) is that $b^2$ is replaced by $\sigma_k^2$ . However, notice that b still appears in the bound.
    
    \item In order to prove the above result they proved a novel tail bound on the sample average of i.i.d. random variables with bounded support. Unlike previous similar bounds, this bound uses the empirical variance and thus it might be of independent interest (corresponding to Theorem 1 in the original paper).
    
    Let $X_1 , . . . , X_t$ be i.i.d. random variables taking their values in [0, b]. Let $\mu = \mathbb{E}[X_1]$ be their common expected value. Consider the empirical mean $\bar{X}_t$ and variance $V_t$ defined respectively by
    $$\overline{X}_{t}=\frac{\sum_{i=1}^{t} X_{i}}{t} \quad \text { and } \quad V_{t}=\frac{\sum_{i=1}^{t}\left(X_{i}-\overline{X}_{t}\right)^{2}}{t}$$
    Then, for any $t \in N$ and $x > 0$, with probability at least $1-3e^{-x}$,
    \begin{align}
        \label{tail bound for ucbv}
        \left|\overline{X}_{t}-\mu\right| \leq \sqrt{\frac{2 V_{t} x}{t}}+\frac{3 b x}{t}
    \end{align}
    
    \item Theorem 2 says something about the sampling times of suboptimal arms (I haven't think and study much about that, but could be a good direction to think about).
    
    \item The paper also analyzed the risk that the
regret of the studied algorithm is much higher than its expected value. It is about concentration of regret distribution. To do that, then defined the (cumulative) pseudo-regret
\begin{align}
    R_{n}=\sum_{k=1}^{K} T_{k}(n) \Delta_{k}
\end{align}
\end{enumerate}

Considerations:

\begin{enumerate}
    \item The UCB-V policy looks like the improved version of UCB-Tuned with theoretical proof of regret. However, \cite{garivier2011kl} reported that UCB-V performs much worse than UCB-Tuned in most cases (need to figure out why). 
    
    \item In order to prove the QuantUCB regret bound, we should first prove (or if it exists, that'd be better) a tail bound on the sample average of i.i.d. random variables (with bounded support) similar as (\ref{tail bound for ucbv}), but with quantiles on the right hand side rather than variance. But this might not be possible, since quantile is not the natural term to describe the deviations of empirical mean from expectation (but variance is, and has been proved in Bernstein inequality). Without concentration proof, just trying multiply terms outside of quantiles doesn't sound good. 
    
    \item Another idea is to replace the variance using quantile differences in (\ref{tail bound for ucbv}), rather than just quantiles. That is, we want to prove
    $$\left|\overline{X}_{t}-\mu\right| \leq \sqrt{\frac{2 (Q_t^u(\alpha) - Q_t^l(\alpha)) x}{t}}+\frac{3 b x}{t},$$
    where $Q_t^u(\alpha)$ and $Q_t^l(\alpha)$ represents the upper and lower quantile function evaluated at $\alpha$ respectively (The quantile level can be same for different time step t?). The right hand side can be slightly different.
    
    To figure out whether it's possible, we need to first understand the proof of Theorem 1 in \cite{AUDIBERT20091876}. However, my feeling is that even if we can prove the relationship between expectation concentration and quantile differences, we need to reply on some other proof rather than \cite{AUDIBERT20091876}, since variance and quantile differences are still very different.
    
    I suspect different choices of quantile level $\alpha$ can give us different risks of regret. 
    
    \item The value at risk is used to measure the risk of the regret distribution. It is interesting. But a different direction is to think about whether using the value at risk to measure the reward distributions of arms could be interesting. It differs from the classic bandit problems. But using quantiles this way might be a more natural choice. That is, to measure true quantile - empirical quantile,  rather than true mean - empirical mean. 
\end{enumerate}

\section{The KL-UCB Algorithm }
\href{https://arxiv.org/abs/1102.2490}{The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond} \cite{garivier2011kl}
\begin{enumerate}
    \item KL-UCB reaches the lower-bound of Lai and Robbins (1985) for binary rewards (optimal).
    \item KL-UCB is a general-purpose procedure for bounded bandits. In terms of the bounded rewards experiment in the paper, KL-UCB-exp and UCB-Tuned is most efficient, where UCB-Tuned performs slightly better. It leads us to think about for non-binary case, including variance/quantiles might be useful.
    \item UCB-Tuned is risky, see figure one right panel. (the variance of the distribution of the number of draws of the suboptimal arm is high).
    \item They claimed that results obtained from N = 1,000 independent runs or less are not reliable. And results obtained in configuration where N is much smaller than the number of rounds n are likely to be unreliable. In our current setting, N = 50, n = 5,000, which might be unreliable according to their claim.
\end{enumerate}



\section{Bandits With Heavy tail} 
\href{http://ieeexplore.ieee.org/document/6576820/}{Bandits With Heavy Tail}\cite{bubeck2013bandits}
\begin{enumerate}
    \item The Hoeffding inequality turns out to be crucial in
order to achieve a regret of optimal order. However, when the
sub-Gaussian assumption does not hold, one cannot expect the
empirical mean to have such an accuracy.
    \item The key to successful handling heavy-tailed reward distributions is to replace the empirical mean with other, more robust estimators of the mean, where in the paper three estimator is used, namely median-of-means, truncated mean, and Catoni’s -estimator.
    \item The proof structure is:
        \begin{enumerate}
            \item first make \textit{Assumption 1} to bound the difference between estimated mean and the true mean (Hoeffding type, but without proof and without specify the estimator). \textit{Robust UCB} algorithm is proposed based the assumption. Interestingly, the assumption may be satisfied for significantly more general distributions (not only suits sub-gaussian) by using more sophisticated mean estimators. The basic requirement for Assumption 1 to be satisfied is that the distribution of the $X_t$ has a finite moment of order $1 + \varepsilon$.
            \item Give a regret bound for \textit{Robust UCB} policy. Proofs are similar as the typical UCB proof, expect the confidence bound is different. 
            \item The only remaining thing is to prove the proposed estimators satisfy \textit{Assumption 1}. 
                \begin{itemize}
                    \item Truncated Empirical Mean: proved with the assumption of the ($1+ \varepsilon$)th raw moment is bounded, using Bernstein’s inequality for bounded random variables.
                    \item Median of Means: The simple idea is to divide the data into various disjoint blocks. Within each block one calculates the standard empirical mean and takes a median value of these empirical means. This approach solves the heavy tail problem with the Hoeffding inequality with a binomial distribution (within or without the blocks). 
                \end{itemize}
        \end{enumerate}
\end{enumerate}

\textbf{Considerations}: heavy tail distributions can be handled by carefully design the estimator, still using the our favourite  Hoeffding type bounds. So for our QuantUCB, can we also extend our approach to heavy tails easily by designing estimators like the Bubeck did (not only constrained as subgaussian)? 

\section{Bandits with risk measure}
Quantiles and CVaR are used in multiarmed bandit problem with respect to risk measures:\cite{Maillard2013}  considers so-called coherence risk measures (CVaR, is one example of such a risk measure), and with an approach where the regret itself is redefined. They studied the deviations of the regret instead of the less informative expected regret.
Value-at-Risk is considered in the context of a specific bandit policy family by \cite{Audibert2007}, \cite{AUDIBERT20091876}.

\section{Concentration inequalities for order statistics}

Concentration inequalities for order statistics \cite{boucheron2012}


This note describes non-asymptotic variance and tail bounds for order statistics of samples of independent identically distributed random variables.

There are two main assumptions: 
\begin{enumerate}
    \item Fixed number of samples.
    \item Non-decreasing hazard rate (a condition that is satisfied by Gaussian, exponential, Gumbel, logistic distributions, . . .). The hazard rate of an absolutely continuous probability distribution with distribution function F is: h = f /F where f and F = 1 - F are respectively the density and the survival function associated with F.
\end{enumerate}

The authors makes use of Efron-Stein inequalities to derive the \textbf{Exponential Efron-Stein inequality (Theorem 2.9 in the paper)}, we state it here, let $X_1,..., X_n$ be independently distributed according to F, $X_{(1)} \geq \ldots \geq X_{(n)}$ be the order statistics and let $\triangle_k = X_{(k)} - X_{(k+1)}$ be the $k^{th}$ spacing. Let $V_k = k \triangle_k^2$ denote the Efron-Stein estimate of the variance of $X_{(k)}$ (for k = 1, ..., n/2). If F has a non-decreasing hazard rate h, then for $1 \leq k \leq n/2$,
\begin{align}
    \operatorname{Var}\left[X_{(k)}\right] \leq \mathbb{E} V_{k} \leq \frac{2}{k} \mathbb{E}\left[\left(\frac{1}{h\left(X_{(k+1)}\right)}\right)^{2}\right]
\end{align}
For $\lambda \geq 0$, and $1\leq k \leq n/2$,
\begin{align}
    \label{Boucheron theorem 9-2}
    \log \operatorname{Ee}^{\lambda\left(X_{(k)}-\mathrm{E} X_{(k)}\right)} \leq \lambda \frac{k}{2} \mathbb{E}\left[\Delta_{k}\left(e^{\lambda \Delta_{k}}-1\right)\right]=\lambda \frac{k}{2} \mathbb{E}\left[\sqrt{\frac{V_{k}}{k}}\left(e^{\lambda \sqrt{V_{k} / k}}-1\right)\right]
\end{align}
It connects the logarithmic moment generating function of the k th order statistic with the exponential moments of the square root of the Efron-Stein estimate of variance $k \triangle_k^2$.
Based on the above theorem, order statistics of Gaussian samples are analyzed and Bernstein inequalities for order statistics of \underline{absolute value} of independent \underline{standard Gaussian} random variables in Section 4 in \cite{boucheron2012}. \textbf{As stated in Proposition 4.6}: Let $v_{n}=8 /(n \log 2)$, for all $0 \leq \lambda<n /\left(2 \sqrt{v_{n}}\right)$,

\begin{align}
    \label{Boucheron proposition 4.6-1}
    \log \operatorname{Ee}^{\lambda\left(X_{(n / 2)}-\mathrm{E} X_{(n / 2)}\right)} \leq \frac{v_{n} \lambda^{2}}{2\left(1-2 \lambda \sqrt{v_{n} / n}\right)}
\end{align}
For all $t > 0$
\begin{align}
    \mathbb{P}\left\{X_{(n / 2)}-\mathbb{E} X_{(n / 2)}>\sqrt{2 v_{n} t}+2 t \sqrt{v_{n} / n}\right\} \leq e^{-t}
\end{align}

Considerations:
\begin{enumerate}
    \item From formula (\ref{Boucheron theorem 9-2}), we can derive tail bound 
    \begin{align}
        P(X_{(k)} - \mathbb{E}X_{(k)} > t) &\leq \exp\{\log \operatorname{Ee}^{\lambda\left(X_{(k)}-\mathrm{E} X_{(k)}\right)} - \lambda t \}\\
        &\leq \exp \{\lambda \frac{k}{2} \mathbb{E}\left[\sqrt{\frac{V_{k}}{k}}\left(e^{\lambda \sqrt{V_{k} / k}}-1\right)\right] - \lambda t\}\\
        &\leq \inf_{\lambda \geq 0} \exp \{\lambda \frac{k}{2} \mathbb{E}\left[\sqrt{\frac{V_{k}}{k}}\left(e^{\lambda \sqrt{V_{k} / k}}-1\right)\right] - \lambda t\}
    \end{align}
    with the above two assumptions holding (fixed sample size, non-decreasing hazard rate). Note the one the right hand side, we have k and $k^{th}$ spacing $\triangle_k$ (also, why not dependent on n?). But still quantiles are not included.
    
    \item The paper includes Gaussian as an example, because for Gaussian we can easily derive a bound using the pdf and cdf distribution. However, if we don't specify a distribution form, we cannot get rid of $\triangle_k$ in the bound, which may make the bound looks not that interesting. In terms of whether we can give an interesting bound for other distributions other than Gaussian, we still need to do more proofs to verify.  
    
    \item One problem of (\ref{Boucheron proposition 4.6-1}) is that $v_n$ is not variance or second order moment. 
\end{enumerate}
\printbibliography
\end{document}


