@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proceedings of the 24th annual Conference On Learning Theory},
  pages={359--376},
  year={2011}
}

@article{bubeck2013bandits,
  title={Bandits with heavy tail},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  journal={IEEE Transactions on Information Theory},
  volume={59},
  number={11},
  pages={7711--7717},
  year={2013},
  publisher={IEEE}
}

@InProceedings{Maillard2013,
author="Maillard, Odalric-Ambrym",
editor="Jain, Sanjay
and Munos, R{\'e}mi
and Stephan, Frank
and Zeugmann, Thomas",
title="Robust Risk-Averse Stochastic Multi-armed Bandits",
booktitle="Algorithmic Learning Theory",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="218--233",
abstract="We study a variant of the standard stochastic multi-armed bandit problem when one is not interested in the arm with the best mean, but instead in the arm maximizing some coherent risk measure criterion. Further, we are studying the deviations of the regret instead of the less informative expected regret. We provide an algorithm, called RA-UCB to solve this problem, together with a high probability bound on its regret.",
isbn="978-3-642-40935-6"
}

@InProceedings{Audibert2007,
author="Audibert, Jean-Yves
and Munos, R{\'e}mi
and Szepesv{\'a}ri, Csaba",
editor="Hutter, Marcus
and Servedio, Rocco A.
and Takimoto, Eiji",
title="Tuning Bandit Algorithms in Stochastic Environments",
booktitle="Algorithmic Learning Theory",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="150--165",
abstract="Algorithms based on upper-confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. In this paper we consider a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. The purpose of this paper is to provide a theoretical explanation of these findings and provide theoretical guidelines for the tuning of the parameters of these algorithms. For this we analyze the expected regret and for the first time the concentration of the regret. The analysis of the expected regret shows that variance estimates can be especially advantageous when the payoffs of suboptimal arms have low variance. The risk analysis, rather unexpectedly, reveals that except for some very special bandit problems, the regret, for upper confidence bounds based algorithms with standard bias sequences, concentrates only at a polynomial rate. Hence, although these algorithms achieve logarithmic expected regret rates, they seem less attractive when the risk of suffering much worse than logarithmic regret is also taken into account.",
isbn="978-3-540-75225-7"
}

@article{AUDIBERT20091876,
title = "Exploration–exploitation tradeoff using variance estimates in multi-armed bandits",
journal = "Theoretical Computer Science",
volume = "410",
number = "19",
pages = "1876 - 1902",
year = "2009",
note = "Algorithmic Learning Theory",
issn = "0304-3975",
doi = "https://doi.org/10.1016/j.tcs.2009.01.016",
url = "http://www.sciencedirect.com/science/article/pii/S030439750900067X",
author = "Jean-Yves Audibert and Rémi Munos and Csaba Szepesvári",
keywords = "Exploration–exploitation tradeoff, Multi-armed bandits, Bernstein’s inequality, High-probability bound, Risk analysis",
abstract = "Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations."
}

