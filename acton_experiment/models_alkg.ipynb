{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible models for active learning on knowledge graph\n",
    "\n",
    "Given random variables: $$e_i \\thicksim P(e_i|...),$$ $$e_j \\thicksim P(e_j|...),$$ $$R_k \\thicksim P(R_k|...).$$\n",
    "\n",
    "\n",
    "#### Model a\n",
    "\n",
    "Consider $y_{ijk}$ as binary variable, i.e. $y_{ijk} \\in \\{0,1\\}$. We first Sample $\\hat{e_i}, \\hat{e_j}, \\hat{R_k}$ from random variable's distribution. We place a normal distribution over $x_{ijk}$, \n",
    "   $$x_{ijk} \\thicksim \\mathcal{N}(\\hat{e_i}^T \\hat{R_k} \\hat{e_j}, \\sigma_x^2)$$\n",
    "   \n",
    "Take $\\hat{x_{ijk}}$ as the expection as variable $x_{ijk}$ \n",
    "$$\\hat{x_{ijk}} = \\mathbb{E}(x_{ijk}) = \\hat{e_i}^T \\hat{R_k} \\hat{e_j}$$ Then we make prediction on $P(y_{ijk} = 1|x_{ijk}) = \\sigma(\\hat{x_{ijk}})$, where $\\sigma(x)$ is sigmod function of x. Thus $P(y_{ijk} = 0|x_{ijk}) = 1 - \\sigma(\\hat{x_{ijk}})$\n",
    "\n",
    "Then we apply uncertainty sampling (least confidence) onto the predictions:\n",
    "   $$r_{LC} (\\mathcal{u})  = {argmax}_{x \\in \\mathcal{u}} \\{ - {max}_{y} p(y|x)\\}$$\n",
    "   \n",
    "Note the assumption is that we can <span style=\"color:blue\">*map the prediction probability into the expected sample product*</span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Model b\n",
    "\n",
    "Instead of giving entry $x_{ijk}$ a predicted probability, we try to calculate the uncertainty given the distribution associated with $x_{ijk}$. Then we pick the instance with greatest uncertainty (Variance).\n",
    "\n",
    "See [Computing the Distribution of the Product of Two Continuous Random Variables](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.7546&rep=rep1&type=pdf) for Example 4.3 and Figure 3 for visualization. We extend this example for our case, we know that <span style=\"color:blue\">*$x_{ijk}$ is not Guassian distribution*</span>. So there are two questions need to be solved:\n",
    "\n",
    "1. How to calculate the distribution of $x_{ijk}$?\n",
    "2. How to calculate the uncertainty of $x_{ijk}$ given its distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model c\n",
    "\n",
    "We can also calculate the uncertainty of latent variables first, and then get the uncertainty of $x_{ijk}$ by:\n",
    "\n",
    "$$uncer(x_{ijk}) = f(uncer(e_i), uncer(e_j), uncer(R_k))$$\n",
    "\n",
    "The uncertainty of variables can be expressed by their variance.\n",
    "\n",
    "So <span style=\"color:blue\">*the point is to figure out what's $f(\\cdot)$*</span>?\n",
    "\n",
    "According to http://www.odelama.com/data-analysis/Commonly-Used-Math-Formulas/, variance of the product of correlated variables X, Y:\n",
    "\n",
    "$$Var(X \\cdot Y) = Cov(X^2, Y^2) + [Var(X) + {Eva}^2(X)] \\cdot [Var(Y) + {Eva}^2(Y)] - [Cov(X,Y) + Eva(X) \\cdot Eva(Y)]^2$$\n",
    "\n",
    "The variance of two independent random variables ($Cov(X^2, Y^2) = 0, Cov(X,Y) = 0$):\n",
    "\n",
    "$$Var(X \\cdot Y) = Var(X) Var(Y) + Var(Y) {Eva}^2(X) + Var(X) {Eva}^2(Y)$$\n",
    "\n",
    "See [The Variance of the Product of K Random Variables](http://www.cs.cmu.edu/~cga/var/2282440.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations:\n",
    "\n",
    "1. The Model b and c have the same goal: use the variance of the random variable x as its uncertainty. So if we can get the variance of x by directly deducting the variance (Model c, or error propagation?), we don't need to derivate the formula of x's distribution (Model b).\n",
    "\n",
    "\n",
    "2. When the class is unbalanced, we consider the top-k approach. However, I wonder whether this idea is to find the k triple which we are most certain to be valid? \n",
    "However,  for knowledge population, we actually not really care about those unobserved triples with great certainty to be valid. We can just predict them as valid if we are very sure about that. Instead, we care about those are likely to be valid, but we are not very sure about that.    \n",
    "We need to discuss what is the goal setting.\n",
    "\n",
    "\n",
    "3. Stochastic Gradient Descent vs. Bayesian Sequential updating:  \n",
    "Consider the table in the [From Linear Regression to Matrix/ Tensor factorization](https://github.com/chengsoonong/eheye/blob/master/matrix_factorization/MF_LR.tex), when we deal with linear regression problem, we define the best-fitting line as the line that minimizes the sum of squared errors (SSE) or mean squared error (MSE) between our target variable (y) and our predicted output over all samples i in our dataset of size n, then we can implement a linear regression model for performing ordinary least squares regression using one of the following approaches:  \n",
    "i) Solving the model parameters analytically (closed-form equations, Maximum Likelihood, MAP)  \n",
    "ii) Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent)  \n",
    "The two approaches achieve the same goal (but whether converge at the same point? convex? multiple local minima?).   \n",
    "For our approaches, <span style=\"color:blue\">*we didn't find the optimal feature representation for latent variable, instead we sample from the posterior distributions (for the purpose of using TS)*</span>. To find the optimal feature representation, we can either MAP or SGD based on the tensor factorization approach (RESCAL/PRESCAL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
